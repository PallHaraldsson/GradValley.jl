var documenterSearchIndex = {"docs":
[{"location":"tutorials_and_examples/#Tutorials-and-Examples","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Here, you can find detailed explanations on how to build and train specific models with GradValley.jl.","category":"page"},{"location":"tutorials_and_examples/#A-LeNet-like-model-for-handwritten-digit-recognition","page":"Tutorials and Examples","title":"A LeNet-like model for handwritten digit recognition","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"In this tutorial, we will learn the basics of GradValley.jl while building a model for handwritten digit recognition, reaching approximately 99% accuracy on the MNIST-dataset. The whole code at once can be found here.","category":"page"},{"location":"tutorials_and_examples/#Importing-modules","page":"Tutorials and Examples","title":"Importing modules","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"using GradValley # the master module of GradValley.jl\r\nusing GradValley.Layers # The \"Layers\" module provides all the building blocks for creating a model. \r\nusing GradValley.Optimization # The \"Optimization\" module provides different loss functions and optimizers.","category":"page"},{"location":"tutorials_and_examples/#Using-the-dataset","page":"Tutorials and Examples","title":"Using the dataset","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"We will use the MLDatasets package which downloads the MNIST-dataset for us automatically. If you haven't installed MLDatasets yet, write this for installation:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"import Pkg; Pkg.add(\"MLDatasets\")","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Then we can import MLDatasets:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"using MLDatasets # a package for downloading datasets","category":"page"},{"location":"tutorials_and_examples/#Splitting-up-the-dataset-into-a-train-and-a-test-partition","page":"Tutorials and Examples","title":"Splitting up the dataset into a train and a test partition","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The MNIST-dataset contains 70,000 images, we will use 60,000 images for training the network and 10,000 images for evaluating accuracy.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# initialize train- and test-dataset\r\nmnist_train = MNIST(:train) \r\nmnist_test = MNIST(:test)","category":"page"},{"location":"tutorials_and_examples/#Using-GradValley.DataLoader-for-handling-data","page":"Tutorials and Examples","title":"Using GradValley.DataLoader for handling data","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"A typical workflow when dealing with datasets is to use the GradValley.DataLoader struct. A data loader makes it easy to iterate directly over the batches in a dataset.  Due to better memory efficiency, the data loader loads the batches just in time. When initializing a data loader, we specify a function that returns exactly one element from the dataset at a given index. We also have to specify the size of the dataset (e.g. the number of images). All parameters that the data loader accepts (see Reference for more information):","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"DataLoader(get_function::Function, dataset_size::Integer; batch_size::Integer=1, shuffle::Bool=false, drop_last::Bool=false)","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Now we write the get function for the two data loaders.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# function for getting an image and the corresponding target vector from the train or test partition\r\nfunction get_element(index, partition)\r\n    # load one image and the corresponding label\r\n    if partition == \"train\"\r\n        image, label = mnist_train[index]\r\n    else # test partition\r\n        image, label = mnist_test[index]\r\n    end\r\n    # add channel dimension and rescaling the values to their original 8 bit gray scale values\r\n    image = reshape(image, 28, 28, 1) .* 255\r\n    # generate the target vector from the label, one for the correct digit, zeros for the wrong digits\r\n    # the element type of the image is Float32, so the target vector should have the same element type\r\n    target = zeros(Float32, 10) \r\n    target[label + 1] = 1.f0\r\n\r\n    return image, target\r\nend","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"We can now initialize the data loaders.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# initialize the data loaders\r\ntrain_data_loader = DataLoader(index -> get_element(index, \"train\"), length(mnist_train), batch_size=32, shuffle=true)\r\ntest_data_loader = DataLoader(index -> get_element(index, \"test\"), length(mnist_test), batch_size=32)","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"If you want to force the data loader to load the data all at once, you could do:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# force the data loaders to load all the data at once into memory, depending on the dataset's size, this may take a while\r\ntrain_data = train_data_loader[begin:end]\r\ntest_data = test_data_loader[begin:end]","category":"page"},{"location":"tutorials_and_examples/#Building-the-neuronal-network-aka.-the-model","page":"Tutorials and Examples","title":"Building the neuronal network aka. the model","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The recommend way to build feed forward models is to use the GradValley.Layers.SequentialContainer struct. A SequtialContainer can take an array of layers or other containers (sub-models). While forward-pass, the given inputs are sequentially propagated through every layer (or sub-model) and the output will be returned. For more details, see Reference. The LeNet5 model is one of the earliest convolutional neuronal networks (CNNs) reaching approximately 99% accuracy on the MNIST-dataset. The LeNet5 is built of two main parts, the feature extractor and the classifier. So it would be a good idea to clarify that in the code:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# Definition of a LeNet-like model consisting of a feature extractor and a classifier\r\nfeature_extractor = SequentialContainer([ # a convolution layer with 1 in channel, 6 out channels, a 5*5 kernel and a relu activation\r\n                                         Conv(1, 6, (5, 5), activation_function=\"relu\"),\r\n                                         # an average pooling layer with a 2*2 filter (when not specified, stride is automatically set to kernel size)\r\n                                         AvgPool((2, 2)),\r\n                                         Conv(6, 16, (5, 5), activation_function=\"relu\"),\r\n                                         AvgPool((2, 2))])\r\nflatten = Reshape((256, ))\r\nclassifier = SequentialContainer([ # a fully connected layer (also known as dense or linear) with 256 in features, 120 out features and a relu activation\r\n                                  Fc(256, 120, activation_function=\"relu\"),\r\n                                  Fc(120, 84, activation_function=\"relu\"),\r\n                                  Fc(84, 10),\r\n                                  # a softmax activation layer, the softmax will be calculated along the first dimension (the features dimension)\r\n                                  Softmax(dims=1)])\r\n# The final model consists of three different submodules, \r\n# which shows that a SequentialContainer can contain not only layers, but also other SequentialContainers\r\nmodel = SequentialContainer([feature_extractor, flatten, classifier])\r\n\r\n# After a model is initialized, its parameters are Float32 arrays by default. The input to the model must always be of the same element type as its parameters!\r\n# You can change the device (CPU/GPU) and element type of the model's parameters with the function module_to_eltype_device!\r\n# The element type of our data (image/target) is already Float32 and because this LeNet is such a small model, using the CPU is just fine.","category":"page"},{"location":"tutorials_and_examples/#Printing-a-nice-looking-summary-of-the-model","page":"Tutorials and Examples","title":"Printing a nice looking summary of the model","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Summarizing a model and counting the number of trainable parameters is easily done with the GradValley.Layers.summarie_model function.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# printing a nice looking summary of the model\r\nsummary, num_params = summarize_model(model)\r\nprintln(summary)","category":"page"},{"location":"tutorials_and_examples/#Defining-hyperparameters","page":"Tutorials and Examples","title":"Defining hyperparameters","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Before we start to train and test the model, we define all necessary hyperparameters. If we want to change the learning rate or the loss function for example, this is the one place to do this.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# defining hyperparameters\r\nloss_function = mse_loss # mean squared error\r\nlearning_rate = 0.05\r\noptimizer = MSGD(model, learning_rate, momentum=0.5) # momentum stochastic gradient descent with a momentum of 0.5\r\nepochs = 5 # 5 or 10, for example","category":"page"},{"location":"tutorials_and_examples/#Train-and-test-the-model","page":"Tutorials and Examples","title":"Train and test the model","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The next step is to write a function for training the model using the above defined hyperparameters. For example, the network is trained 5 or 10 times (epochs) with the entire training data set. After each batch, the weights/parameters of the network are adjusted/optimized. However, we want to test the model after each epoch, so we need to write a function for evaluating the model's accuracy first.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# evaluate the model's accuracy\r\nfunction test()\r\n    num_correct_preds = 0\r\n    avg_test_loss = 0\r\n    for (batch, (images_batch, targets_batch)) in enumerate(test_data_loader)\r\n        # computing predictions\r\n        predictions_batch = model(images_batch) # equivalent to forward(model, images_batch)\r\n        # checking for each image in the batch individually if the prediction is correct\r\n        batch_size = size(predictions_batch)[end] # the batch dimension is always the last dimension\r\n        for index_batch in 1:batch_size\r\n            single_prediction = predictions_batch[:, index_batch]\r\n            single_target = targets_batch[:, index_batch]\r\n            if argmax(single_prediction) == argmax(single_target)\r\n                num_correct_preds += 1\r\n            end\r\n        end\r\n        # adding the loss for measuring the average test loss\r\n        avg_test_loss += loss_function(predictions_batch, targets_batch, return_derivative=false)\r\n    end\r\n\r\n    accuracy = num_correct_preds / size(test_data_loader) * 100 # size(data_loader) returns the dataset size\r\n    avg_test_loss /= length(test_data_loader) # length(data_loader) returns the number of batches\r\n\r\n    return accuracy, avg_test_loss\r\nend\r\n\r\n# train the model with the above defined hyperparameters\r\nfunction train()\r\n    for epoch in 1:epochs\r\n\r\n        @time begin # for measuring time taken by one epoch\r\n\r\n            avg_train_loss = 0.00\r\n            # iterating over the whole data set\r\n            for (batch, (images_batch, targets_batch)) in enumerate(train_data_loader)\r\n                # computing predictions\r\n                predictions_batch = model(images_batch) # equivalent to forward(model, images_batch)\r\n                # backpropagation\r\n                zero_gradients(model) # reset the gradients\r\n                loss, derivative_loss = loss_function(predictions_batch, targets_batch)\r\n                backward(model, derivative_loss) # compute the gradients\r\n                # optimize the model's parameters\r\n                step!(optimizer)\r\n                # printing status\r\n                if batch % 100 == 0\r\n                    image_index = batch * train_data_loader.batch_size\r\n                    data_set_size = size(train_data_loader)\r\n                    println(\"Batch $batch, Image [$image_index/$data_set_size], Loss: $(round(loss, digits=5))\")\r\n                end\r\n                # adding the loss for measuring the average train loss\r\n                avg_train_loss += loss\r\n            end\r\n\r\n            avg_train_loss /= length(train_data_loader)\r\n            accuracy, avg_test_loss = test()\r\n            print(\"Results of epoch $epoch: Avg train loss: $(round(avg_train_loss, digits=5)), Avg test loss: $(round(avg_test_loss, digits=5)), Accuracy: $accuracy%, Time taken:\")\r\n\r\n        end\r\n\r\n    end\r\nend","category":"page"},{"location":"tutorials_and_examples/#Run-the-training-and-save-the-trained-model-afterwards","page":"Tutorials and Examples","title":"Run the training and save the trained model afterwards","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"When the file is run as the main script, we want to actually call the train() function and save the final model afterwards. We will use the BSON.jl package for saving the model easily.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# when this file is run as the main script,\r\n# then train() is run and the final model will be saved using a package called BSON.jl\r\nimport Pkg; Pkg.add(\"BSON\")\r\nusing BSON: @save # a package for saving and loading julia objects as files\r\nif abspath(PROGRAM_FILE) == @__FILE__\r\n    train()\r\n    file_name = \"MNIST_with_LeNet5_model.bson\"\r\n    @save file_name model\r\n    println(\"Saved trained model as $file_name\")\r\nend","category":"page"},{"location":"tutorials_and_examples/#Use-the-trained-model","page":"Tutorials and Examples","title":"Use the trained model","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"If you want to easily use the trained model, you firstly need to import the necessary modules from GradValley. Then you can use the @load macro of BSON to load the model object. Now you can let the model make a few individual predictions, for example. Use this code in in another file.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# load the model and make some individual predictions\r\n\r\nusing GradValley\r\nusing GradValley.Layers \r\nusing GradValley.Optimization\r\nusing MLDatasets\r\nusing BSON: @load\r\n\r\n# load the pre-trained model\r\n@load \"MNIST_with_LeNet5_model.bson\" model\r\n\r\n# make some individual predictions\r\nmnist_test = MNIST(:test)\r\nfor i in 1:5\r\n    random_index = rand(1:length(mnist_test))\r\n    image, label = mnist_test[random_index]\r\n    # remember to add batch and channel dimensions and to rescale the image as was done during training and testing\r\n    image_batch = reshape(image, 28, 28, 1, 1) .* 255\r\n    prediction = model(image_batch)\r\n    predicted_label = argmax(prediction[:, 1]) - 1\r\n    println(\"Predicted label: $predicted_label, Correct Label: $label\")\r\nend","category":"page"},{"location":"tutorials_and_examples/#Running-the-file-with-multiple-threads","page":"Tutorials and Examples","title":"Running the file with multiple threads","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"It is heavily recommended to run this file, and any other files using GradValley, with multiple threads. Using multiple threads can make training and calculating predictions much faster. To do this, use the -t option when running a julia script in terminal/PowerShell/command line/etc. If your CPU has 24 threads, for example, then run:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"julia -t 24 ./MNIST_with_LeNet5.jl","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The specified number of threads should match the number of threads your CPU provides.","category":"page"},{"location":"tutorials_and_examples/#Results","page":"Tutorials and Examples","title":"Results","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"These were my results after 5 training epochs: Results of epoch 5: Avg train loss: 0.00239, Avg test loss: 0.00248, Accuracy: 98.36%, Time taken:  5.649449 seconds (20.96 M allocations: 13.025 GiB, 10.04% gc time) On my Ryzen 9 5900X CPU (using all 24 threads, slightly overclocked), one epoch took around ~6 seconds (no compilation time), so the whole training (5 epochs) took around ~30 seconds (no compilation time).","category":"page"},{"location":"tutorials_and_examples/#Generic-ResNet-(18/34/50/101/152)-implementation","page":"Tutorials and Examples","title":"Generic ResNet (18/34/50/101/152) implementation","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The same code can be also found here.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"This example shows the ResNet implementation used by the pre-trained ResNets. The function ResBlock generates a standard residual block (with one residual/skipped connection) with optional downsampling.  On the other hand, the function ResBottelneckBlock generates a bottleneck residual block (a variant of the residual block that utilises 1x1 convolutions to create a bottleneck) with optional downsampling. The residual connections can be easily implemented using the GraphContainer. GraphContainer allows differentiation for any computational graphs (not only sequential graphs for which the SequentialContainer is intended). The function ResNet constructs a generic ResNet. The functions ResNetXX use this function to create the individual models.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Note that this implementation is inspired by this article.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# import GradValley\r\nusing GradValley\r\nusing GradValley.Layers\r\n\r\n# define a ResBlock (with optional downsampling) \r\nfunction ResBlock(in_channels::Int, out_channels::Int, downsample::Bool)\r\n    # define modules\r\n    if downsample\r\n        shortcut = SequentialContainer([\r\n            Conv(in_channels, out_channels, (1, 1), stride=(2, 2), use_bias=false),\r\n            BatchNorm(out_channels)\r\n        ])\r\n        conv1 = Conv(in_channels, out_channels, (3, 3), stride=(2, 2), padding=(1, 1), use_bias=false)\r\n    else\r\n        shortcut = Identity()\r\n        conv1 = Conv(in_channels, out_channels, (3, 3), padding=(1, 1), use_bias=false)\r\n    end\r\n\r\n    conv2 = Conv(out_channels, out_channels, (3, 3), padding=(1, 1), use_bias=false)\r\n    bn1 = BatchNorm(out_channels, activation_function=\"relu\")\r\n    bn2 = BatchNorm(out_channels) # , activation_function=\"relu\"\r\n\r\n    relu = Identity(activation_function=\"relu\")\r\n\r\n    # define the forward pass with the residual/skipped connection\r\n    function forward_pass(modules, input)\r\n        # extract modules from modules vector (not necessary (therefore commented out) because the forward_pass function is defined in the ResBlock function (not somewhere \"outside\").)\r\n        # shortcut, conv1, conv2, bn1, bn2, relu = modules\r\n        # compute shortcut\r\n        output_shortcut = forward(shortcut, input)\r\n        # compute sequential part\r\n        output = forward(bn1, forward(conv1, input))\r\n        output = forward(bn2, forward(conv2, output))\r\n        # residual/skipped connection\r\n        output = forward(relu, output + output_shortcut) \r\n        \r\n        return output\r\n    end\r\n\r\n    # initialize a container representing the ResBlock\r\n    modules = [shortcut, conv1, conv2, bn1, bn2, relu]\r\n    res_block = GraphContainer(forward_pass, modules)\r\n\r\n    return res_block\r\nend\r\n\r\n# define a ResBottelneckBlock (with optional downsampling) \r\nfunction ResBottelneckBlock(in_channels::Int, out_channels::Int, downsample::Bool)\r\n    # define modules\r\n    shortcut = Identity()   \r\n\r\n    if downsample || in_channels != out_channels\r\n        if downsample\r\n            shortcut = SequentialContainer([\r\n                Conv(in_channels, out_channels, (1, 1), stride=(2, 2), use_bias=false),\r\n                BatchNorm(out_channels)\r\n            ])\r\n        else\r\n            shortcut = SequentialContainer([\r\n                Conv(in_channels, out_channels, (1, 1), use_bias=false),\r\n                BatchNorm(out_channels)\r\n            ])\r\n        end\r\n    end\r\n\r\n    conv1 = Conv(in_channels, out_channels ÷ 4, (1, 1), use_bias=false)\r\n    if downsample\r\n        conv2 = Conv(out_channels ÷ 4, out_channels ÷ 4, (3, 3), stride=(2, 2), padding=(1, 1), use_bias=false)\r\n    else\r\n        conv2 = Conv(out_channels ÷ 4, out_channels ÷ 4, (3, 3), padding=(1, 1), use_bias=false)\r\n    end\r\n    conv3 = Conv(out_channels ÷ 4, out_channels, (1, 1), use_bias=false)\r\n\r\n    bn1 = BatchNorm(out_channels ÷ 4, activation_function=\"relu\")\r\n    bn2 = BatchNorm(out_channels ÷ 4, activation_function=\"relu\")\r\n    bn3 = BatchNorm(out_channels) # , activation_function=\"relu\"\r\n\r\n    relu = Identity(activation_function=\"relu\")\r\n\r\n    # define the forward pass with the residual/skipped connection\r\n    function forward_pass(modules, input)\r\n        # extract modules from modules vector (not necessary (therefore commented out) because the forward_pass function is defined in the ResBlock function (not somewhere \"outside\").)\r\n        # shortcut, conv1, conv2, conv3, bn1, bn2, bn3, relu = modules\r\n        # compute shortcut\r\n        output_shortcut = forward(shortcut, input)\r\n        # compute sequential part\r\n        output = forward(bn1, forward(conv1, input))\r\n        output = forward(bn2, forward(conv2, output))\r\n        output = forward(bn3, forward(conv3, output))\r\n        # residual/skipped connection\r\n        output = forward(relu, output + output_shortcut) \r\n        \r\n        return output\r\n    end\r\n\r\n    # initialize a container representing the ResBlock\r\n    modules = [shortcut, conv1, conv2, conv3, bn1, bn2, bn3, relu]\r\n    res_bottelneck_block = GraphContainer(forward_pass, modules)\r\n\r\n    return res_bottelneck_block\r\nend\r\n\r\n# define a ResNet \r\nfunction ResNet(in_channels::Int, ResBlock::Union{Function, DataType}, repeat::Vector{Int}; use_bottelneck::Bool=false, classes::Int=1000)\r\n    # define layer0\r\n    layer0 = SequentialContainer([\r\n        Conv(in_channels, 64, (7, 7), stride=(2, 2), padding=(3, 3), use_bias=false),\r\n        BatchNorm(64, activation_function=\"relu\"),\r\n        MaxPool((3, 3), stride=(2, 2), padding=(1, 1))\r\n    ])\r\n\r\n    # define number of filters/channels\r\n    if use_bottelneck\r\n        filters = Int[64, 256, 512, 1024, 2048]\r\n    else\r\n        filters = Int[64, 64, 128, 256, 512]\r\n    end\r\n\r\n    # define the following modules\r\n    layer1_modules = [ResBlock(filters[1], filters[2], false)]\r\n    for i in 1:repeat[1] - 1\r\n        push!(layer1_modules, ResBlock(filters[2], filters[2], false))\r\n    end\r\n    layer1 = SequentialContainer(layer1_modules)\r\n\r\n    layer2_modules = [ResBlock(filters[2], filters[3], true)]\r\n    for i in 1:repeat[2] - 1\r\n        push!(layer2_modules, ResBlock(filters[3], filters[3], false))\r\n    end\r\n    layer2 = SequentialContainer(layer2_modules)\r\n\r\n    layer3_modules = [ResBlock(filters[3], filters[4], true)]\r\n    for i in 1:repeat[3] - 1\r\n        push!(layer3_modules, ResBlock(filters[4], filters[4], false))\r\n    end\r\n    layer3 = SequentialContainer(layer3_modules)\r\n\r\n    layer4_modules = [ResBlock(filters[4], filters[5], true)]\r\n    for i in 1:repeat[4] - 1\r\n        push!(layer4_modules, ResBlock(filters[5], filters[5], false))\r\n    end\r\n    layer4 = SequentialContainer(layer4_modules)\r\n\r\n    gap = AdaptiveAvgPool((1, 1))\r\n    flatten = Reshape((filters[5], ))\r\n    fc = Fc(filters[5], classes)\r\n\r\n    # initialize a container representing the ResNet\r\n    res_net = SequentialContainer([layer0, layer1, layer2, layer3, layer4, gap, flatten, fc])\r\n\r\n    return res_net\r\nend\r\n\r\n# construct a ResNet18\r\nfunction ResNet18(in_channels=3, classes=1000)\r\n    return ResNet(in_channels, ResBlock, [2, 2, 2, 2], use_bottelneck=false, classes=classes)\r\nend\r\n\r\n# construct a ResNet34\r\nfunction ResNet34(in_channels=3, classes=1000)\r\n    return ResNet(in_channels, ResBlock, [3, 4, 6, 3], use_bottelneck=false, classes=classes)\r\nend\r\n\r\n# construct a ResNet50\r\nfunction ResNet50(in_channels=3, classes=1000)\r\n    return ResNet(in_channels, ResBottelneckBlock, [3, 4, 6, 3], use_bottelneck=true, classes=classes)\r\nend\r\n\r\n# construct a ResNet101\r\nfunction ResNet101(in_channels=3, classes=1000)\r\n    return ResNet(in_channels, ResBottelneckBlock, [3, 4, 23, 3], use_bottelneck=true, classes=classes)\r\nend\r\n\r\n# construct a ResNet152\r\nfunction ResNet152(in_channels=3, classes=1000)\r\n    return ResNet(in_channels, ResBottelneckBlock, [3, 8, 36, 3], use_bottelneck=true, classes=classes)\r\nend","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"It is heavily recommended to run this file (or the file in which you include and use ResNet.jl), and any other files using GradValley, with multiple threads. Using multiple threads can make training and calculating predictions much faster. To do this, use the -t option when running a julia script in terminal/PowerShell/command line/etc. If your CPU has 24 threads, for example, then run:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"julia -t 24 ./ResNet.jl","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The specified number of threads should match the number of threads your CPU provides.","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#First-Impressions","page":"Getting Started","title":"First Impressions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This example shows the basic workflow on model building and how to use loss functions and optimizers to train the model:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using GradValley\r\nusing GradValley.Layers # The \"Layers\" module provides all the building blocks for creating a model.\r\nusing GradValley.Optimization # The \"Optimization\" module provides different loss functions and optimizers.\r\n\r\n# Definition of a LeNet-like model consisting of a feature extractor and a classifier\r\nfeature_extractor = SequentialContainer([ # a convolution layer with 1 in channel, 6 out channels, a 5*5 kernel and a relu activation\r\n                                         Conv(1, 6, (5, 5), activation_function=\"relu\"),\r\n                                         # an average pooling layer with a 2*2 filter (when not specified, stride is automatically set to kernel size)\r\n                                         AvgPool((2, 2)),\r\n                                         Conv(6, 16, (5, 5), activation_function=\"relu\"),\r\n                                         AvgPool((2, 2))])\r\nflatten = Reshape((256, ))\r\nclassifier = SequentialContainer([ # a fully connected layer (also known as dense or linear) with 256 in features, 120 out features and a relu activation\r\n                                  Fc(256, 120, activation_function=\"relu\"),\r\n                                  Fc(120, 84, activation_function=\"relu\"),\r\n                                  Fc(84, 10),\r\n                                  # a softmax activation layer, the softmax will be calculated along the first dimension (the features dimension)\r\n                                  Softmax(dims=1)])\r\n# The final model consists of three different submodules, \r\n# which shows that a SequentialContainer can contain not only layers, but also other SequentialContainers\r\nmodel = SequentialContainer([feature_extractor, flatten, classifier])\r\n                                  \r\n# feeding the network with some random data\r\n# After a model is initialized, its parameters are Float32 arrays by default. The input to the model must always be of the same element type as its parameters!\r\n# You can change the device (CPU/GPU) and element type of the model's parameters with the function module_to_eltype_device!\r\ninput = rand(Float32, 28, 28, 1, 32) # a batch of 32 images with one channel and a size of 28*28 pixels\r\nprediction = model(input) # layers and containers are callable, alternatively, you can call the forward function directly: forward(model, input)\r\n\r\n# choosing an optimizer for training\r\nlearning_rate = 0.05\r\noptimizer = MSGD(model, learning_rate, momentum=0.5) # momentum stochastic gradient decent with a momentum of 0.5\r\n\r\n# generating some random target data for a training step\r\ntarget = rand(Float32, size(prediction)...) # remember to specify the correct element type here as well\r\n# backpropagation\r\nzero_gradients(model)\r\nloss, derivative_loss = mse_loss(prediction, target) # mean squared error\r\nbackward(model, derivative_loss) # computing gradients\r\nstep!(optimizer) # making a optimization step with the calculated gradients and the optimizer","category":"page"},{"location":"getting_started/#First-Real-Project","page":"Getting Started","title":"First Real Project","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here are some suggestions to implement your first real project with GradValley.jl:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The \"Hello World\" of Deep Learning: Try the Tutorial on training A LeNet-like model for handwritten digit recognition.\nThe Reference: In the reference, you can find descriptions of all the layers, loss functions and optimizers.\nDownload a pre-trained model: More (Pre-Trained) Models will likely be deployed over time.\nLook at more Tutorials and Examples.","category":"page"},{"location":"(pre-trained)_models/#(Pre-Trained)-Models","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"","category":"section"},{"location":"(pre-trained)_models/#ResNet18/34/50/101/152-(Image-Classification)","page":"(Pre-Trained) Models","title":"ResNet18/34/50/101/152 (Image Classification)","text":"","category":"section"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"In the following table, the provided pre-trained ResNets are presented based on their key data (requires at least GradValley.jl v0.1.0). ","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"Model Parameters Pre-Trained on ImageNet Accuracy on ImageNet-1K: Top-1/Top-5 Categories File size Download\nResNet18 11,689,512 ✓ ~70%/~89% 1000 89 MB Here\nResNet34 21,797,672 ✓ ~73%/~91% 1000 166 MB Here\nResNet50 25,557,032 ✓ ~81%/~95% 1000 195 MB Here\nResNet101 44,549,160 ✓ ~82%/~96% 1000 340 MB Here\nResNet152 60,192,808 ✓ ~82%/~96% 1000 460 MB Here","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"Currently, the files are hosted on my website. If you know a better place to host files which are too big for GitHub but are part of an open source julia package, please let me know.","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"The downloaded folders each contain seven files:","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"ResNetXX/\r\n├── ResNet.jl\r\n├── ResNetXX_pre-trained.jld2\r\n├── categories_array.jld2\r\n├── preprocessing_for_resnets.jl\r\n├── load_ResNet_and_make_predictions.jl\r\n├── example_image1.jpg\r\n├── example_image2.jpg","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"Where XX stands for the \"ResNet ID\" (18/34/50/101/152). For using this files, the follwing packages must be installed: GradValley.jl, FileIO.jl, Images.jl and ImageTransformations.jl. The last two packages are used by preprocessing_for_resnets.jl.","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"ResNet.jl: The source script of the model definition, required for loading the ResNetXX_pre-trained.jld2 file.\nResNetXX_pre-trained.jld2: The pre-trained ResNet in form of a .jld2 file.\ncategories_array.jld2: A vector containing the 1000 category names of ImageNet in the form of a .jld2 file.\npreprocessing_for_resnets.jl: A script with useful functions for loading and pre-processing images before they can be classified by the ResNets. \nload_ResNet_and_make_predictions.jl: A short script showing how to load the model and use it to make predictions.\nexample_image1.jpg, example_image2.jpg: The example images used by load_ResNets_make_prediction.jl. The images don't come from the Image Net dataset or similar, instead, the two images are from my personal photo gallery on my phone.","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"For example, the load_ResNet_and_make_predictions.jl script for ResNet18 looks like this (all scripts look exactly the same except the ResNet \"ID\" in the model = load(\"ResNetXX_pre-trained.jld2\", \"model\") line and the function which is called for pre-processing, in this case preprocess_for_resnet18_and_34().): ","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"include(\"ResNet.jl\") # required for loading the ResNetXX_pre-trained.jld2 file\r\ninclude(\"preprocessing_for_resnets.jl\") # takes care of all the pre-processing of the images before they can be fed into the model\r\nusing FileIO # front-end package for loading e.g. JLD2 files \r\nusing CUDA\r\n\r\nmodel = load(\"ResNet18_pre-trained.jld2\", \"model\") # loads the pre-trained ResNet into the variable model (model is a SequentialContainer)\r\ntestmode!(model) # because we just want to make predictions, we switch the mode to testing/evaluation mode\r\ncategories = load(\"categories_array.jld2\", \"categories_array\") # categories is a vector containing all the 1000 classes of ImageNet-1k\r\n\r\n# define the element type you want to use\r\ndtype = Float32 # Float32 or Float64 (Float32 is usually faster, especially on the GPU!)\r\nuse_cuda = CUDA.functional() # check if a CUDA capable GPU is available\r\n# move the loaded model to the correct element type and device\r\nif use_cuda\r\n    println(\"The GPU is used\")\r\n    module_to_eltype_device!(model, element_type=dtype, device=\"gpu\")\r\nelse\r\n    println(\"The CPU is used\")\r\n    module_to_eltype_device!(model, element_type=dtype, device=\"cpu\")\r\nend\r\n\r\n# a function taking a path to an image file, returns the name of the predicted class and the score in percent\r\nfunction make_prediciton(path::AbstractString)\r\n    image = read_image_from_file(path) # reads the image into an UInt8 3d-array of size (width, height, channels) where channels must be 3\r\n    image = preprocess_for_resnet18_and_34(image, dtype=dtype) # pre-processing when using ResNet18/34\r\n    # image = preprocess_for_resnet50_and_101_and_152(image, dtype=dtype) # pre-processing when using ResNet50/101/152\r\n    image = add_batch_dim(image) # add a batch dimension is required before the image can be fed into the model, image is now a 4d-array of size (width, height, channels, 1) where channels must be 3\r\n    \r\n    # if CUDA is available, move the image array to the GPU\r\n    if use_cuda\r\n        image = CuArray(image)\r\n    end\r\n    \r\n    prediction = GradValley.Functional.softmax_forward(model(image), dims=1)[:, 1] # make the prediction (note that the first run can take a while because a lot of code has to get compiled at the first run)\r\n    # prediction = model(image)[:, 1] # try without softmax if you get NaN values using softmax (if the images have been prepared well by the pre-processing, this should not happen)\r\n\r\n    prediction = Array(prediction) # if a GPU is used, move the GPU array back to the CPU memory\r\n\r\n    class_index = argmax(prediction)\r\n    score = prediction[class_index] * 100\r\n    class = categories[class_index]\r\n\r\n    return class, score\r\nend\r\n\r\nfor testimage in [\"example_image1.jpg\", \"example_image2.jpg\"] # add more images here if you want \r\n    class, score = make_prediciton(testimage)\r\n    println(\"\"\"Image: $testimage: Predicted class: \"$(class)\" with score of $(round(score, digits=1))%\"\"\")\r\nend","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"It is heavily recommended to run this file, and any other files using GradValley, with multiple threads. Using multiple threads can make calculating predictions and training much faster. To do this, use the -t option when running a julia script in terminal/PowerShell/command line/etc. If your CPU has 24 threads, for example, then run: julia -t 24 ./load_ResNet_and_make_predictions.jl The specified number of threads should match the number of threads your CPU provides. When you run the script, you will get:","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"Image: example_image1.jpg: Predicted class: \"castle\" with score of 99.2%\r\nImage: example_image2.jpg: Predicted class: \"llama\" with score of 81.6%","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"We can see that both predicted classes are correct!","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"example_image1.jpg (Image: The first example image used by load_ResNets_make_prediction.jl) example_image2.jpg (Image: The second example image used by load_ResNets_make_prediction.jl)","category":"page"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"What's next? Maybe try changing the script to print out the top 5 categories and not just the top 1.","category":"page"},{"location":"learning/#Learning","page":"Learning","title":"Learning","text":"","category":"section"},{"location":"learning/#Detailed-introduction-to-Machine-Learning","page":"Learning","title":"Detailed introduction to Machine Learning","text":"","category":"section"},{"location":"learning/","page":"Learning","title":"Learning","text":"Go to the article (Pluto-Notebook): www.jst-projekte.de/einfuehrung-ml/","category":"page"},{"location":"learning/","page":"Learning","title":"Learning","text":"Note that the article is written in German. However, with today's translation tools built into browsers, it is easy to translate it into English, for example.","category":"page"},{"location":"learning/","page":"Learning","title":"Learning","text":"The article (Pluto notebook) entitled \"An interactive introduction to Machine Learning\" gives a detailed introduction to the most important concepts of Machine Learning.  Including e.g. data sets, machine learning models, weight initialization and training of machine learning models (optimization using loss functions and gradient descent).  The entire article is based on an example that uses Machine Learning to solve a non-linear regression problem.  Because the article was written using Pluto-Notebooks, it offers a lot of interactive functionalities. For example, you can change the learning using a slider rate and all the other code cells using the adjusted learning rate will update automatically!  To use the interactive features, you have to download the Pluto-Notebook (click on the button \"Edit or run this notebook\" in the upper right corner of the article) and run it on your local system (the Pluto package must be installed of course).","category":"page"},{"location":"learning/","page":"Learning","title":"Learning","text":"The article was written by Jonas Steinebach, the main author of GradValley and its documentation.","category":"page"},{"location":"learning/","page":"Learning","title":"Learning","text":"Over time, more helpful articles will be made available here.","category":"page"},{"location":"saving_and_loading/#Saving-and-Loading","page":"Saving and Loading","title":"Saving and Loading","text":"","category":"section"},{"location":"saving_and_loading/","page":"Saving and Loading","title":"Saving and Loading","text":"There doesn't exist the one right way to save and load models. However, at the moment, the JLD2 package is recommended. In the MNIST-Tutorial however, the BSON package was used. But this package has problems with very large files, for example with large ResNets (e.g. the pre-trained ResNets in the (Pre-Trained) Models section).","category":"page"},{"location":"saving_and_loading/","page":"Saving and Loading","title":"Saving and Loading","text":"Because GradValley saves some information for the backward pass (e.g. the during forward pass recorded computational graph) directly in the containers, it is highly recommended to run clean_model_from_backward_information! on the model first. Otherwise, the files may get larger than they would have to. If the model was moved to the GPU, it's also heavily recommend to move the model to the CPU before saving it to a file. To move the model back to the CPU, use module_to_eltype_device!. Then, you can save the model in the JLD2 file format with the FileIO package:","category":"page"},{"location":"saving_and_loading/","page":"Saving and Loading","title":"Saving and Loading","text":"# import all packages \r\nusing GradValley\r\nusing GradValley.Layer\r\nusing FileIO # the recommended package for saving/loading models\r\n# define a model as an example\r\nmodel = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\r\n# recommended: run clean_model_from_backward_information! on the model (doesn't necessary in this specific case because no forward/backward pass was run before)\r\nclean_model_from_backward_information!(model)\r\n# recommended: run module_to_eltype_device! with kw-arg device=\"cpu\" on the model (doesn't necessary in this specific case because the model wasn't moved to the GPU before)\r\nmodule_to_eltype_device!(model, device=\"cpu\")\r\n# save the model to a file \r\nfile_name = \"my_example_model.jld2\"\r\nsave(file_name, Dict(\"model\" => model))","category":"page"},{"location":"saving_and_loading/","page":"Saving and Loading","title":"Saving and Loading","text":"Loading the model is then normally done in another file. Note that all used packages that were used in connection with the saved model must be also imported in the script where the file is loaded again (in this case GradValley/GradValley.Layers).","category":"page"},{"location":"saving_and_loading/","page":"Saving and Loading","title":"Saving and Loading","text":"# import all used packages \r\nusing GradValley\r\nusing GradValley.Layer\r\nusing FileIO \r\n# load the model from a file\r\nfile_name = \"my_example_model.jld2\"\r\nmodel = load(file_name, \"model\")\r\n# you can run module_to_eltype_device! to make sure the parameter's element type is correct\r\nmodule_to_eltype_device!(model, element_type=Float32, device=\"cpu\")\r\n# test if the model works correctly\r\ninput = rand(Float32, 1000, 32)\r\noutput = model(input)","category":"page"},{"location":"gpu_support/#GPU-Support","page":"GPU Support","title":"GPU Support","text":"","category":"section"},{"location":"gpu_support/","page":"GPU Support","title":"GPU Support","text":"GradValley.jl supports CUDA capable Nvidia GPUs. GPU support is made possible by CUDA.jl and cuDNN.jl. Layers, containers, loss functions and optimizers work for GPUs exactly the same as they do for the CPU.  To move a model (single layer or container) to the GPU, use module_to_eltype_device! and set the keyword argument device to \"gpu\".  Because on the GPU, Float32 is usually much faster than Float64, always use Float32 if you don't need more precision. If your model was moved to the GPU, the input to your model and the target values must be of type CuArray. To learn more about how to use GPUs in Julia, see the website of JuliaGPU and the documentation of the CUDA.jl package.","category":"page"},{"location":"gpu_support/","page":"GPU Support","title":"GPU Support","text":"A comman workflow to enable training and inference on both on GPU and CPU is to use CUDA.functional() to check if a working GPU enviroment was found. If CUDA.functional() is true, than the model and the input/target data can be moved to the GPU. This way, the code works not just for one type of device.","category":"page"},{"location":"gpu_support/","page":"GPU Support","title":"GPU Support","text":"The following example describes how GPU and CPU ready code can look like.  A real example that uses the GPU (if availabel) for training is the DCGAN Tutorial. With a RTX 3090, for example, it's possible to train the DCGAN on Celeb-A HQ (30,000 images) for 25 epochs in just (5, 7) 10 minutes (much faster than on the CPU)!","category":"page"},{"location":"gpu_support/","page":"GPU Support","title":"GPU Support","text":"using GradValley\r\nusing GradValley.Layers\r\nusing GradValley.Optimization\r\nusing CUDA\r\n\r\n# AlexNet like model (without grouped convolution and dropout and with AvgPool instead of MaxPool)\r\nfeature_extractor = SequentialContainer([\r\n    Conv(3, 64, (11, 11), stride=(4, 4), activation_function=\"relu\"),\r\n    # MaxPool((3, 3), stride=(2, 2)),\r\n    AvgPool((3, 3), stride=(2, 2)),\r\n    Conv(64, 192, (5, 5), padding=(2, 2), activation_function=\"relu\"),\r\n    # MaxPool((3, 3), stride=(2, 2)),\r\n    AvgPool((3, 3), stride=(2, 2)),\r\n    Conv(192, 384, (3, 3), padding=(1, 1), activation_function=\"relu\"),\r\n    Conv(384, 256, (3, 3), padding=(1, 1), activation_function=\"relu\"),\r\n    Conv(256, 256, (3, 3), padding=(1, 1), activation_function=\"relu\"),\r\n    # MaxPool((3, 3), stride=(2, 2))\r\n    AvgPool((3, 3), stride=(2, 2))\r\n])\r\nclassifier = SequentialContainer([\r\n    AdaptiveAvgPool((6, 6)),\r\n    Reshape((6 * 6 * 256, )), # 6 * 6 * 256 = 9.216\r\n    Fc(9216, 4096, activation_function=\"relu\"),\r\n    Fc(4096, 4096, activation_function=\"relu\"),\r\n    Fc(4096, 1000),\r\n    # Softmax(dims=1)\r\n])\r\nmodel = SequentialContainer([feature_extractor, classifier])\r\n\r\n# define the element type you want to use \r\ndtype = Float32\r\n# check if CUDA is availabel\r\nuse_cuda = CUDA.functional()\r\n# move to model to the correct device and convert its parameters to the specified dtype\r\nif use_cuda\r\n    module_to_eltype_device!(model, element_type=dtype, device=\"gpu\")\r\nelse    \r\n    module_to_eltype_device!(model, element_type=dtype, device=\"cpu\")\r\nend\r\n\r\n# create some random data for testing\r\ninput = rand(dtype, 224, 224, 3, 32)\r\ntarget = rand(dtype, 1000, 32)\r\n# move the data to the GPU if necessary\r\nif use_cuda\r\n    input, target = CuArray.((input, target))\r\nend\r\n\r\n# define some hyperparamters for testing\r\nloss_fn = mse_loss\r\noptim = Adam(model)\r\n\r\n# forward pass\r\nprediction = model(input)\r\n# backward pass\r\nzero_gradients(model)\r\nloss, derivative_loss = loss_fn(prediction, target)\r\nbackward(model, derivative_loss)\r\n# optimization step\r\nstep!(optim)","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"The package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add https://github.com/jonas208/GradValley.jl","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia> import Pkg; Pkg.add(url=\"https://github.com/jonas208/GradValley.jl\")","category":"page"},{"location":"installation/#Used-Dependencies","page":"Installation","title":"Used Dependencies","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"You can look at the Project.toml file to find information about used dependencies and compatibility.","category":"page"},{"location":"installation/#All-used-dependencies-will-be-automatically-installed-due-installation-of-GradValley.jl.","page":"Installation","title":"All used dependencies will be automatically installed due installation of GradValley.jl.","text":"","category":"section"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"note: Note\nNote that for all keyword arguments of type NTuple{2, Int}, the order of dimensions is (y/height-dimension, x/width-dimension).","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]\r\nDepth = 4","category":"page"},{"location":"reference/#GradValley","page":"Reference","title":"GradValley","text":"","category":"section"},{"location":"reference/#DataLoader","page":"Reference","title":"DataLoader","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"DataLoader\r\nreshuffle!","category":"page"},{"location":"reference/#GradValley.DataLoader","page":"Reference","title":"GradValley.DataLoader","text":"DataLoader(get_function::Function, dataset_size::Integer; batch_size::Integer=1, shuffle::Bool=false, drop_last::Bool=false)\n\nThe DataLoader was designed to easily iterate over batches. Each time a new batch is requested, the data loader loads this batch \"just in time\" (instead of loading all the batches to memory at once). \n\nThe get_function is expected to load one item from a dataset at a given index. The specified get_function is expected to accept exactly one positional argument, which is the index of the item the get_function will return. A tuple of arbitrary length is expected as the return value of the get_function. Each element in this tuple must be an array. The length/size and type of the tuple and array is expected to be the same at each index. When a batch is requested, the data loader returns the tuple containing the with batch dimensions extended arrays.\n\nnote: Note\nThe DataLoader is iteratabel and indexable. size(dataloader) returns the given size of the dataset, length(dataloader) returns the total number of batches (equal if batch_size=1). When a range is given as the index argument, a vector containing multiple batches (arrays) is returned.\n\ntip: Tip\nIf you really want to load the whole dataset to memory (e.g. useful when training over multiple epochs, with this way, you don't have to reload the dataset each epoch over and over again), you can do so of course: all_batches = dataloader[start:end] where typeof(dataloader) == DataLoader\n\nArguments\n\nget_function::Function: the function which takes the index of an item from a dataset and returns that item (an arbitrary sized tuple containing arrays)\ndataset_size::Integer: the maximum index the get_function accepts (the number of items in the dataset, the dataset size)\nbatch_size::Integer=1: the batch size (the last dimension, the extended batch dimension, of each array in the returned tuple has this size)\nshuffle::Bool=false: reshuffle the data (doesn't reshuffle automatically after each epoch, use reshuffle! instead)\ndrop_last::Bool=false: set to true to drop the last incomplete batch, if the dataset size is not divisible by the batch size, if false and the size of dataset is not divisible by the batch size, then the last batch will be smaller\n\nExamples\n\n# EXAMPLE FROM https://jonas208.github.io/GradValley.jl/tutorials_and_examples/#Tutorials-and-Examples\njulia> using MLDatasets # a package for downloading datasets\n# initialize train- and test-dataset\njulia> mnist_train = MNIST(:train) \njulia> mnist_test = MNIST(:test)\n# define the get_element function:\n# function for getting an image and the corresponding target vector from the train or test partition\njulia> function get_element(index, partition)\n            # load one image and the corresponding label\n            if partition == \"train\"\n                image, label = mnist_train[index]\n            else # test partition\n                image, label = mnist_test[index]\n            end\n            # add channel dimension and rescaling the values to their original 8 bit gray scale values\n            image = reshape(image, 28, 28, 1) .* 255\n            # generate the target vector from the label, one for the correct digit, zeros for the wrong digits\n            target = zeros(10)\n            target[label + 1] = 1.00\n\n            return image, target\n       end\n# initialize the data loaders (with anonymous function which helps to easily distinguish between test- and train-partition)\ntrain_data_loader = DataLoader(index -> get_element(index, \"train\"), length(mnist_train), batch_size=32, shuffle=true)\ntest_data_loader = DataLoader(index -> get_element(index, \"test\"), length(mnist_test), batch_size=32)\n# in most cases NOT recommended: you can force the data loaders to load all the data at once into memory, depending on the dataset's size, this may take a while\njulia> # train_data = train_data_loader[begin:end] # turned off to save time\njulia> # test_data = test_data_loader[begin:end] # turned off to save time\n# now you can write your train- or test-loop like so \njulia> for (batch, (image_batch, target_batch)) in enumerate(test_data_loader) #=do anything useful here=# end\njulia> for (batch, (image_batch, target_batch)) in enumerate(train_data_loader) #=do anything useful here=# end\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.reshuffle!","page":"Reference","title":"GradValley.reshuffle!","text":"reshuffle!(data_loader::DataLoader)\n\nManually shuffle the data loader (even if shuffle is disabled in the given data loader). It is recommended to reshuffle after each epoch during training.\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Layers","page":"Reference","title":"GradValley.Layers","text":"","category":"section"},{"location":"reference/#Containers","page":"Reference","title":"Containers","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SequentialContainer\r\nGraphContainer\r\nsummarize_model\r\nmodule_to_eltype_device!\r\nclean_model_from_backward_information!","category":"page"},{"location":"reference/#GradValley.Layers.SequentialContainer","page":"Reference","title":"GradValley.Layers.SequentialContainer","text":"SequentialContainer(layer_stack::Vector{<: Any})\n\nA sequential container (recommended method for building models). A SequtialContainer can take a vector of layers or other containers (submodules). While forward-pass, the given inputs are sequentially propagated through every layer (or submodule) and the output will be returned. The execution order during forward pass is of course the same as the order in the vector containing the layers or submodules.\n\nnote: Note\nYou can use a SequentialContainer in a GraphContainer (and vice versa). You can also use a SequentialContainer in a SequentialContainer (nesting allowed).\n\nArguments\n\nlayer_stack::Vector{<: Any}: the vector containing the layers (or submodules, so other containers), the order of the modules in the vector corresponds to the execution order\n\nIndexing and Iteration\n\nThe sequential container is indexable and iterable. Indexing one element/iterating behaves like indexing one element of/iterating over  the sequential_container.layer_stack passed to the container at initialization. However, if the index is a range (UnitRange{<: Integer}),  a new SequentialContainer containing all the requested submodules/layers is initialized and returned.  length(sequential_container) and size(sequential_container) both just return the number of modules in the layers vector (equivalent to length(sequential_container.layer_stack)).\n\nExamples\n\n# a simple chain of fully connected layers\njulia> m = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# computing the output of the module (with random inputs)\njulia> input = rand(1000, 32)\njulia> output = forward(m, input)\n\n# a more complicated example with with nested submodules\njulia> feature_extractor_part_1 = SequentialContainer([Conv(1, 6, (5, 5), activation_function=\"relu\"), AvgPool((2, 2))])\njulia> feature_extractor_part_2 = SequentialContainer([Conv(6, 16, (5, 5), activation_function=\"relu\"), AvgPool((2, 2))])\njulia> feature_extractor = SequentialContainer([feature_extractor_part_1, feature_extractor_part_2])\njulia> classifier = SequentialContainer([Fc(256, 120, activation_function=\"relu\"), Fc(120, 84, activation_function=\"relu\"), Fc(84, 10)])\njulia> m = SequentialContainer([feature_extractor, Reshape((256, )), classifier, Softmax(dims=1)])\n# computing the output of the module (with random inputs)\njulia> input = rand(28, 28, 1, 32)\njulia> output = forward(m, input)\n\n# indexing \njulia> m[begin] # returns the feature_extractor_part_1 submodule (SequentialContainer)\njulia> m[end] # returns the softmax layer (Softmax)\njulia> m[begin:end-1] # returns the entire model except the softmax layer (a new SequentialContainer is initialized and returned) \n\n# if a SequentialContainer contains BatchNorm layers (regardless of whether they are nested somewhere in a submodule or not), \n# the mode of all these layers at once can be switched as follows\njulia> trainmode!(m)\njulia> testmode!(m)\n\n# if a SequentialContainer contains layers with trainable parameters/weights (what is hopefully in nearly all situations the case),\n# regardless of whether they are nested somewhere in a submodule or not, the gradients of all these layers at once can be reset as follows\njulia> zero_gradients(m)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.GraphContainer","page":"Reference","title":"GradValley.Layers.GraphContainer","text":"GraphContainer(forward_pass::Function, layer_stack::Vector{<: Any})\n\nA computational graph container (recommended method for building models). A GraphContainer can take a function representing the forward pass of a model and a vector of layers or other containers (submodules). While forward-pass, a tracked version of the given inputs are passed through the given forward pass function and the output will be returned. During forward pass, the computational graph is build by a function overload based automatic differentiation system (AD). During backward pass, this computational graph  is used to compute the gradients.\n\nnote: Note\nYou can use a GraphContainer in a SequentialContainer (and vice versa). You can also use a GraphContainer in a GraphContainer (nesting allowed).\n\nwarning: Warning\nNote that the GraphContainer is an experimental feature. The behavior of this module could change dramatically in the future. Using this module can may cause problems.\n\nArguments\n\nforward_pass::Function: the function representing the forward pass of a model\nlayer_stack::Vector{<: Any}: the vector containing the layers (or submodules, so other Containers), the order doesn't matter\n\nGuidelines\n\nGradValley has its own little, rudimentary function overload based automatic differentiation system based on ChainRules.jl. It was designed to allow simple modifications of a normal sequential signal flow, which is the basis of most neural networks.  For example, to be able to implement ResNet's residual connections. So it represents an alternative to data flow layers known from other Deep Learning packages. In a way, it is similar to the forward function known from every PyTorch model. Since the AD does not offer that much functionality at this point in time, the following guidelines must be observed:\n\nThe forward pass function must take at least two arguments. The first is the vector containing the layers (which was passed to GraphContainer at initialization). The following arguments (the last could also be a Vararg argument) are the data inputs.\nThe forward pass function must be written generically enough to accept arrays of type T<:AbstractArray or numbers of type T<:Real as input (starting with the second argument).\nArray inputs that are being differentiated cannot be mutated.\nThe initialization of new arrays (for example with zeros or rand) and their use in mix with the inputs passed to the forward function is not allowed.\nAvoid dot syntax in most cases, there only exist a few differentiation rules for the most basic vectorized operators (.+, .-, .*, ./, .^).\n\nExamples\n\n# a simple chain of fully connected layers (equivalent to the first example of SequentialContainer)\njulia> layers = [Fc(1000, 500), Fc(500, 250), Fc(250, 125)]\njulia> function forward_pass(layers::Vector, input::AbstractArray)\n           fc_1, fc_2, fc_3 = layers\n           output = forward(fc_1, input)\n           output = forward(fc_2, output)\n           output = forward(fc_3, output)\n           return output\n       end\njulia> m = GraphContainer(forward_pass, layers)\n# computing the output of the module (with random inputs)\njulia> input = rand(1000, 32)\njulia> output = forward(m, input)\n\n# a more complicated example: implementation of an inverted residual block\njulia> layers = [Conv(16, 64, (1, 1), activation_function=\"relu\"), \n                 Conv(64, 64, (3, 3), padding=(1, 1), groups=64, activation_function=\"relu\"), # depthwise-conv layer because groups==in_channels\n                 Conv(64, 16, (1, 1), activation_function=\"relu\")]\njulia> function forward_pass(layers::Vector, input::AbstractArray)\n           conv_1, depthwise_conv, conv_2 = layers\n           output = forward(conv_1, input)\n           output = forward(depthwise_conv, output)\n           output = forward(conv_2, output)\n           output = output + input # residual/skipped connection\n           return output\n       end\njulia> m = GraphContainer(forward_pass, layers)\n# computing the output of the module (with random inputs)\njulia> input = rand(50, 50, 16, 32)\njulia> output = forward(m, input)\n\n# a simple example with a polynomial, just to show that it is possible to use the GraphContainer like an automatic differentiation (AD) tool \njulia> f(layers, x) = 0.5x^3 - 2x^2 + 10\njulia> df(x) = 1.5x^2 - 4x # checking the result of the AD with this manually written derivation \njulia> m = GraphContainer(f, [])\njulia> y = forward(m, 3)\njulia> dydx = backward(m, 1) # in this case, no loss function was used, so we have no gradient information, therefore we use 1 as the so-called seed\n1-element Vector{Float64}:\n 1.5\njulia> manual_dydx = df(3)\n1.5\njulia> isapprox(dydx[1], manual_dydx)\ntrue\n\n# if a GraphContainer contains BatchNorm layers (regardless of whether they are nested somewhere in a submodule or not), \n# the mode of all these layers at once can be switched as follows\njulia> trainmode!(m)\njulia> testmode!(m)\n\n# if a GraphContainer contains layers with trainable parameters/weights (what is hopefully in nearly all situations the case),\n# regardless of whether they are nested somewhere in a submodule or not, the gradients of all these layers at once can be reset as follows\njulia> zero_gradients(m)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.summarize_model","page":"Reference","title":"GradValley.Layers.summarize_model","text":"summarize_model(container::AbstractContainer)\n\nReturn a string (and the total number of parameters) intended for printing with an overview of the model  (currently doesn't show an visualization of the computational graph) and its number of parameters.\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Layers.module_to_eltype_device!","page":"Reference","title":"GradValley.Layers.module_to_eltype_device!","text":"module_to_eltype_device!(layer::AbstractLayer; element_type::DataType=Float32, device::AbstractString=\"cpu\")\n\nConvert the parameters of a container or layer to a different element type and move the parameters to the specified device.\n\nArguments\n\nlayer::AbstractLayer: the layer or container (often just the entire model) holding the parameters to be changed\nelement_type::DataType=Float32: the element type into which the parameters will be converted to\ndevice::AbstractString=\"cpu\": the device to which the parameters will be moved, can be \"cpu\" or \"gpu\" (only CUDA is supported)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Layers.clean_model_from_backward_information!","page":"Reference","title":"GradValley.Layers.clean_model_from_backward_information!","text":"clean_model_from_backward_information!(model::AbstractContainer)\n\nClean a container from backward pass information (e.g. computational graph). It is recommended to run this function on a model which should be saved to a file. ```\n\n\n\n\n\n","category":"function"},{"location":"reference/#Forward-and-Backward-Pass","page":"Reference","title":"Forward- and Backward-Pass","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"forward\r\nbackward","category":"page"},{"location":"reference/#GradValley.Layers.forward","page":"Reference","title":"GradValley.Layers.forward","text":"forward(layer, input::AbstractArray{T, N}) where {T, N}\n\nThe forward function for computing the output of a module. For every layer/container, an individual method exists. However, all these methods work exactly the same. They all take the layer/container as the first argument and the input data as the second argument. The output is returned. \n\nExamples\n\n# define some layers and containers\njulia> layer = Conv(3, 6, (5, 5))\njulia> container = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# create some random input data\njulia> layer_input = rand(50, 50, 3, 32)\njulia> container_input = rand(1000, 32)\n# compute the output of the modules\njulia> layer_output = forward(layer, layer_input)\njulia> container_output = forward(container, container_input)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Layers.backward","page":"Reference","title":"GradValley.Layers.backward","text":"backward(sc::SequentialContainer, derivative_loss::Union{AbstractArray{T, N}, Real}) where {T, N}\n\nThe backward function for computing the gradients for a SequentialContainer (highly recommend for model building). The function takes the container (so mostly the whole model) as the first argument and the derivative of the loss as the second argument. No gradients are returned, they are just saved in the layers the container contains.\n\nwarning: Warning\nCalling backward multiple times can have serious consequences. Gradients are added (accumulated) by convention, so calling backward multiple times after the corresponding forward call, the gradients of the weights AND the returned gradient w.r.t. the input are added up (accumulated)!  So even the gradient returned by the backward call doesn't stay the same when calling backward multiple times after the forward call. Note that the gradients of the weights can be reset by zero_gradients but the gradient w.r.t. to the input of a container cannot be reset (except of course by another forward call).\n\nExamples\n\n# define a model\njulia> m = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# compute the output of the model (with random inputs)\njulia> output = forward(m, rand(1000, 32))\n# use a loss function (with random data as target values) and save the derivative of the loss\njulia> loss, derivative_loss = mse_loss(output, rand(125, 32)) # note that GradValley.Optimization.mse_loss must be imported\n# before the gradients are recalculated, the old gradients should always be reset first\nzero_gradients(m)\n# backpropagation \njulia> backward(model, derivative_loss)\n\n\n\n\n\nbackward(grc::GraphContainer, derivative_loss::Union{AbstractArray{T, N}, Real}) where {T, N}\n\nThe backward function for computing the gradients for a GraphContainer (recommend for model building). The function takes the container (so mostly the whole model) as the first argument and the derivative of the loss as the second argument. The gradients to the input arguments are returned (in a vector, in the same order as the inputs were passed to the forward function).\n\nwarning: Warning\nCalling backward multiple times can have serious consequences. Gradients are added (accumulated) by convention, so calling backward multiple times after the corresponding forward call, the gradients of the weights AND the returned gradients w.r.t. the inputs are added up (accumulated)!  So even the gradients returned by the backward call doesn't stay the same when calling backward multiple times after the forward call.  Note that the gradients of the weights can be reset by zero_gradients but the gradients w.r.t. to the inputs of a container cannot be reset (except of course by another forward call).\n\nExamples\n\n# define a model\njulia> layers = [Fc(1000, 500), Fc(500, 250), Fc(250, 125)]\njulia> function forward_pass(layers::Vector, input::AbstractArray)\n           fc_1, fc_2, fc_3 = layers\n           output = forward(fc_1, input)\n           output = forward(fc_2, output)\n           output = forward(fc_3, output)\n           return output\n       end\njulia> m = GraphContainer(forward_pass, layers)\n# compute the output of the model (with random inputs)\njulia> input = rand(1000, 32)\njulia> output = forward(m, input)\n# use a loss function (with random data as target values) and save the derivative of the loss\njulia> loss, derivative_loss = mse_loss(output, rand(125, 32)) # note that GradValley.Optimization.mse_loss must be imported\n# before the gradients are (re)calculated, the old gradients should always be reset first\nzero_gradients(m)\n# backpropagation \njulia> input_gradients = backward(model, derivative_loss) # input_gradients is a vector of length 1 because we only passed one input to the forward function\njulia> input_gradient = input_gradients[1] # input_gradient is the gradient w.r.t to the single input\n\n\n\n\n\n","category":"function"},{"location":"reference/#Reset/zero-gradients","page":"Reference","title":"Reset/zero gradients","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"zero_gradients","category":"page"},{"location":"reference/#GradValley.Layers.zero_gradients","page":"Reference","title":"GradValley.Layers.zero_gradients","text":"zero_gradients(layer_or_container)\n\nResets the gradients of a layer or a container (any kind of module with trainable parameters). \n\nThere only exists methods for layers with parameters, however, if a container without layers with trainable parameters is given, NO error will be thrown. So if the given container contains layers with trainable parameters/weights, regardless of whether they are nested somewhere in a submodule or not,  the gradients of all these layers at once will be reset.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Training-mode/test-mode","page":"Reference","title":"Training mode/test mode","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"trainmode!\r\ntestmode!","category":"page"},{"location":"reference/#GradValley.Layers.trainmode!","page":"Reference","title":"GradValley.Layers.trainmode!","text":"trainmode!(batch_norm_layer_or_container)\n\nSwitches the mode of the given batch normalization layer or container to training mode. See Normalization\n\nIf the given container contains batch normalization layers (regardless of whether they are nested somewhere in a submodule or not),  the mode of all these layers at once will be switched to training mode.\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Layers.testmode!","page":"Reference","title":"GradValley.Layers.testmode!","text":"testmode!(batch_norm_layer_or_container)\n\nSwitches the mode of the given batch normalization layer or container to test mode. See Normalization\n\nIf the given container contains batch normalization layers (regardless of whether they are nested somewhere in a submodule or not),  the mode of all these layers at once will be switched to test mode.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convolution","page":"Reference","title":"Convolution","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Conv\r\nConvTranspose","category":"page"},{"location":"reference/#GradValley.Layers.Conv","page":"Reference","title":"GradValley.Layers.Conv","text":"Conv(in_channels::Int, out_channels::Int, kernel_size::NTuple{2, Int}; stride::NTuple{2, Int}=(1, 1), padding::NTuple{2, Int}=(0, 0), dilation::NTuple{2, Int}=(1, 1), groups::Int=1, activation_function::Union{Nothing, AbstractString}=nothing, init_mode::AbstractString=\"default_uniform\", use_bias::Bool=true)\n\nA convolution layer. Apply a 2D convolution over an input signal with additional batch and channel dimensions.\n\nArguments\n\nin_channels::Int: the number of channels in the input image\nout_channels::Int: the number of channels produced by the convolution\nkernel_size::NTuple{2, Int}: the size of the convolving kernel\nstride::NTuple{2, Int}=(1, 1): the stride of the convolution\npadding::NTuple{2, Int}=(0, 0): the zero padding added to all four sides of the input\ndilation::NTuple{2, Int}=(1, 1): the spacing between kernel elements\ngroups::Int=1: the number of blocked connections from input channels to output channels (in-channels and out-channels must both be divisible by groups)\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the convolution \ninit_mode::AbstractString=\"default_uniform\": the initialization mode of the weights   (can be \"default_uniform\", \"default\", \"kaiming_uniform\", \"kaiming\", \"xavier_uniform\" or \"xavier\")\nuse_bias::Bool=true: if true, adds a learnable bias to the output\n\nShapes\n\nInput: (W_in H_in C_in N)\nWeight: (W_w H_w fracC_ingroups C_out)\nBias: (C_out )\nOutput: (W_out H_out C_out N)\nH_out = fracH_in + 2 cdot padding1 - dilation1 cdot (H_w - 1) - 1stride1 + 1\nW_out = fracW_in + 2 cdot padding2 - dilation2 cdot (W_w - 1) - 1stride2 + 1\n\nUseful Fields/Variables\n\nweight::AbstractArray{<: Real, 4}: the learnable weight of the layer\nbias::AbstractVector{<: Real}: the learnable bias of the layer (used when use_bias=true)\nweight_gradient::AbstractArray{<: Real, 4}: the current gradient of the weight/kernel\nbias_gradient::AbstractVector{<: Real}: the current gradient of the bias\n\nDefinition\n\nFor one group, a multichannel 2D convolution (disregarding batch dimension and activation function) can be described as:\n\no_c_out y_out x_out = big(sum_c_in=1^C_insum_y_w=1^H_wsum_x_w=1^W_w i_c_in y_in x_in cdot w_c_out c_in y_w x_wbig) + b_c_out, where\ny_in = y_out + (stride1 - 1) cdot (y_out - 1) + (y_w - 1) cdot dilation1\nx_in = x_out + (stride2 - 1) cdot (x_out - 1) + (x_w - 1) cdot dilation2\n\nO is the output array, I the input array, W the weight array and B the bias array.\n\nExamples\n\n# square kernels and fully default values of keyword arguments\njulia> m = Conv(3, 6, (5, 5))\n# non-square kernels and unequal stride and with padding as well as specified weight initialization mode\n# (init_mode=\"kaiming\" stands for kaiming weight initialization with normally distributed values)\njulia> m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(2, 1))\n# non-square kernels and unequal stride and with padding, dilation and 3 groups\n# (because groups=in_channels and out_channles is divisible by groups, it is even a depthwise convolution)\njulia> m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), groups=3)\n# computing the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.ConvTranspose","page":"Reference","title":"GradValley.Layers.ConvTranspose","text":"ConvTranspose(in_channels::Int, out_channels::Int, kernel_size::NTuple{2, Int}; stride::NTuple{2, Int}=(1, 1), padding::NTuple{2, Int}=(0, 0), output_padding::NTuple{2, Int}=(0, 0), dilation::NTuple{2, Int}=(1, 1), groups::Int=1, activation_function::Union{Nothing, AbstractString}=nothing, init_mode::AbstractString=\"default_uniform\", use_bias::Bool=true)\n\nA transpose convolution layer (also known as fractionally-strided convolution or deconvolution). Apply a 2D transposed convolution over an input signal with additional batch and channel dimensions.\n\nArguments\n\nin_channels::Int: the number of channels in the input image\nout_channels::Int: the number of channels produced by the convolution\nkernel_size::NTuple{2, Int}: the size of the convolving kernel\nstride::NTuple{2, Int}=(1, 1): the stride of the convolution\npadding::NTuple{2, Int}=(0, 0): because transposed convolution can be seen as a partly (not true) inverse of convolution, padding means is this case to cut off the desired number of pixels on each side (instead of adding pixels)\noutput_padding::NTuple{2, Int}=(0, 0): additional size added to one side of each dimension in the output shape (note that output_padding is only used to calculate the output shape, but does not actually add zero-padding to the output)\ndilation::NTuple{2, Int}=(1, 1): the spacing between kernel elements\ngroups::Int=1: the number of blocked connections from input channels to output channels (in-channels and out-channels must both be divisible by groups)\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the convolution \ninit_mode::AbstractString=\"default_uniform\": the initialization mode of the weights   (can be \"default_uniform\", \"default\", \"kaiming_uniform\", \"kaiming\", \"xavier_uniform\" or \"xavier\")\nuse_bias::Bool=true: if true, adds a learnable bias to the output\n\nShapes\n\nInput: ( W_in H_in C_in N)\nWeight: (W_w H_w fracC_outgroups C_in)\nBias: (C_out )\nOutput: (W_out H_out C_out N), where\nH_out = (H_in - 1) cdot stride1 - 2 cdot padding1 + dilation1 cdot (H_w - 1) + output_padding1 + 1\nW_out = (W_in - 1) cdot stride2 - 2 cdot padding2 + dilation2 cdot (W_w - 1) + output_padding2 + 1\n\nUseful Fields/Variables\n\nweight::AbstractArray{<: Real, 4}: the learnable weight of the layer\nbias::AbstractVector{<: Real}: the learnable bias of the layer (used when use_bias=true)\nweight_gradient::AbstractArray{<: Real, 4}: the current gradient of the weight/kernel\nbias_gradient::Vector{<: Real}: the current gradient of the bias\n\nDefinition\n\nA transposed convolution can be seen as the gradient of a normal convolution with respect to its input.  The forward pass of a transposed convolution is the backward pass of a normal convolution, so the forward pass of a normal convolution becomes the backward pass of a transposed convolution (with respect to its input).  For more detailed information, you can look at the source code of (transposed) convolution. A nice looking visualization of (transposed) convolution can be found here.\n\nExamples\n\n# square kernels and fully default values of keyword arguments\njulia> m = ConvTranspose(6, 3, (5, 5))\n# upsampling an output from normal convolution like in GANS, Unet, etc.\njulia> input = forward(Conv(3, 6, (5, 5)), rand(50, 50, 3, 32))\njulia> output = forward(m, input)\n# the size of the output of the transposed convolution is equal to the size of the original input of the normal convolution\njulia> size(output)\n(50, 50, 3, 32)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pooling","page":"Reference","title":"Pooling","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"MaxPool\r\nAvgPool\r\nAdaptiveMaxPool\r\nAdaptiveAvgPool","category":"page"},{"location":"reference/#GradValley.Layers.MaxPool","page":"Reference","title":"GradValley.Layers.MaxPool","text":"MaxPool(kernel_size::NTuple{2, Int}; stride::NTuple{2, Int}=kernel_size, padding::NTuple{2, Int}=(0, 0), dilation::NTuple{2, Int}=(1, 1), activation_function::Union{Nothing, AbstractString}=nothing)\n\nA maximum pooling layer. Apply a 2D maximum pooling over an input signal with additional batch and channel dimensions.\n\nArguments\n\nkernel_size::NTuple{2, Int}: the size of the window to take the maximum over\nstride::NTuple{2, Int}=kernel_size: the stride of the window\npadding::NTuple{2, Int}=(0, 0): the zero padding added to all four sides of the input\ndilation::NTuple{2, Int}=(1, 1): the spacing between the window elements\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the pooling\n\nShapes\n\nInput: (W_in H_in C N)\nOutput: (W_out H_out C N)\nH_out = fracH_in + 2 cdot padding1 - dilation1 cdot (H_w - 1) - 1stride1 + 1\nW_out = fracW_in + 2 cdot padding2 - dilation2 cdot (W_w - 1) - 1stride2 + 1\n\nDefinition\n\nA multichannel 2D maximum pooling (disregarding batch dimension and activation function) can be described as:\n\nbeginalign*\no_c y_out x_out = max\n_y_w = 1  kernel_size1  x_w = 1  kernel_size2\ni_c y_in x_in\nendalign*\n\nWhere\n\ny_in = y_out + (stride1 - 1) cdot (y_out - 1) + (y_w - 1) cdot dilation1\nx_in = x_out + (stride2 - 1) cdot (x_out - 1) + (x_w - 1) cdot dilation2\n\nO is the output array and I the input array.\n\nExamples\n\n# pooling of square window of size=(3, 3) and automatically selected stride\njulia> m = MaxPool((3, 3))\n# pooling of non-square window with custom stride and padding\njulia> m = MaxPool((3, 2), stride=(2, 1), padding=(1, 1))\n# computing the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.AvgPool","page":"Reference","title":"GradValley.Layers.AvgPool","text":"AvgPool(kernel_size::NTuple{2, Int}; stride::NTuple{2, Int}=kernel_size, padding::NTuple{2, Int}=(0, 0), dilation::NTuple{2, Int}=(1, 1), activation_function::Union{Nothing, AbstractString}=nothing)\n\nAn average pooling layer. Apply a 2D average pooling over an input signal with additional batch and channel dimensions.\n\nArguments\n\nkernel_size::NTuple{2, Int}: the size of the window to take the average over\nstride::NTuple{2, Int}=kernel_size: the stride of the window\npadding::NTuple{2, Int}=(0, 0): the zero padding added to all four sides of the input\ndilation::NTuple{2, Int}=(1, 1): the spacing between the window elements\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the pooling\n\nShapes\n\nInput: (W_in H_in C N)\nOutput: (W_out H_out C N)\nH_out = fracH_in + 2 cdot padding1 - dilation1 cdot (H_w - 1) - 1stride1 + 1\nW_out = fracW_in + 2 cdot padding2 - dilation2 cdot (W_w - 1) - 1stride2 + 1\n\nDefinition\n\nA multichannel 2D average pooling (disregarding batch dimension and activation function) can be described as:\n\no_c y_out x_out = frac1kernel_size1 cdot kernel_size2 sum_i=1^kernel_size1sum_j=1^kernel_size2 i_c y_in x_in, where\ny_in = y_out + (stride1 - 1) cdot (y_out - 1) + (y_w - 1) cdot dilation1\nx_in = x_out + (stride2 - 1) cdot (x_out - 1) + (x_w - 1) cdot dilation2\n\nO is the output array and I the input array.\n\nExamples\n\n# pooling of square window of size=(3, 3) and automatically selected stride\njulia> m = AvgPool((3, 3))\n# pooling of non-square window with custom stride and padding\njulia> m = AvgPool((3, 2), stride=(2, 1), padding=(1, 1))\n# computing the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.AdaptiveMaxPool","page":"Reference","title":"GradValley.Layers.AdaptiveMaxPool","text":"AdaptiveMaxPool(output_size::NTuple{2, Int}; activation_function::Union{Nothing, AbstractString}=nothing)\n\nAn adaptive maximum pooling layer. Apply a 2D adaptive maximum pooling over an input signal with additional batch and channel dimensions. For any input size, the spatial size of the output is always equal to the specified output_size.\n\nArguments\n\noutput_size::NTuple{2, Int}: the target output size of the image (can even be larger than the input size) of the form (H_out W_out)\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the pooling\n\nShapes\n\nInput: (W_in H_in C N)\nOutput: (W_out H_out C N), where (H_out W_out) = output_size\n\nDefinition\n\nIn some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard maximum pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The function get_in_indices(in_len, out_len) in  gv_functional.jl (line 68 - 85) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at MaxPool with the difference that the indices y_in and x_in  have already been calculated beforehand.\n\nExamples\n\n# target output size of 5x5\njulia> m = AdaptiveMaxPool((5, 5))\n# computing the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Layers.AdaptiveAvgPool","page":"Reference","title":"GradValley.Layers.AdaptiveAvgPool","text":"AdaptiveAvgPool(output_size::NTuple{2, Int}; activation_function::Union{Nothing, AbstractString}=nothing)\n\nAn adaptive average pooling layer. Apply a 2D adaptive average pooling over an input signal with additional batch and channel dimensions. For any input size, the size of the output is always equal to the specified output_size.\n\nArguments\n\noutput_size::NTuple{2, Int}: the target output size of the image (can even be larger than the input size) of the form (H_out W_out)\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output after the pooling\n\nShapes\n\nInput: (W_in H_in C N)\nOutput: (W_out H_out C N), where (H_out W_out) = output_size\n\nDefinition\n\nIn some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard average pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The function get_in_indices(in_len, out_len) in  gv_functional.jl (line 68 - 85) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at AvgPool with the difference that the indices y_in and x_in  have already been calculated beforehand.\n\nExamples\n\n# target output size of 5x5\njulia> m = AdaptiveAvgPool((5, 5))\n# computing the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Fully-connected","page":"Reference","title":"Fully connected","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Fc","category":"page"},{"location":"reference/#GradValley.Layers.Fc","page":"Reference","title":"GradValley.Layers.Fc","text":"Fc(in_features::Int, out_features::Int; activation_function::Union{Nothing, AbstractString}=nothing, init_mode::AbstractString=\"default_uniform\", use_bias::Bool=true)\n\nA fully connected layer (sometimes also known as dense or linear). Apply a linear transformation (matrix multiplication) to the input signal with additional batch dimension.\n\nArguments\n\nin_features::Int: the size of each input sample (\"number of input neurons\")\nout_features::Int: the size of each output sample (\"number of output neurons\")\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output\ninit_mode::AbstractString=\"default_uniform\": the initialization mode of the weights   (can be \"default_uniform\", \"default\", \"kaiming_uniform\", \"kaiming\", \"xavier_uniform\" or \"xavier\")\n\nuse_bias::Bool=true: if true, adds a learnable bias to the output\n\nShapes\n\nInput: (in_features N)\nWeight: (out_features in_features)\nBias: (out_features )\nOutput: (out_features N)\n\nUseful Fields/Variables\n\nweight::AbstractArray{<: Real, 2}: the learnable weights of the layer\nbias::AbstractVector{<: Real}: the learnable bias of the layer (used when use_bias=true)\nweight_gradient::AbstractArray{<: Real, 2}: the current gradients of the weights\nbias_gradient::AbstractVector{<: Real}: the current gradients of the bias\n\nDefinition\n\nThe forward pass of a fully connected layer is given by the matrix multiplication between the weight matrix and the input vector  (disregarding batch dimension and activation function):\n\nO = WI + B\n\nThis operation can also be described by:\n\no_j = big(sum_k=1^in_features w_jk cdot i_kbig) + b_j\n\nO is the output vector, I the input vector, W the weight matrix and B the bias vector. Visually interpreted, it means that each input neuron i is weighted with the corresponding weight w connecting the input neuron  to the output neuron o where all the incoming signals are summed up.\n\nExamples\n\n# a fully connected layer with 784 input features and 120 output features\njulia> m = Fc(784, 120)\n# computing the output of the layer (with random inputs)\njulia> input = rand(784, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Identity","page":"Reference","title":"Identity","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Identity","category":"page"},{"location":"reference/#GradValley.Layers.Identity","page":"Reference","title":"GradValley.Layers.Identity","text":"Identity(; activation_function::Union{Nothing, AbstractString}=nothing)\n\nAn identity layer (can be used as an activation function layer). If no activation function is used, this layer does not change the signal in any way. However, if an activation function is used, the activation function will be applied to the input element-wise. \n\ntip: Tip\nThis layer is helpful to apply an element-wise activation independent of a \"normal\" computational layer.\n\nArguments\n\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the inputs\n\nShapes\n\nInput: (*), where * means any number of dimensions\nOutput: (*) (same shape as input)\n\nDefinition\n\nA placeholder identity operator, except the optional activation function, the input signal is not changed in any way. If an activation function is used, the activation function will be applied to the input element-wise. \n\nExamples\n\n# an independent relu activation\njulia> m = Identity(activation_function=\"relu\")\n# computing the output of the layer (with random inputs)\njulia> input = rand(10, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Normalization","page":"Reference","title":"Normalization","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"BatchNorm","category":"page"},{"location":"reference/#GradValley.Layers.BatchNorm","page":"Reference","title":"GradValley.Layers.BatchNorm","text":"BatchNorm(num_features::Int; epsilon::Real=1e-05, momentum::Real=0.1, affine::Bool=true, track_running_stats::Bool=true, activation_function::Union{Nothing, AbstractString}=nothing)\n\nA batch normalization layer. Apply a batch normalization over a 4D input signal (a mini-batch of 2D inputs with additional channel dimension).\n\nThis layer has two modes: training mode and test mode. If track_running_stats::Bool=true, this layer behaves differently in the two modes. During training, this layer always uses the currently calculated batch statistics. If track_running_stats::Bool=true, the running mean and variance are tracked during training and will be used while testing. If track_running_stats::Bool=false, even in test mode, the currently calculated batch statistics are used. The mode can be switched with trainmode! or testmode! respectively. The training mode is active by default.\n\nArguments\n\nnum_features::Int: the number of channels\nepsilon::Real=1e-05: a value added to the denominator for numerical stability\nmomentum::Real=0.1: the value used for the running mean and running variance computation\naffine::Bool=true: if true, this layer uses learnable affine parameters/weights (gamma and beta)\ntrack_running_stats::Bool=true: if true, this layer tracks the running mean and variance during training and will use them for testing/evaluation, if false, such statistics are not tracked and, even in test mode, the batch statistics are always recalculated for each new input\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output\n\nShapes\n\nInput: (W H C N)\ngamma Weight, beta Bias: (C )\nRunning Mean/Variance: (C )\nOutput: (W H C N) (same shape as input)\n\nUseful Fields/Variables\n\nWeights (used if affine::Bool=true)\n\nweight::AbstractVector{<: Real}: gamma, a learnabele parameter for each channel, initialized with ones\nbias::AbstractVector{<: Real}: beta, a learnabele parameter for each channel, initialized with zeros\n\nGradients of weights (used if affine::Bool=true)\n\nweight_gradient::AbstractVector{<: Real}: the gradient of gamma\nbias_gradient::AbstractVector{<: Real}: the gradient of beta\n\nRunning statistics (used if rack_running_stats::Bool=true)\n\nrunning_mean::AbstractVector{<: Real}: the continuously updated batch statistics of the mean\nrunning_variance::AbstractVector{<: Real}: the continuously updated batch statistics of the variance\n\nDefinition\n\nA batch normalization operation can be described as: For input values over a mini-batch: mathcalB = x_1 x_2  x_n\n\nbeginalign*\ny_i = fracx_i - overlinemathcalBsqrtVar(mathcalB) + epsilon cdot gamma + beta\nendalign*\n\nWhere y_i is an output value and x_i an input value. overlinemathcalB is the mean of the input values in mathcalB and Var(mathcalB)  is the variance of the input values in mathcalB. Note that this definition is fairly general and not specified to 4D inputs. In this case, the input values of mathcalB are taken for each channel individually.  So the mean and variance are calculated per channel over the mini-batch.\n\nThe update rule for the running statistics (running mean/variance) is:\n\nbeginalign*\nhatx_new = (1 - momentum) cdot hatx + momentum cdot x\nendalign*\n\nWhere hatx is the estimated statistic and x is the new observed value. So hatx_new is the new, updated estimated statistic.\n\nExamples\n\n# a batch normalization layer (3 channels) with learnabel parameters and continuously updated batch statistics for evaluation\njulia> m = BatchNorm(3)\n# the mode can be switched with trainmode! or testmode!\njulia> trainmode!(m)\njulia> testmode!(m)\n# compute the output of the layer (with random inputs)\njulia> input = rand(50, 50, 3, 32)\njulia> output = forward(m, input)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Reshape-/-Flatten","page":"Reference","title":"Reshape / Flatten","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Reshape","category":"page"},{"location":"reference/#GradValley.Layers.Reshape","page":"Reference","title":"GradValley.Layers.Reshape","text":"Reshape(out_shape; activation_function::Union{Nothing, AbstractString}=nothing)\n\nA reshape layer (probably mostly used as a flatten layer). Reshape the input signal (effects all dimensions except the batch dimension).\n\nArguments\n\nout_shape: the target output size (the output has the same data as the input and must have the same number of elements)\nactivation_function::Union{Nothing, AbstractString}=nothing: the element-wise activation function which will be applied to the output\n\nShapes\n\nInput: (* N), where * means any number of dimensions\nOutput: (out_shape N)\n\nDefinition\n\nThis layer uses the standard reshape function inbuilt in Julia.\n\nExamples\n\n# flatten the input of size 28*28*1 to a vector of length 784 (each plus batch dimension of course)\njulia> m = Reshape((784, ))\n# computing the output of the layer (with random inputs)\njulia> input = rand(28, 28, 1, 32)\njulia> output = forward(m, input)\njulia> size(output) # specified size plus batch dimension\n(784, 32)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Activation-functions","page":"Reference","title":"Activation functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Almost every layer constructor has the keyword argument activation_function specifying the element-wise activation function. activation_function can be nothing or a string. nothing means no activation function, a string gives the name of the activation. Because softmax isn’t a simple element-wise activation function like the most activations, Softmax has it’s own layer. The following element-wise activation functions are currently implemented:  ","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"\"relu\": applies the element-wise relu activation (recified linear unit): f(x) = max(0 x)\n\"sigmoid\": applies the element-wise sigmoid acivation (logistic curve): f(x) = frac11 + e^-x\n\"tanh\": applies the element-wise tanh activation (tangens hyperbolicus): f(x) = tanh(x) = frace^x - e^-xe^x + e^-x\n\"leaky_relu\": applies a element-wise leaky relu activation with a negative slope of 0.01: f(x) = begincasesx textif x geq 0textnormal001 times x textif x  0endcases\n\"leaky_relu:$(negative_slope)\": applies a element-wise leaky relu activation with a negative slope of negative_slope (e.g. \"leaky_relu:0.2\" means a leaky relu activation with a negative slope of 0.2): f(x) = begincasesx textif x geq 0textnormalnegative_slope times x textif x  0endcases","category":"page"},{"location":"reference/#Special-activation-functions","page":"Reference","title":"Special activation functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Softmax","category":"page"},{"location":"reference/#GradValley.Layers.Softmax","page":"Reference","title":"GradValley.Layers.Softmax","text":"Softmax(; dims=1)\n\nA softmax activation function layer (probably mostly used at the \"end\" of a classifier model). Apply the softmax function to an n-dimensional input array. The softmax will be computed along the given dimensions (dims), so every slice along these dimensions will sum to 1.\n\nnote: Note\nNote that this is the only activation function in form of a layer. All other activation functions can be used with the activation_function::AbstractString keyword argument nearly every layer provides. All the activation functions which can be used that way are simple element-wise activation functions. Softmax is currently the only non-element-wise activation function. Besides, it is very important to be able to select a specific dimension along the  softmax should be computed. That would also not work well with the use of simple keyword argument taking only a string which is the name of the function.\n\nArguments\n\ndims=1: the dimensions along the softmax will be computed (so every slice along these dimensions will sum to 1)\n\nShapes\n\nInput: (*), where * means any number of dimensions\nOutput: (*) (same shape as input)\n\nDefinition\n\nThe softmax function converts a vector of real numbers into a probability distribution. The softmax function is defined as:\n\nbeginalign*\nsoftmax(x_i) = frace^x_isum_je^x_j = fracexp(x_i)sum_jexp(x_j)\nendalign*\n\nWhere X is the input array (slice). Note that the x_j values are taken from each slice individually along the specified dimension. So each slice along the specified dimension will sum to 1. All values in the output are between 0 and 1.\n\nExamples\n\n# the softmax will be computed along the first dimension\njulia> m = Softmax(dims=1)\n# computing the output of the layer \n# (with random input data which could represent a batch of unnormalized output values from a classifier)\njulia> input = rand(10, 32)\njulia> output = forward(m, input)\n# summing up the values in the output along the first dimension result in a batch of 32 ones\njulia> sum(output, dims=1)\n1x32 Matrix{Float64}:\n1.0 1.0 ... 1.0\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Optimization","page":"Reference","title":"GradValley.Optimization","text":"","category":"section"},{"location":"reference/#Optimizers","page":"Reference","title":"Optimizers","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Adam\r\nSGD\r\nMSGD\r\nNesterov","category":"page"},{"location":"reference/#GradValley.Optimization.Adam","page":"Reference","title":"GradValley.Optimization.Adam","text":"Adam(layer_stack::Union{Vector, AbstractContainer}; learning_rate::Real=0.001, beta1::Real=0.9, beta2::Real=0.999, epsilon::Real=1e-08, weight_decay::Real=0, amsgrad::Bool=false, maximize::Bool=false)\n\nImplementation of Adam optimization algorithm (including the optional AMSgrad version of this algorithm and optional weight decay).\n\nArguments\n\nlayer_stack::Union{Vector, SequentialContainer, GraphContainer}: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without any parameters)\nlearning_rate::Real=0.001: the learning rate (shouldn't be 0)\nbeta1::Real=0.9, beta2::Real=0.999: the two coefficients used for computing running averages of gradient and its square\nepsilon::Real=1e-08: value for numerical stability \nweight_decay::Real=0.00: the weight decay (L2 penalty)\namsgrad::Bool=false: use the AMSgrad version of this algorithm\nmaximize::Bool=false: maximize the parameters, instead of minimizing \n\nDefinition\n\nFor example, a definition of this algorithm in pseudocode can be found here.\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# initialize a Adam optimizer with default arguments\njulia> optimizer = Adam(model)\n# create some random input data\njulia> input = rand(32, 1000)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative \njulia> loss, loss_derivative = mse_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n# perform a single optimization step (parameter update)\njulia> step!(optimizer)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Optimization.SGD","page":"Reference","title":"GradValley.Optimization.SGD","text":"SGD(layer_stack::Union{Vector, AbstractContainer}, learning_rate::Real; weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)\n\nImplementation of stochastic gradient descent optimization algorithm (including optional weight decay and dampening).\n\nArguments\n\nlayer_stack::Union{Vector, AbstractContainer}: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without parameters)\nlearning_rate::Real: the learning rate (shouldn't be 0)\nweight_decay::Real=0.00: the weight decay (L2 penalty)\ndampening::Real=0.00: the dampening (normally just for optimizers with momentum, however, can be theoretically used without, in this case acts like: (1 - dampening) cdot learning_rate)\nmaximize::Bool=false: maximize the parameters, instead of minimizing \n\nDefinition\n\nFor example, a definition of this algorithm in pseudocode can be found here. (Note that in this case of a simple SGD with no momentum, the momentum μ is zero in the sense of the mentioned documentation.)\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# initialize a SGD optimizer with learning-rate equal 0.1 and weight decay equal to 0.5 (otherwise default values)\njulia> optimizer = SGD(model, 0.1, weight_decay=0.5)\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative \njulia> loss, loss_derivative = mse_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n# perform a single optimization step (parameter update)\njulia> step!(optimizer)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Optimization.MSGD","page":"Reference","title":"GradValley.Optimization.MSGD","text":"MSGD(layer_stack::Union{Vector, AbstractContainer}, learning_rate::Real; momentum::Real=0.90, weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)\n\nImplementation of stochastic gradient descent with momentum optimization algorithm (including optional weight decay and dampening).\n\nArguments\n\nlayer_stack::Union{Vector, AbstractContainer}: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without any parameters)\nlearning_rate::Real: the learning rate (shouldn't be 0)\nmomentum::Real=0.90: the momentum factor (shouldn't be 0)\nweight_decay::Real=0.00: the weight decay (L2 penalty)\ndampening::Real=0.00: the dampening for the momentum \nmaximize::Bool=false: maximize the parameters, instead of minimizing \n\nDefinition\n\nFor example, a definition of this algorithm in pseudocode can be found here. (Note that in this case of SGD with default momentum, in the sense of the mentioned documentation, the momentum mu isn't zero (mu neq 0) and nesterov is false.)\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# initialize a MSGD optimizer with learning-rate equal 0.1 and momentum equal to 0.75 (otherwise default values)\njulia> optimizer = Nesterov(model, 0.1, momentum=0.75)\n# create some random input data\njulia> input = rand(32, 1000)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative \njulia> loss, loss_derivative = mse_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n# perform a single optimization step (parameter update)\njulia> step!(optimizer)\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradValley.Optimization.Nesterov","page":"Reference","title":"GradValley.Optimization.Nesterov","text":"Nesterov(layer_stack::Union{Vector, AbstractContainer}, learning_rate::Real; momentum::Real=0.90, weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)\n\nImplementation of stochastic gradient descent with nesterov momentum optimization algorithm (including optional weight decay and dampening).\n\nArguments\n\nlayer_stack::Union{Vector, AbstractContainer}: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without any parameters)\nlearning_rate::Real: the learning rate (shouldn't be 0)\nmomentum::Real=0.90: the momentum factor (shouldn't be 0)\nweight_decay::Real=0.00: the weight decay (L2 penalty)\ndampening::Real=0.00: the dampening for the momentum (for true nesterov momentum, dampening must be 0)\nmaximize::Bool=false: maximize the parameters, instead of minimizing \n\nDefinition\n\nFor example, a definition of this algorithm in pseudocode can be found here. (Note that in this case of SGD with nesterov momentum, nesterov is true in the sense of the mentioned documentation.)\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# initialize a Nesterov optimizer with learning-rate equal 0.1 and nesterov momentum equal to 0.8 (otherwise default values)\njulia> optimizer = Nesterov(model, 0.1, momentum=0.8)\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative \njulia> loss, loss_derivative = mse_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n# perform a single optimization step (parameter update)\njulia> step!(optimizer)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Optimization-step-function","page":"Reference","title":"Optimization step function","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"step!","category":"page"},{"location":"reference/#GradValley.Optimization.step!","page":"Reference","title":"GradValley.Optimization.step!","text":"step!(optimizer::Union{SGD, MSGD, Nesterov})\n\nPerform a single optimization step (parameter update) for the given optimizer.\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# initialize an optimizer (which optimizer specifically dosen't matter)\njulia> optimizer = SGD(model, 0.1)\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative \njulia> loss, loss_derivative = mse_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n# perform a single optimization step (parameter update)\njulia> step!(optimizer)\n\n\n\n\n\n","category":"function"},{"location":"reference/#Loss-functions","page":"Reference","title":"Loss functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"mae_loss\r\nmse_loss\r\nbce_loss","category":"page"},{"location":"reference/#GradValley.Optimization.mae_loss","page":"Reference","title":"GradValley.Optimization.mae_loss","text":"mae_loss(prediction::AbstractArray{<: Real, N}, target::AbstractArray{<: Real, N}; reduction_method::Union{AbstractString, Nothing}=\"mean\", return_derivative::Bool=true) where N\n\nCalculate the (mean) absolute error (L1 norm, with optional reduction to a single loss value (mean or sum)) and it's derivative with respect to the prediction input.\n\nArguments\n\nprediction::AbstractArray{<: Real, N}: the prediction of the model of shape (*), where * means any number of dimensions \ntarget::AbstractArray{<: Real, N}: the corresponding target values of shape (*), must have the same shape as the prediction input \nreduction_method::Union{AbstractString, Nothing}=\"mean\": can be \"mean\", \"sum\" or nothing, specifies the reduction method which reduces the element-wise computed loss to a single value\nreturn_derivative::Bool=true: it true, the loss and it's derivative with respect to the prediction input is returned, if false, just the loss will be returned\n\nDefinition\n\nL is the loss value which will be returned. If return_derivative is true, then an array with the same shape as prediction/target is returned as well, it contains the partial derivatives of L w.r.t. to each prediction value: fracpartial Lpartial p_i, where p_i in one prediction value. If reduction_method is nothing, the element-wise computed losses are returned. Note that for reduction_method=nothing, the derivative is just the same as when reduction_method=\"sum\". The element-wise calculation can be defined as (where t_i is one target value and l_i is one loss value): \n\nbeginalign*\nl_i = p_i - t_i\nendalign*\n\nThen, L and fracpartial Lpartial p_i differ a little bit from case to case (n is the number of values in prediction/target):\n\nbeginalign*\nLfracpartial Lpartial p_i = begincasesfrac1nsum_i=1^n l_i fracp_i - t_il_i cdot n textfor reduction_method=meansum_i=1^n l_i fracp_i - t_il_i textfor reduction_method=sumendcases\nendalign*\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative (with default reduction method \"mean\")\njulia> loss, loss_derivative = mae_loss(output, target)\n# computet the gradients \njulia> backward(model, loss_derivative)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Optimization.mse_loss","page":"Reference","title":"GradValley.Optimization.mse_loss","text":"mse_loss(prediction::AbstractArray{<: Real, N}, target::AbstractArray{<: Real, N}; reduction_method::Union{AbstractString, Nothing}=\"mean\", return_derivative::Bool=true) where N\n\nCalculate the (mean) squared error (squared L2 norm, with optional reduction to a single loss value (mean or sum)) and it's derivative with respect to the prediction input.\n\nArguments\n\nprediction::AbstractArray{<: Real, N}: the prediction of the model of shape (*), where * means any number of dimensions \ntarget::AbstractArray{<: Real, N}: the corresponding target values of shape (*), must have the same shape as the prediction input \nreduction_method::Union{AbstractString, Nothing}=\"mean\": can be \"mean\", \"sum\" or nothing, specifies the reduction method which reduces the element-wise computed loss to a single value\nreturn_derivative::Bool=true: it true, the loss and it's derivative with respect to the prediction input is returned, if false, just the loss will be returned\n\nDefinition\n\nL is the loss value which will be returned. If return_derivative is true, then an array with the same shape as prediction/target is returned as well, it contains the partial derivatives of L w.r.t. to each prediction value: fracpartial Lpartial p_i, where p_i in one prediction value. If reduction_method is nothing, the element-wise computed losses are returned. Note that for reduction_method=nothing, the derivative is just the same as when reduction_method=\"sum\". The element-wise calculation can be defined as (where t_i is one target value and l_i is one loss value): \n\nbeginalign*\nl_i = (p_i - t_i)^2\nendalign*\n\nThen, L and fracpartial Lpartial p_i differ a little bit from case to case (n is the number of values in prediction/target):\n\nbeginalign*\nLfracpartial Lpartial p_i = begincasesfrac1nsum_i=1^n l_i frac2n(p_i - t_i)  textfor reduction_method=meansum_i=1^n l_i 2(p_i - t_i) textfor reduction_method=sumendcases\nendalign*\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# compute the loss and it's derivative (with default reduction method \"mean\")\njulia> loss, loss_derivative = mse_loss(output, target)\n# compute the gradients \njulia> backward(model, loss_derivative)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Optimization.bce_loss","page":"Reference","title":"GradValley.Optimization.bce_loss","text":"bce_loss(prediction::AbstractArray{<: Real, N}, target::AbstractArray{<: Real, N}; epsilon::Real=eps(eltype(prediction)), reduction_method::Union{AbstractString, Nothing}=\"mean\", return_derivative::Bool=true) where N\n\nCalculate the binary cross entropy loss (with optional reduction to a single loss value (mean or sum)) and it's derivative with respect to the prediction input.\n\nArguments\n\nprediction::AbstractArray{<: Real, N}: the prediction of the model of shape (*), where * means any number of dimensions \ntarget::AbstractArray{<: Real, N}: the corresponding target values (should be between 0 and 1) of shape (*), must have the same shape as the prediction input \nepsilon::Real=eps(eltype(prediction)): term to avoid infinity\nreduction_method::Union{AbstractString, Nothing}=\"mean\": can be \"mean\", \"sum\" or nothing, specifies the reduction method which reduces the element-wise computed loss to a single value\nreturn_derivative::Bool=true: it true, the loss and it's derivative with respect to the prediction input is returned, if false, just the loss will be returned\n\nDefinition\n\nL is the loss value which will be returned. If return_derivative is true, then an array with the same shape as prediction/target is returned as well, it contains the partial derivatives of L w.r.t. to each prediction value: fracpartial Lpartial p_i, where p_i in one prediction value. If reduction_method is nothing, the element-wise computed losses are returned. Note that for reduction_method=nothing, the derivative is just the same as when reduction_method=\"sum\". w_i is one rescaling weight value. The element-wise calculation can be defined as (where t_i is one target value and l_i is one loss value): \n\nbeginalign*\nl_i = -w_n(t_i cdot log(p_i + epsilon) + (1 - t_i) cdot log(1 - p_i + epsilon))\nendalign*\n\nThen, L and fracpartial Lpartial p_i differ a little bit from case to case (n is the number of values in prediction/target):\n\nbeginalign*\nLfracpartial Lpartial p_i = begincasesfrac1nsum_i=1^n l_i frac1n(frac-w_i t_ip_i - frac-w_i + w_i t_i1 - p_i) textfor reduction_method=meansum_i=1^n l_i frac-w_i t_ip_i - frac-w_i + t_i w_i1 - p_i textfor reduction_method=sumendcases\nendalign*\n\nExamples\n\n# define a model\njulia> model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])\n# create some random input data\njulia> input = rand(1000, 32)\n# compute the output of the model\njulia> output = forward(model, input)\n# generate some random target values \njulia> target = rand(size(output)...)\n# generate some random rescaling weight values (without a batch dimension)\njulia> weight = rand(size(output)[2:end]...)\n# compute the loss and it's derivative (with default reduction method \"mean\")\njulia> loss, loss_derivative = bce_loss(output, target, weight=weight)\n# computet the gradients \njulia> backward(model, loss_derivative)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Functional","page":"Reference","title":"GradValley.Functional","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"GradValley.Functional contains many primitives common for various neuronal networks. Not all of these functions (better said the fewest) are documented because they are mostly used only internally (not by the user).","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"GradValley.Functional.zero_pad_nd\r\nGradValley.Functional.zero_pad_2d","category":"page"},{"location":"reference/#GradValley.Functional.zero_pad_nd","page":"Reference","title":"GradValley.Functional.zero_pad_nd","text":"zero_pad_nd(input::AbstractArray{T, N}, padding::NTuple{N, Int}) where {T, N}\n\nPerform a padding-operation (nd => number of dimensions doesn't matter) as is usual for neural networks: equal padding at each \"end\" of each axis/dimension.\n\nArguments\n\ninput::AbstractArray{T, N}: of shape(d1, d2, ..., dn)\npadding::NTuple{2, Int}: must be always a tuple of length of the number of dimensions of input: (pad-d1, pad-d2, ..., pad-dn)\n\nShape of returned output: (d1 + padding[1] * 2, d2 + padding[2] * 2, ..., dn + padding[n] * 2)\n\n\n\n\n\nzero_pad_nd(input::CuArray{T, N}, padding::NTuple{N, Int}) where {T, N}\n\nPerform a padding-operation (nd => number of dimensions doesn't matter) as is usual for neural networks: equal padding at each \"end\" of each axis/dimension.\n\nArguments\n\ninput::CuArray{T, N}: of shape(d1, d2, ..., dn)\npadding::NTuple{2, Int}: must be always a tuple of length of the number of dimensions of input: (pad-d1, pad-d2, ..., pad-dn)\n\nShape of returned output: (d1 + padding[1] * 2, d2 + padding[2] * 2, ..., dn + padding[n] * 2)\n\n\n\n\n\n","category":"function"},{"location":"reference/#GradValley.Functional.zero_pad_2d","page":"Reference","title":"GradValley.Functional.zero_pad_2d","text":"zero_pad_nd(input::AbstractArray{T, 4}, padding::NTuple{2, Int) where {T}\n\nPerform a padding-operation (2d => 4 dimensions, where the last 2 dimensions will be padded) as is usual for neural networks: equal padding at each \"end\" of each spatial axis/dimension.\n\nArguments\n\ninput::AbstractArray{T, 4}: of shape(d1, d2, d3, d4), d2 is expected to be the height dimension, d1 is expected to be the width dimension\npadding::NTuple{2, Int}: must be always a tuple of length 2: (pad-d2, pad-d1) == (pad-height, pad-width)\n\nShape of returned output: (d1 + padding[2] * 2, d2 + padding[1] * 2, d3, d4)\n\n\n\n\n\nzero_pad_nd(input::CuArray{T, 4}, padding::NTuple{2, Int) where {T}\n\nPerform a padding-operation (2d => 4 dimensions, where the last 2 dimensions will be padded) as is usual for neural networks: equal padding at each \"end\" of each spatial axis/dimension.\n\nArguments\n\ninput::CuArray{T, 4}: of shape(d1, d2, d3, d4), d2 is expected to be the height dimension, d1 is expected to be the width dimension\npadding::NTuple{2, Int}: must be always a tuple of length 2: (pad-d2, pad-d1) == (pad-height, pad-width)\n\nShape of returned output: (d1 + padding[2] * 2, d2 + padding[1] * 2, d3, d4)\n\n\n\n\n\n","category":"function"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the GradValley.jl documentation!","category":"page"},{"location":"","page":"Home","title":"Home","text":"GradValley.jl is a new lightweight package for Deep Learning written in 100% Julia. GradValley offers a high level interface for flexible model building and training. It is independent of other machine learning packages like Flux, Knet, NNlib or NNPACK (see dependencies). It is based on Julia’s standard array type and needs no additional tensor type. To get started, see Installation and Getting Started. After that, you could look at the Tutorials and Examples section. Or directly start using a pre-trained model, for example a pre-trained ResNet.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Because GradValley is just 100% high level Julia code, the implemented backend algorithms powering Deep Learning (e.g. convolution) are pretty nice to read. So if you're looking into how exactly such Deep Learning algorithms work, looking at the source code (and at it's documentation in Reference) could also be a helpful learning resource. See Learning for further learning resources. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis software package and its documentation are in an early stage of development and are therefore still a beta version. If you are missing certain features, see Current Limitations for planned future features, or directly share your ideas in the discussion section of the GitHub repository. This software package and its documentation are currently being continuously adapted and improved.","category":"page"},{"location":"#Why-GradValley.jl","page":"Home","title":"Why GradValley.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Intuitive Model Building: Model building is normally done using Containers. With Containers, large models can be broken down into smaller components (e.g. ResNets in ResBlocks), which in turn can then be easily combined into one large model. See the ResNets example in the Tutorials and Examples section.\nFlexible: Containers behave like layers, so you can use containers in containers in containers... (arbitrary nesting allowed). GraphContainer's automatic differentiation allows defining your own computational graph in a function, which then can be automatically differentiated during backward pass (using reverse mode AD, aka Backpropagation).\nSwitching from Python to Julia: Model building is very similar to other frameworks and and the behavior of the layers is strongly oriented towards PyTorch, e.g. the algorithm behind adaptive pooling. \n100% Julia: Julia's biggest advantage compared to Python is speed. This allows you to easily extend existing Julia packages yourself. Extending python packages is, at least if they use e.g. C code in the backend, much more difficult. \nJulia's environment: The Julia community developed a lot of awesome packages. Julia packages have the advantage that they can be usually always used very well together. For example, take a look at Flux.jl, Plots.jl, MLJ.jl, DifferentialEquations.jl or CUDA.jl.\nWell documented: The documentation aims to provide detailed information about all of GradValley’s functionalities. For example, the documentation of each layer contains e.g. a description, an argument list, a mathematical definition and extensive examples.\nSee for yourself: To get started, see Installation and Getting Started. After that, you could look at the Tutorials and Examples section. Or directly start using a pre-trained model, for exmaple a pre-trained ResNet.","category":"page"},{"location":"#About","page":"Home","title":"About","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A while ago I started looking into machine learning. The topic fascinated me from the beginning, so I wanted to gain a deeper understanding of the way such models work. In my opinion, the best way to do this is to write your own small software package for machine learning and not just blindly use one of the big, established frameworks such as PyTorch or TensorFlow. The Julia programming language was my choice because of its popularity in academia and its very good performance compared to pure Python, which is after all very popular in the world of artificial intelligence. The product of this work is this package called GradValley.jl with which various current neural networks (e.g. CNNs) can be implemented easily and intuitively.","category":"page"},{"location":"#Array-structure-convention","page":"Home","title":"Array structure convention","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The order used in GradValley for processing images (or similar data) is WHCN, where N is the batch dimension, C is the channel dimension, H is the vertical and W is the horizontal size of the image. The batch dimension is always the last. ","category":"page"},{"location":"#Explanation-of-the-name-\"GradValley\"","page":"Home","title":"Explanation of the name \"GradValley\"","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"When optimizing the weights of a machine learning model, an attempt is always made to find the best possible error minimum. The derivatives, i.e. the gradients, of the error function in relation to the weights are required for this. So the goal is to find the \"valley\" of the error using the gradients (\"grad\" stands for gradient). That's why it's called GradValley.","category":"page"},{"location":"#Current-Limitations","page":"Home","title":"Current Limitations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following features are planned and likely to be added in the future:","category":"page"},{"location":"","page":"Home","title":"Home","text":"more predefined activation function, loss functions and optimizers\nfurther performance improvments for the cpu ","category":"page"},{"location":"#GitHub-Repository","page":"Home","title":"GitHub Repository","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the GitHub repository of GradValley.jl, you can find e.g. the source code, the source of this documentation and information about continues testing and it's code coverage. The repo is also a place to ask questions and share your thoughts about this project. Contributing or opening issues is of course also welcome. (This documentation page is also hosted on GitHub using GitHub Pages.)","category":"page"},{"location":"#Questions-and-Discussions","page":"Home","title":"Questions and Discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you have any questions about this software package, please let us know. For example, use the discussion section of the GitHub repository. Or interact with the community on the Julia Discourse Forum (specific domains > Machine Learning), the Julia Slack (channel #machine-learning) or the Julia Zulip (stream #machine-learning). The Julia Discourse Forum is usually the preferred place for asking questions. ","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributors are more than welcome! A proper guide for contributors will be added soon. Normally the rough procedure is as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fork the current-most state of the main branch\nImplement features or changes\nAdd your name to AUTHORS.md\nCreate a pull-request to the repository","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GradValley.jl software package is currently published under the MIT \"Expat\" license. See LICENSE in the GitHub repository for further information.","category":"page"}]
}
