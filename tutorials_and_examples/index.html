<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorials and Examples Â· GradValley.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="GradValley.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="GradValley.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GradValley.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li class="is-active"><a class="tocitem" href>Tutorials and Examples</a><ul class="internal"><li><a class="tocitem" href="#A-LeNet-like-model-for-handwritten-digit-recognition"><span>A LeNet-like model for handwritten digit recognition</span></a></li></ul></li><li><a class="tocitem" href="../(pre-trained)_models/">(Pre-Trained) Models</a></li><li><a class="tocitem" href="../learning/">Learning</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorials and Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorials and Examples</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorials-and-Examples"><a class="docs-heading-anchor" href="#Tutorials-and-Examples">Tutorials and Examples</a><a id="Tutorials-and-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorials-and-Examples" title="Permalink"></a></h1><p>Here, you can find detailed explanations on how to build and train specific models with GradValley.jl.</p><h2 id="A-LeNet-like-model-for-handwritten-digit-recognition"><a class="docs-heading-anchor" href="#A-LeNet-like-model-for-handwritten-digit-recognition">A LeNet-like model for handwritten digit recognition</a><a id="A-LeNet-like-model-for-handwritten-digit-recognition-1"></a><a class="docs-heading-anchor-permalink" href="#A-LeNet-like-model-for-handwritten-digit-recognition" title="Permalink"></a></h2><p>In this tutorial, we will learn the basics of GradValley.jl while building a model for handwritten digit recognition, reaching approximately 99% accuracy on the MNIST-dataset. The whole code at once can be found <a href="https://github.com/jonas208/GradValley.jl/blob/main/tutorials/MNIST_with_LeNet5.jl">here</a>.</p><h3 id="Importing-modules"><a class="docs-heading-anchor" href="#Importing-modules">Importing modules</a><a id="Importing-modules-1"></a><a class="docs-heading-anchor-permalink" href="#Importing-modules" title="Permalink"></a></h3><pre><code class="language-julia hljs">using GradValley # the master module of GradValley.jl
using GradValley.Layers # The &quot;Layers&quot; module provides all the building blocks for creating a model. 
using GradValley.Optimization # The &quot;Optimization&quot; module provides different loss functions and optimizers.</code></pre><h3 id="Using-the-dataset"><a class="docs-heading-anchor" href="#Using-the-dataset">Using the dataset</a><a id="Using-the-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-dataset" title="Permalink"></a></h3><p>We will use the MLDatasets package which downloads the MNIST-dataset for us automatically. If you haven&#39;t installed MLDatasets yet, write this for <a href="https://juliaml.github.io/MLDatasets.jl/stable/#Installation">installation</a>:</p><pre><code class="language-julia hljs">import Pkg; Pkg.add(&quot;MLDatasets&quot;)</code></pre><p>Then we can import MLDatasets:</p><pre><code class="language-julia hljs">using MLDatasets # a package for downloading datasets</code></pre><h4 id="Splitting-up-the-dataset-into-a-train-and-a-test-partition"><a class="docs-heading-anchor" href="#Splitting-up-the-dataset-into-a-train-and-a-test-partition">Splitting up the dataset into a train and a test partition</a><a id="Splitting-up-the-dataset-into-a-train-and-a-test-partition-1"></a><a class="docs-heading-anchor-permalink" href="#Splitting-up-the-dataset-into-a-train-and-a-test-partition" title="Permalink"></a></h4><p>The MNIST-dataset contains 70,000 images, we will use 60,000 images for training the network and 10,000 images for evaluating accuracy.</p><pre><code class="language-julia hljs"># initialize train- and test-dataset
mnist_train = MNIST(:train) 
mnist_test = MNIST(:test)</code></pre><h4 id="Using-GradValley.DataLoader-for-handling-data"><a class="docs-heading-anchor" href="#Using-GradValley.DataLoader-for-handling-data">Using GradValley.DataLoader for handling data</a><a id="Using-GradValley.DataLoader-for-handling-data-1"></a><a class="docs-heading-anchor-permalink" href="#Using-GradValley.DataLoader-for-handling-data" title="Permalink"></a></h4><p>A typical workflow when dealing with datasets is to use the GradValley.DataLoader struct. A data loader makes it easy to iterate directly over the batches in a dataset.  Due to better memory efficiency, the data loader loads the batches <em>just in time</em>. When initializing a data loader, we specify a function that returns exactly one element from the dataset at a given index. We also have to specify the size of the dataset (e.g. the number of images). All parameters that the data loader accepts (see <a href="../reference/#Reference">Reference</a> for more information):</p><pre><code class="language-julia hljs">DataLoader(get_function::Function, dataset_size::Integer; batch_size::Integer=1, shuffle::Bool=false, drop_last::Bool=false)</code></pre><p>Now we write the <em>get function</em> for the two data loaders.</p><pre><code class="language-julia hljs"># function for getting an image and the corresponding target vector from the train or test partition
function get_element(index, partition)
    # load one image and the corresponding label
    if partition == &quot;train&quot;
        image, label = mnist_train[index]
    else # test partition
        image, label = mnist_test[index]
    end
    # add channel dimension and rescaling the values to their original 8 bit gray scale values
    image = reshape(image, 1, 28, 28) .* 255
    # generate the target vector from the label, one for the correct digit, zeros for the wrong digits
    targets = zeros(10)
    targets[label + 1] = 1.00

    return convert(Array{Float64, 3}, image), targets
end</code></pre><p>We can now initialize the data loaders.</p><pre><code class="language-julia hljs"># initialize the data loaders
train_data_loader = DataLoader(index -&gt; get_element(index, &quot;train&quot;), length(mnist_train), batch_size=32, shuffle=true)
test_data_loader = DataLoader(index -&gt; get_element(index, &quot;test&quot;), length(mnist_test), batch_size=32)</code></pre><p>If you want to force the data loader to load the data all at once, you could do:</p><pre><code class="language-julia hljs"># force the data loaders to load all the data at once into memory, depending on the dataset&#39;s size, this may take a while
train_data = train_data_loader[begin:end]
test_data = test_data_loader[begin:end]</code></pre><h3 id="Building-the-neuronal-network-aka.-the-model"><a class="docs-heading-anchor" href="#Building-the-neuronal-network-aka.-the-model">Building the neuronal network aka. the model</a><a id="Building-the-neuronal-network-aka.-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-neuronal-network-aka.-the-model" title="Permalink"></a></h3><p>The most recommend way to build models is to use the GradValley.Layers.SequentialContainer struct. A SequtialContainer can take an array of layers or other SequentialContainers (sub-models). While forward-pass, the given inputs are <em>sequentially</em> propagated through every layer (or sub-model) and the output will be returned. For more details, see <a href="../reference/#Reference">Reference</a>. The LeNet5 model is one of the earliest convolutional neuronal networks (CNNs) reaching approximately 99% accuracy on the MNIST-dataset. The LeNet5 is built of two main parts, the feature extractor and the classifier. So it would be a good idea to clarify that in the code:</p><pre><code class="language-julia hljs"># Definition of a LeNet-like model consisting of a feature extractor and a classifier
feature_extractor = SequentialContainer([ # a convolution layer with 1 in channel, 6 out channels, a 5*5 kernel and a relu activation
                                         Conv(1, 6, (5, 5), activation_function=&quot;relu&quot;),
                                         # an average pooling layer with a 2*2 filter (when not specified, stride is automatically set to kernel size)
                                         AvgPool((2, 2)),
                                         Conv(6, 16, (5, 5), activation_function=&quot;relu&quot;),
                                         AvgPool((2, 2))])
flatten = Reshape((256, ))
classifier = SequentialContainer([ # a fully connected layer (also known as dense or linear) with 256 in features, 120 out features and a relu activation
                                  Fc(256, 120, activation_function=&quot;relu&quot;),
                                  Fc(120, 84, activation_function=&quot;relu&quot;),
                                  Fc(84, 10),
                                  # a softmax activation layer, the softmax will be calculated along the second dimension (the features dimension)
                                  Softmax(dim=2)])
# The final model consists of three different submodules, 
# which shows that a SequentialContainer can contain not only layers, but also other SequentialContainers
model = SequentialContainer([feature_extractor, flatten, classifier])</code></pre><h4 id="Printing-a-nice-looking-summary-of-the-model"><a class="docs-heading-anchor" href="#Printing-a-nice-looking-summary-of-the-model">Printing a nice looking summary of the model</a><a id="Printing-a-nice-looking-summary-of-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Printing-a-nice-looking-summary-of-the-model" title="Permalink"></a></h4><p>Summarizing a model and counting the number of trainable parameters is easily done with the GradValley.Layers.summarie_model function.</p><pre><code class="language-julia hljs"># printing a nice looking summary of the model
summary, num_params = summarize_model(model)
println(summary)</code></pre><h4 id="Defining-hyperparameters"><a class="docs-heading-anchor" href="#Defining-hyperparameters">Defining hyperparameters</a><a id="Defining-hyperparameters-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-hyperparameters" title="Permalink"></a></h4><p>Before we start to train and test the model, we define all necessary hyperparameters. If we want to change the learning rate or the loss function for example, this is the one place to do this.</p><pre><code class="language-julia hljs"># defining hyperparameters
loss_function = mse_loss # mean squared error
learning_rate = 0.05
optimizer = MSGD(model, learning_rate, momentum=0.5) # momentum stochastic gradient descent with a momentum of 0.5
epochs = 5 # 5 or 10</code></pre><h3 id="Train-and-test-the-model"><a class="docs-heading-anchor" href="#Train-and-test-the-model">Train and test the model</a><a id="Train-and-test-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-and-test-the-model" title="Permalink"></a></h3><p>The next step is to write a function for training the model using the above defined hyperparameters. The network is trained 10 times (epochs) with the entire training data set. After each batch, the weights/parameters of the network are adjusted/optimized. However, we want to test the model after each epoch, so we need to write a function for evaluating the model&#39;s accuracy first.</p><pre><code class="language-julia hljs"># evaluate the model&#39;s accuracy
function test()
    num_correct_preds = 0
    avg_test_loss = 0
    for (batch, (images_batch, targets_batch)) in enumerate(test_data_loader)
        # computing predictions
        predictions_batch = forward(model, images_batch)
        # checking for each image in the batch individually if the prediction is correct
        for index_batch in 1:size(predictions_batch)[1]
            single_prediction = predictions_batch[index_batch, :]
            single_target = targets_batch[index_batch, :]
            if argmax(single_prediction) == argmax(single_target)
                num_correct_preds += 1
            end
        end
        # adding the loss for measuring the average test loss
        avg_test_loss += loss_function(predictions_batch, targets_batch, return_derivative=false)
    end

    accuracy = num_correct_preds / size(test_data_loader) * 100 # size(data_loader) returns the dataset size
    avg_test_loss /= length(test_data_loader) # length(data_loader) returns the number of batches

    return accuracy, avg_test_loss
end

# train the model with the above defined hyperparameters
function train()
    for epoch in 1:epochs

        @time begin # for measuring time taken by one epoch

            avg_train_loss = 0.00
            # iterating over the whole data set
            for (batch, (images_batch, targets_batch)) in enumerate(train_data_loader)
                # computing predictions
                predictions_batch = forward(model, images_batch)
                # backpropagation
                zero_gradients(model)
                loss, derivative_loss = loss_function(predictions_batch, targets_batch)
                backward(model, derivative_loss)
                # optimize the model&#39;s parameters
                step!(optimizer)
                # printing status
                if batch % 100 == 0
                    image_index = batch * train_data_loader.batch_size
                    data_set_size = size(train_data_loader)
                    println(&quot;Batch $batch, Image [$image_index/$data_set_size], Loss: $(round(loss, digits=5))&quot;)
                end
                # adding the loss for measuring the average train loss
                avg_train_loss += loss
            end

            avg_train_loss /= length(train_data_loader)
            accuracy, avg_test_loss = test()
            print(&quot;Results of epoch $epoch: Avg train loss: $(round(avg_train_loss, digits=5)), Avg test loss: $(round(avg_test_loss, digits=5)), Accuracy: $accuracy%, Time taken:&quot;)

        end

    end
end</code></pre><h4 id="Run-the-training-and-save-the-trained-model-afterwards"><a class="docs-heading-anchor" href="#Run-the-training-and-save-the-trained-model-afterwards">Run the training and save the trained model afterwards</a><a id="Run-the-training-and-save-the-trained-model-afterwards-1"></a><a class="docs-heading-anchor-permalink" href="#Run-the-training-and-save-the-trained-model-afterwards" title="Permalink"></a></h4><p>When the file is run as the main script, we want to actually call the train() function and save the final model afterwards. We will use the <a href="https://github.com/JuliaIO/BSON.jl">BSON.jl</a> package for saving the model easily.</p><pre><code class="language-julia hljs"># when this file is run as the main script,
# then train() is run and the final model will be saved using a package called BSON.jl
import Pkg; Pkg.add(&quot;BSON&quot;)
using BSON: @save
if abspath(PROGRAM_FILE) == @__FILE__
    train()
    file_name = &quot;MNIST_with_LeNet5_model.bson&quot;
    @save file_name model
    println(&quot;Saved trained model as $file_name&quot;)
end</code></pre><h4 id="Use-the-trained-model"><a class="docs-heading-anchor" href="#Use-the-trained-model">Use the trained model</a><a id="Use-the-trained-model-1"></a><a class="docs-heading-anchor-permalink" href="#Use-the-trained-model" title="Permalink"></a></h4><p>If you want to easily use the trained model, you firstly need to import the necessary modules from GradValley. Then you can use the @load macro of BSON to load the model object. Now you can let the model make a few individual predictions, for example. Use this code in an extra file.</p><pre><code class="language-julia hljs">using GradValley
using GradValley.Layers 
using GradValley.Optimization
using MLDatasets
using BSON: @load

# load the trained model
@load &quot;MNIST_with_LeNet5_model.bson&quot; model

# make some individual predictions
mnist_test = MNIST(:test)
for i in 1:5
    random_index = rand(1:length(mnist_test))
    image, label = mnist_test[random_index]
    # remember to add batch and channel dimensions and to rescale the image as was done during training and testing
    image_batch = convert(Array{Float64, 4}, reshape(image, 1, 1, 28, 28)) .* 255
    prediction = forward(model, image_batch)
    predicted_label = argmax(prediction[1, :]) - 1
    println(&quot;Predicted label: $predicted_label, Correct Label: $label&quot;)
end</code></pre><h3 id="Running-the-file-with-multiple-threads"><a class="docs-heading-anchor" href="#Running-the-file-with-multiple-threads">Running the file with multiple threads</a><a id="Running-the-file-with-multiple-threads-1"></a><a class="docs-heading-anchor-permalink" href="#Running-the-file-with-multiple-threads" title="Permalink"></a></h3><p>It is heavily recommended to run this file, and any other files using GradValley, with multiple threads. Using multiple threads can make training much faster. To do this, use the <code>-t</code> option when running a julia script in terminal/PowerShell/command line/etc. If your CPU has 24 threads, for example, then run:</p><pre><code class="nohighlight hljs">julia -t 24 ./MNIST_with_LeNet5.jl</code></pre><p>The specified number of threads should match the number of threads your CPU provides.</p><h3 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h3><p>These were my results after 5 training epochs: <em>Results of epoch 5: Avg train loss: 0.00237, Avg test loss: 0.00283, Accuracy: 98.21%, Time taken: 13.416619 seconds (20.34 M allocations: 30.164 GiB, 5.86% gc time)</em> On my Ryzen 9 5900X CPU (using all 24 threads, slightly overclocked), one epoch took around ~15 seconds (no compilation time), so the whole training (5 epochs) took around ~75 seconds (no compilation time).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference/">Â« Reference</a><a class="docs-footer-nextpage" href="../(pre-trained)_models/">(Pre-Trained) Models Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Saturday 8 April 2023 16:42">Saturday 8 April 2023</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
