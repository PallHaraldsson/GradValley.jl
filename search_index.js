var documenterSearchIndex = {"docs":
[{"location":"tutorials_and_examples/#Tutorials-and-Examples","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Here you can find detailed explanations on how to build and train specific models with GradValley.jl.","category":"page"},{"location":"tutorials_and_examples/#A-LeNet-like-model-for-handwritten-digit-recognition","page":"Tutorials and Examples","title":"A LeNet-like model for handwritten digit recognition","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"In this tutorial, we will learn the basics of GradValley.jl while building a model for handwritten digit recognition reaching approximately 99% accuracy on the MNIST-dataset. The whole code at once can be found here.","category":"page"},{"location":"tutorials_and_examples/#Importing-modules","page":"Tutorials and Examples","title":"Importing modules","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"using GradValley # The master module of GradValley.jl\r\nusing GradValley.Layers # The \"Layers\" module provides all the building blocks for creating a model. \r\nusing GradValley.Optimization # The \"Optimization\" module provides different loss functions and optimizers.","category":"page"},{"location":"tutorials_and_examples/#Using-the-dataset","page":"Tutorials and Examples","title":"Using the dataset","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"We will using the MLDatasets package which downloads the MNIST-dataset for us automatically. If you haven't installed MLDatasets yet, write this for installation:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"import Pkg; Pkg.add(\"MLDatasets\")","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Then we can import MLDatasets:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"using MLDatasets # A package for downloading datasets","category":"page"},{"location":"tutorials_and_examples/#Splitting-up-the-dataset-in-a-train-and-a-test-partition","page":"Tutorials and Examples","title":"Splitting up the dataset in a train- and a test-partition","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The MNIST-dataset contains 70,000 images, we will use 60,000 images for training the network and 10,000 images for evaluating accuracy.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# Initialize train- and test-dataset\r\nmnist_train = MNIST(:train) \r\nmnist_test = MNIST(:test)","category":"page"},{"location":"tutorials_and_examples/#Using-GradValley.DataLoader-for-handling-data","page":"Tutorials and Examples","title":"Using GradValley.DataLoader for handling data","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"A typical workflow when dealing with datasets is to use the GradValley.DataLoader struct. A data loader makes it easy to iterate directly over the batches in a dataset.  Due to better memory efficiency, the data loader loads the batches just in time. When initializing a data loader, we specify a function that returns exactly one element from the dataset at a given index. We also have to specify the size of the dataset (e.g. the number of images). All parameters that the dataloader accepts (see Reference for more information):","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"DataLoader(get_function::Function, dataset_size::Integer; batch_size::Integer=1, shuffle::Bool=false, drop_last::Bool=false)","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Now we write the get function for the two data loaders.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# function for getting an image and the corressponding target vector from the train or test partition\r\nfunction get_element(index, partition)\r\n    # load one image and the corresponding label\r\n    if partition == \"train\"\r\n        image, label = mnist_train[index]\r\n    else # test partition\r\n        image, label = mnist_test[index]\r\n    end\r\n    # add channel dimension\r\n    image = reshape(image, 1, 28, 28)\r\n    # generate the target vector from the label, one for the correct digit, zeros for the wrong digits\r\n    targets = zeros(10)\r\n    targets[label + 1] = 1.00\r\n\r\n    return image, label\r\nend","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"We can now initialitze the data loaders.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"train_data_loader = DataLoader(index -> get_element(index, \"train\"), length(mnist_train), batch_size=32)\r\ntest_data_loader = DataLoader(index -> get_element(index, \"test\"), length(mnist_test), batch_size=32)","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"If you want to force the data loader to load the data all at once, you could do:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# depending on the dataset's size, this may take a while\r\ntrain_data = train_data_loader[begin:end]\r\ntest_data = test_data_loader[begin:end]","category":"page"},{"location":"tutorials_and_examples/#Building-the-neuronal-network-aka.-the-model","page":"Tutorials and Examples","title":"Building the neuronal network aka. the model","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"The most recommend way to build models is to use the GradValley.Layers.SequentialContainer struct. A SequtialContainer can take an array of layers or other SequentialContainers (sub-models). While forward-pass, the given inputs are sequentially propagated through every layer (or sub-model) and the output will be returned. For more details, see Reference. The LeNet5 model is one of the earliest convolutional neuronal networks (CNNs) reaching approximately 99% accuracy on the MNIST-dataset. The LeNet5 is build from two main parts, the feature extractor and the classifier. So it would be a good idea to clarify that in the code:","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# Definition of a LeNet-like model consisting of a feature extractor and a classifier\r\nfeature_extractor = SequentialContainer([ # a convolution layer with 1 in channel, 6 out channels, a 5*5 kernel and a relu activation\r\n                                         Conv(1, 6, (5, 5), activation_function=\"relu\"),\r\n                                         # a average pooling layer with a 2*2 filter (when not specified, stride is automatcally set to kernel size)\r\n                                         AvgPool((2, 2)),\r\n                                         Conv(6, 16, (5, 5), activation_function=\"relu\"),\r\n                                         AvgPool((2, 2))])\r\nflatten = Reshape((256, ))\r\nclassifier = SequentialContainer([ # a fully connected layer (also known as dense or linear) with 256 in features, 120 out features and a relu activation\r\n                                  Fc(256, 120, activation_function=\"relu\"),\r\n                                  Fc(120, 84, activation_function=\"relu\"),\r\n                                  Fc(84, 10),\r\n                                  # a softmax activation layer, the softmax will be calculated along the second dimension (the features dimension)\r\n                                  Softmax(dim=2)])\r\n# The final model consists of three different sub-modules, \r\n# which shows that a SequentialContainer can contain not only layers, but also other SequentialContainers\r\nmodel = SequentialContainer([feature_extractor, flatten, classifier])","category":"page"},{"location":"tutorials_and_examples/#Defining-hyperparameters","page":"Tutorials and Examples","title":"Defining hyperparameters","text":"","category":"section"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"Before we start to train and test the model, we define all necessary hyperparamters. If we want to change the learning rate or the loss function for example, this is the one place to do this.","category":"page"},{"location":"tutorials_and_examples/","page":"Tutorials and Examples","title":"Tutorials and Examples","text":"# defining hyperparameters\r\nloss_function = mse_loss # mean squared error\r\nlearning_rate = 0.05\r\noptimizer = MSGD(model, learning_rate, momentum=0.5) # momentum stochastic gradient decent with a momentum of 0.5\r\nepochs = 20","category":"page"},{"location":"tutorials_and_examples/#Train-the-model","page":"Tutorials and Examples","title":"Train the model","text":"","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#First-Impressions","page":"Getting Started","title":"First Impressions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This example shows the basic workflow on model building and how to use loss functions and optimizers to train the model:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using GradValley\r\nusing GradValley.Layers # The \"Layers\" module provides all the building blocks for creating a model.\r\nusing GradValley.Optimization # The \"Optimization\" module provides different loss functions and optimizers.\r\n\r\n# Definition of a LeNet-like model consisting of a feature extractor and a classifier\r\nfeature_extractor = SequentialContainer([Conv(1, 6, (5, 5), activation_function=\"relu\"),\r\n                                         AvgPool((2, 2)),\r\n                                         Conv(6, 16, (5, 5), activation_function=\"relu\"),\r\n                                         AvgPool((2, 2))])\r\nflatten = Reshape((256, ))\r\nclassifier = SequentialContainer([Fc(256, 120, activation_function=\"relu\"),\r\n                                  Fc(120, 84, activation_function=\"relu\"),\r\n                                  Fc(84, 10),\r\n                                  Softmax(dim=2)])\r\n# The final model consists of three different sub-modules, which shows that a SequentialContainer can contain not only layers, but also other SequentialContainers\r\nmodel = SequentialContainer([feature_extractor, flatten, classifier])\r\n                                  \r\n# feeding the network with some random data\r\ninput = rand(32, 1, 28, 28) # a batch of 32 images with one channel and a size of 28*28 pixels\r\nprediction = forward(model, input) # the forward function can work with a layer or a SequentialContainer\r\n\r\n# choosing an optimizer for training\r\nlearning_rate = 0.05\r\noptimizer = MSGD(model, learning_rate, momentum=0.5) # momentum stochastic gradient decent with a momentum of 0.5\r\n\r\n# generating some random data for a training step\r\ntarget = rand(size(prediction)...)\r\n# backpropagation\r\nzero_gradients(model)\r\nloss, derivative_loss = mse_loss(prediction, target) # mean squared error\r\nbackward(model, derivative_loss) # computing gradients\r\nstep!(optimizer) # making a optimization step with the calculated gradients and the optimizer","category":"page"},{"location":"getting_started/#First-Real-Project","page":"Getting Started","title":"First Real Project","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here are some suggestions to implement your first real project with GradValley.jl:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The \"Hello World\" of Deep Learning: Try the Tutorial on training a LeNet-like model for handwritten digit recognition.\nThe Reference: In the reference, you can find descriptions of all the layers, loss functions and optimizers.\nDownload a pre-trained model: More pre-trained models will likely be deployed over time.\nLook at more examples and tutorials.","category":"page"},{"location":"(pre-trained)_models/#(Pre-Trained)-Models","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"","category":"section"},{"location":"(pre-trained)_models/","page":"(Pre-Trained) Models","title":"(Pre-Trained) Models","text":"Unfortunately, no pre-trained models are currently available. Over time, however, models will be made available here.","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"The package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add https://github.com/jonas208/GradValley.jl","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia> import Pkg; Pkg.add(url=\"https://github.com/jonas208/GradValley.jl\")","category":"page"},{"location":"installation/#Used-Dependencies","page":"Installation","title":"Used Dependencies","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"GradValley.jl uses two packages which are inbuilt in the Julia programming language:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Random Numbers (no specific version)\nLinear Algebra (no specific version)","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Besides that, GradValley.jl uses one external package:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"LoopVectorization.jl (at least v0.12.146)","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"You can also look at the Project.toml file to find information about used dependencies and compatibility.","category":"page"},{"location":"installation/#All-used-dependencies-will-be-automatically-installed-due-installation-of-GradValley.jl.","page":"Installation","title":"All used dependencies will be automatically installed due installation of GradValley.jl.","text":"","category":"section"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the GradValley.jl documentation!","category":"page"},{"location":"","page":"Home","title":"Home","text":"GradValley.jl is a new lightweight module for deep learning in 100% Julia. It offers a high level interface for model building and training. It is completely independent from other machine learning packages like Flux, Knet, NNlib or NNPACK. It is based on Julia's standard array type and needs no additional tensor type. GradValley.jl's \"backend\" is written \"human-friendly\". So if you're looking into how exactly such deep learning algorithms work, looking at the source code could also be a good learning resource. See Learning for further information. To get started, see Installation and Getting Started. After that, you could look at the Tutorials and Examples section.","category":"page"},{"location":"#About","page":"Home","title":"About","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A while ago I started looking into machine learning. The topic fascinated me from the beginning, so I wanted to gain a deeper understanding of the way such models work. In my opinion, the best way to do this is to write your own small software package for machine learning and not just blindly use one of the big, established frameworks such as PyTorch or TensorFlow. The Julia programming language was my choice because of its popularity in academia and its very good performance compared to pure Python, which is after all very popular in the world of artificial intelligence. The product of this work is this module called GradValley.jl with which various current neural networks (e.g. CNNs) can be implemented easily and intuitively.","category":"page"},{"location":"#Array-structure-convention:","page":"Home","title":"Array structure convention:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The order used in GradValley for processing images (or similar data) is NCHW, where N is the batch dimension, C is the channel dimension, H is the vertical and W is the horizontal size of the image. In this regard, GradValley differs from some other deep learning packages in Julia and is similar to PyTorch instead. This makes the migration of pre-trained models from PyTorch or from the Python world in general to GradValley much easier.","category":"page"},{"location":"#Explanation-of-the-name-\"GradValley\":","page":"Home","title":"Explanation of the name \"GradValley\":","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"When optimizing the weights of a machine learning model, an attempt is always made to find the best possible error minimum. The derivatives, i.e. the gradients, of the error function in relation to the weights are required for this. So the goal is to find the \"valley\" of the error using the gradients (\"grad\" stands for gradient). That's why it's called GradValley.","category":"page"},{"location":"#Current-Limitations","page":"Home","title":"Current Limitations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Due to the relatively early development status of this software, no GPU support is currently offered. GradValley.jl doesn't provide a real automatic differentiation (AD) engine like PyTorch does, for example. However, in the case of this software package, it is not really necessary to have real AD. Model building is mostly done with the SequentialContainer, this clearly defines the forward pass and thus the backward pass is also known to the software.","category":"page"}]
}
