<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · GradValley.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="GradValley.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="GradValley.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GradValley.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#GradValley"><span>GradValley</span></a></li><li><a class="tocitem" href="#GradValley.Layers"><span>GradValley.Layers</span></a></li><li><a class="tocitem" href="#GradValley.Optimization"><span>GradValley.Optimization</span></a></li><li><a class="tocitem" href="#GradValley.Functional"><span>GradValley.Functional</span></a></li></ul></li><li><a class="tocitem" href="../tutorials_and_examples/">Tutorials and Examples</a></li><li><a class="tocitem" href="../(pre-trained)_models/">(Pre-Trained) Models</a></li><li><a class="tocitem" href="../learning/">Learning</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For many layers and functions, the documentation is still missing because this documentation is still under construction!</p></div></div><ul><li><a href="#Reference">Reference</a></li><li class="no-marker"><ul><li><a href="#GradValley">GradValley</a></li><li><a href="#GradValley.Layers">GradValley.Layers</a></li><li class="no-marker"><ul><li><a href="#Containers">Containers</a></li><li><a href="#Convolution">Convolution</a></li><li><a href="#Pooling">Pooling</a></li><li><a href="#Fully-connected">Fully connected</a></li><li><a href="#Normalization">Normalization</a></li><li><a href="#Reshape-/-Flatten">Reshape / Flatten</a></li><li><a href="#Special-activation-functions">Special activation functions</a></li></ul></li><li><a href="#GradValley.Optimization">GradValley.Optimization</a></li><li><a href="#GradValley.Functional">GradValley.Functional</a></li></ul></li></ul><h2 id="GradValley"><a class="docs-heading-anchor" href="#GradValley">GradValley</a><a id="GradValley-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley" title="Permalink"></a></h2><h2 id="GradValley.Layers"><a class="docs-heading-anchor" href="#GradValley.Layers">GradValley.Layers</a><a id="GradValley.Layers-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Layers" title="Permalink"></a></h2><h3 id="Containers"><a class="docs-heading-anchor" href="#Containers">Containers</a><a id="Containers-1"></a><a class="docs-heading-anchor-permalink" href="#Containers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.SequentialContainer" href="#Main.GradValley.Layers.SequentialContainer"><code>Main.GradValley.Layers.SequentialContainer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SequentialContainer(layer_stack::Vector{&lt;: Any})</code></pre><p>A sequential container (recommended method for building models). A SequtialContainer can take a vector of layers or other SequentialContainers (submodules). While forward-pass, the given inputs are <em>sequentially</em> propagated through every layer (or submodule) and the output will be returned. The execution order during forward pass is of course the same as the order in the vector containing the layers or submodules. This container currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>layer_stack::Vector{&lt;: Any}</code>: the vector containing the layers (or submodules, so other Containers), the order of the modules in the vector corresponds to the execution order</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a simple chain of fully connected layers
julia&gt; m = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# a more complicated example with with nested submodules
julia&gt; feature_extractor_part_1 = SequentialContainer([Conv(1, 6, (5, 5), activation_function=&quot;relu&quot;), AvgPool((2, 2))])
julia&gt; feature_extractor_part_2 = SequentialContainer([Conv(6, 16, (5, 5), activation_function=&quot;relu&quot;), AvgPool((2, 2))])
julia&gt; feature_extractor = SequentialContainer([feature_extractor_part_1, feature_extractor_part_2])
julia&gt; classifier = SequentialContainer([Fc(256, 120, activation_function=&quot;relu&quot;), Fc(120, 84, activation_function=&quot;relu&quot;), Fc(84, 10)])
julia&gt; m = SequentialContainer([feature_extractor, Reshape((256, )), classifier, Softmax(dim=2)])
# if a SequentialContainer contains BatchNorm layers (regardless of whether they are nested somewhere in a submodule or not), 
# the mode of all these layers at once can be switched as follows
julia&gt; trainmode!(m)
julia&gt; testmode!(m)
# computing the output of the module (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Convolution"><a class="docs-heading-anchor" href="#Convolution">Convolution</a><a id="Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.Conv" href="#Main.GradValley.Layers.Conv"><code>Main.GradValley.Layers.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Conv(in_channels::Int, out_channels::Int, kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=(1, 1), padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), groups::Int=1, activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A convolution layer. Apply a 2D convolution over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_channels::Int</code>: the number of channels in the input image</li><li><code>out_channels::Int</code>: the number of channels produced by the convolution</li><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the convolving kernel</li><li><code>stride::Tuple{Int, Int}=(1, 1)</code>: the stride of the convolution</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between kernel elements</li><li><code>groups::Int=1</code>: the number of blocked connections from input channels to output channels (in-channels and out-channels must both be divisible by groups)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the convolution </li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li></ul><p><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</p><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C_{in}, H_{in}, W_{in})$</span></li><li>Weight: <span>$(C_{out}, \frac{C_{in}}{groups}, H_{w}, W_{w})$</span></li><li>Bias: <span>$(C_{out}, )$</span></li><li>Output: <span>$(N, C_{out}, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1)}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1)}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>kernels::Array{Float64, 4}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 4}</code>: the current gradients of the weights/kernels</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>For one group, a multichannel 2D convolution (disregarding batch dimension and activation function) can be described as:</p><ul><li><span>$o_{c_{out}, y_{out}, x_{out}} = \big(\sum_{c_{in=1}}^{C_{in}}\sum_{y_w=1}^{H_{w}}\sum_{x_w=1}^{W_{w}} i_{c_{in}, y_{in}, x_{in}} \cdot w_{c_{out}, c_{in}, y_w, x_w}\big) + b_{c_{out}}$</span>, where<ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul></li></ul><p><em>O</em> is the output array, <em>I</em> the input array, <em>W</em> the weight array and <em>B</em> the bias array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># square kernels and fully default values of keyword arguments
julia&gt; m = Conv(3, 6, (5, 5))
# non-square kernels and unequal stride and with padding as well as specified weight initialization mode
# (init_mode=&quot;kaiming&quot; stands for kaiming weight initialization with normally distributed values)
julia&gt; m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(2, 1))
# non-square kernels and unequal stride and with padding, dilation and 3 groups
# (because groups=in_channels and out_channles is divisible by groups, it is even a depthwise convolution)
julia&gt; m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), groups=3)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.DepthwiseConv" href="#Main.GradValley.Layers.DepthwiseConv"><code>Main.GradValley.Layers.DepthwiseConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DepthwiseConv(in_channels::Int, out_channels::Int, kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=(1, 1), padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A depthwise convolution layer. Apply a 2D depthwise convolution over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_channels::Int</code>: the number of channels in the input image</li><li><code>out_channels::Int</code>: the number of channels produced by the convolution</li><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the convolving kernel</li><li><code>stride::Tuple{Int, Int}=(1, 1)</code>: the stride of the convolution</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between kernel elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the convolution </li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li></ul><p><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</p><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C_{in}, H_{in}, W_{in})$</span></li><li>Weight: <span>$(C_{out}, \frac{C_{in}}{groups}, H_{w}, W_{w})$</span>, where <span>$groups = in\_channels$</span></li><li>Bias: <span>$(C_{out}, )$</span></li><li>Output: <span>$(N, C_{out}, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1)}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1)}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>kernels::Array{Float64, 4}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 4}</code>: the current gradients of the weights/kernels</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>A convolution is called depthwise if <span>$groups=in\_channels$</span> and <span>$out\_channels=k \cdot in\_channels$</span>, where <span>$k$</span> is a positive integer. The second condition ensures that the of number out-channels is divisible by the number of groups/in-channels. In the background, the standard convolution operation is also used for this layer.  It is just an interface making clear that this layer can only perform a depthwise convolution.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># square kernels and fully default values of keyword arguments
julia&gt; m = DepthwiseConv(3, 6, (5, 5))
# non-square kernels and unequal stride and with padding as well as specified weight initialization mode
# (init_mode=&quot;kaiming&quot; stands for kaiming weight initialization with normally distributed values)
julia&gt; m = DepthwiseConv(3, 6, (3, 5), stride=(2, 1), padding=(2, 1))
# non-square kernels and unequal stride and with padding, dilation and 3 groups
# (because groups=in_channels and out_channles is divisible by groups, it is even a depthwise convolution)
julia&gt; m = DepthwiseConv(3, 6, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), groups=3)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Pooling"><a class="docs-heading-anchor" href="#Pooling">Pooling</a><a id="Pooling-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.MaxPool" href="#Main.GradValley.Layers.MaxPool"><code>Main.GradValley.Layers.MaxPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MaxPool(kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=kernel_size, padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing)</code></pre><p>A maximum pooling layer. Apply a 2D maximum pooling over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the window to take the maximum over</li><li><code>stride::Tuple{Int, Int}=kernel_size</code>: the stride of the window</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between the window elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (kernel\_size[1] - 1)}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (kernel\_size[2] - 1)}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Definition</strong></p><p>A multichannel 2D maximum pooling (disregarding batch dimension and activation function) can be described as:</p><p class="math-container">\[\begin{align*}
o_{c, y_{out}, x_{out}} = \max
_{y_w = 1, ..., kernel\_size[1] \ x_w = 1, ..., kernel\_size[2]}
i_{c, y_{in}, x_{in}}
\end{align*}\]</p><p>Where</p><ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul><p><em>O</em> is the output array and <em>I</em> the input array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># pooling of square window of size=(3, 3) and automatically selected stride
julia&gt; m = MaxPool((3, 3))
# pooling of non-square window with custom stride and padding
julia&gt; m = MaxPool((3, 2), stride=(2, 1), padding=(1, 1))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.AvgPool" href="#Main.GradValley.Layers.AvgPool"><code>Main.GradValley.Layers.AvgPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AvgPool(kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=kernel_size, padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing)</code></pre><p>An average pooling layer. Apply a 2D average pooling over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the window to take the average over</li><li><code>stride::Tuple{Int, Int}=kernel_size</code>: the stride of the window</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between the window elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (kernel\_size[1] - 1)}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (kernel\_size[2] - 1)}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Definition</strong></p><p>A multichannel 2D average pooling (disregarding batch dimension and activation function) can be described as:</p><ul><li><span>$o_{c, y_{out}, x_{out}} = \frac{1}{kernel\_size[1] \cdot kernel\_size[2]} \sum_{i=1}^{kernel\_size[1]}\sum_{j=1}^{kernel\_size[2]} i_{c, y_{in}, x_{in}}$</span>, where<ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul></li></ul><p><em>O</em> is the output array and <em>I</em> the input array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># pooling of square window of size=(3, 3) and automatically selected stride
julia&gt; m = AvgPool((3, 3))
# pooling of non-square window with custom stride and padding
julia&gt; m = AvgPool((3, 2), stride=(2, 1), padding=(1, 1))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.AdaptiveMaxPool" href="#Main.GradValley.Layers.AdaptiveMaxPool"><code>Main.GradValley.Layers.AdaptiveMaxPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveMaxPool(output_size::Tuple{Int, Int}; activation_function::Union{Nothing, String}=nothing)</code></pre><p>An adaptive maximum pooling layer. Apply a 2D adaptive maximum pooling over an input signal with additional batch and channel dimensions. For any input size, the size of the output is always equal to the specified <span>$output\_size$</span>. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>output_size::Tuple{Int, Int}</code>: the target output size of the image (can even be larger than the input size) of the form <span>$(H_{out}, W_{out})$</span></li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <span>$(H_{out}, W_{out}) = output\_size$</span></li></ul><p><strong>Definition</strong></p><p>In some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard maximum pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: <a href="https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work">stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work</a>). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The <code>function get_in_indices(in_len, out_len)</code> in  <a href="https://github.com/jonas208/GradValley.jl/blob/main/src/gv_functional.jl"><code>gv_functional.jl</code></a> (line 95 - 113) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at <a href="#Main.GradValley.Layers.MaxPool"><code>MaxPool</code></a> with the difference that the indices <span>$y_{in}$</span> and <span>$x_{in}$</span>  have already been calculated beforehand.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># target output size of 5x5
julia&gt; m = AdaptiveMaxPool((5, 5))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.AdaptiveAvgPool" href="#Main.GradValley.Layers.AdaptiveAvgPool"><code>Main.GradValley.Layers.AdaptiveAvgPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveAvgPool(output_size::Tuple{Int, Int}; activation_function::Union{Nothing, String}=nothing)</code></pre><p>An adaptive average pooling layer. Apply a 2D adaptive average pooling over an input signal with additional batch and channel dimensions. For any input size, the size of the output is always equal to the specified <span>$output\_size$</span>. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>output_size::Tuple{Int, Int}</code>: the target output size of the image (can even be larger than the input size) of the form <span>$(H_{out}, W_{out})$</span></li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <span>$(H_{out}, W_{out}) = output\_size$</span></li></ul><p><strong>Definition</strong></p><p>In some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard average pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: <a href="https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work">stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work</a>). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The <code>function get_in_indices(in_len, out_len)</code> in  <a href="https://github.com/jonas208/GradValley.jl/blob/main/src/gv_functional.jl"><code>gv_functional.jl</code></a> (line 95 - 113) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at <a href="#Main.GradValley.Layers.AvgPool"><code>AvgPool</code></a> with the difference that the indices <span>$y_{in}$</span> and <span>$x_{in}$</span>  have already been calculated beforehand.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># target output size of 5x5
julia&gt; m = AdaptiveAvgPool((5, 5))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Fully-connected"><a class="docs-heading-anchor" href="#Fully-connected">Fully connected</a><a id="Fully-connected-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-connected" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.Fc" href="#Main.GradValley.Layers.Fc"><code>Main.GradValley.Layers.Fc</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Fc(in_features::Int, out_features::Int; activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A fully connected layer (sometimes also known as dense or linear). Apply a linear transformation (matrix multiplication) to the input signal with additional batch dimension. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_features::Int</code>: the size of each input sample (<em>&quot;number of input neurons&quot;</em>)</li><li><code>out_features::Int</code>: the size of each output sample (<em>&quot;number of output neurons&quot;</em>)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li></ul><p><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</p><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, in\_features)$</span></li><li>Weight: <span>$(out\_features, in\_features)$</span></li><li>Bias: <span>$(out\_features, )$</span></li><li>Output: <span>$(N, out\_features)$</span></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>weights::Array{Float64, 2}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 2}</code>: the current gradients of the weights</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>The forward pass of a fully connected layer is given by the matrix multiplication between the weight matrix and the input vector  (disregarding batch dimension and activation function):</p><ul><li><span>$O = WI + B$</span></li></ul><p>This operation can also be described by:</p><ul><li><span>$o_{j} = \big(\sum_{k=1}^{in\_features} w_{j,k} \cdot i_{k}\big) + b_{j}$</span></li></ul><p><em>O</em> is the output vector, <em>I</em> the input vector, <em>W</em> the weight matrix and <em>B</em> the bias vector. Visually interpreted, it means that each input neuron <em>i</em> is weighted with the corresponding weight <em>w</em> connecting the input neuron  to the output neuron <em>o</em> where all the incoming signals are summed up.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a fully connected layer with 784 input features and 120 output features
julia&gt; m = Fc(784, 120)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 784)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Normalization"><a class="docs-heading-anchor" href="#Normalization">Normalization</a><a id="Normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Normalization" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.BatchNorm2d" href="#Main.GradValley.Layers.BatchNorm2d"><code>Main.GradValley.Layers.BatchNorm2d</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchNorm2d(num_features::Int; epsilon::Float64=1e-05, momentum::Float64=0.1, affine::Bool=true, track_running_stats::Bool=true, activation_function::Union{Nothing, String}=nothing)</code></pre><p>A batch normalization layer. Apply a batch normalization over a 4D input signal (a mini-batch of 2D inputs with additional channel dimension). This layer currently (!) only accepts Float64 array inputs. </p><p>This layer has two modes: training mode and test mode. If <code>track_running_stats::Bool=true</code>, this layer behaves differently in the two modes. During training, this layer always uses the currently calculated batch statistics. If <code>track_running_stats::Bool=true</code>, the running mean and variance are tracked during training and will be used while testing. If <code>track_running_stats::Bool=false</code>, even in test mode, the currently calculated batch statistics are used. The mode can be switched with <a href="@ref">trainmode!</a> or <a href="@ref">testmode!</a> respectively. The training mode is active by default.</p><p><strong>Arguments</strong></p><ul><li><code>num_features::Int</code>: the number of channels</li><li><code>epsilon::Float64=1e-05</code>: a value added to the denominator for numerical stability</li><li><code>momentum::Float64=0.1</code>: the value used for the running mean and running variance computation</li><li><code>affine::Bool=true</code>: if true, this layer uses learnable affine parameters/weights (<span>$\gamma$</span> and <span>$\beta$</span>)</li><li><code>track_running_stats::Bool=true</code>: if true, this layer tracks the running mean and variance during training and will use them for testing/evaluation, if false, such statistics are not tracked and, even in test mode, the batch statistics are always recalculated for each new input</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H, W)$</span></li><li><span>$\gamma$</span> Weight, <span>$\beta$</span> Bias: <span>$(C, )$</span></li><li>Running Mean/Variance: <span>$(C, )$</span></li><li>Output: <span>$(N, C, H, W)$</span> (same shape as input)</li></ul><p><strong>Useful Fields/Variables</strong></p><p><strong>Weights (used if <code>affine::Bool=true</code>)</strong></p><ul><li><code>weight_gamma::Vector{Float64}</code>: <span>$\gamma$</span>, a learnabele parameter for each channel, initialized with ones</li><li><code>weight_beta::Vector{Float64}</code>: <span>$\beta$</span>, a learnabele parameter for each channel, initialized with zeros</li></ul><p><strong>Gradients of weights (used if <code>affine::Bool=true</code>)</strong></p><ul><li><code>gradient_gamma::Vector{Float64}</code>: the gradients of <span>$\gamma$</span></li><li><code>gradient_beta::Vector{Float64}</code>: the gradients of <span>$\beta$</span></li></ul><p><strong>Running statistics (used if <code>rack_running_stats::Bool=true</code>)</strong></p><ul><li><code>running_mean::Vector{Float64}</code>: the continuously updated batch statistics of the mean</li><li><code>running_variance::Vector{Float64}</code>: the continuously updated batch statistics of the variance</li></ul><p><strong>Definition</strong></p><p>A batch normalization operation can be described as: For input values over a mini-batch: <span>$\mathcal{B} = \{x_1, x_2, ..., x_n\}$</span></p><p class="math-container">\[\begin{align*}
y_i = \frac{x_i - \overline{\mathcal{B}}}{\sqrt{Var(\mathcal{B}) + \epsilon}} \cdot \gamma + \beta
\end{align*}\]</p><p>Where <span>$y_i$</span> is an output value and <span>$x_i$</span> an input value. <span>$\overline{\mathcal{B}}$</span> is the mean of the input values in <span>$\mathcal{B}$</span> and <span>$Var(\mathcal{B})$</span>  is the variance of the input values in <span>$\mathcal{B}$</span>. Note that this definition is fairly general and not specified to 4D inputs. In this case, the input values of <span>$\mathcal{B}$</span> are taken for each channel individually.  So the mean and variance are calculated per channel over the mini-batch.</p><p>The update rule for the running statistics (running mean/variance) is:</p><p class="math-container">\[\begin{align*}
\hat{x}_{new} = (1 - momentum) \cdot \hat{x} + momentum \cdot x
\end{align*}\]</p><p>Where <span>$\hat{x}$</span> is the estimated statistic and <span>$x$</span> is the new observed value. So <span>$\hat{x}_{new}$</span> is the new, updated estimated statistic.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a batch normalization layer (3 channels) with learnabel parameters and continuously updated batch statistics for evaluation
julia&gt; m = BatchNorm2d(3)
# the mode can be switched with trainmode! or testmode!
julia&gt; trainmode!(m)
julia&gt; testmode!(m)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Reshape-/-Flatten"><a class="docs-heading-anchor" href="#Reshape-/-Flatten">Reshape / Flatten</a><a id="Reshape-/-Flatten-1"></a><a class="docs-heading-anchor-permalink" href="#Reshape-/-Flatten" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Main.GradValley.Layers.Reshape" href="#Main.GradValley.Layers.Reshape"><code>Main.GradValley.Layers.Reshape</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Reshape(out_shape; activation_function::Union{Nothing, String}=nothing)</code></pre><p>A reshape layer (probably mostly used as a flatten layer). Reshape the input signal (effects all dimensions except the batch dimension). This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>out_shape</code>: the target output size (the output has the same data as the input and must have the same number of elements)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, *)$</span>, where * means any number of dimensions</li><li>Output: <span>$(N, out\_shape...)$</span></li></ul><p><strong>Definition</strong></p><p>This layer uses the standard <a href="https://docs.julialang.org/en/v1/base/arrays/#Base.reshape">reshape function</a> inbuilt in Julia.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># flatten the input of size 1*28*28 to a vector of length 784 (each plus batch dimension of course)
julia&gt; m = Reshape((784, ))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Special-activation-functions"><a class="docs-heading-anchor" href="#Special-activation-functions">Special activation functions</a><a id="Special-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Special-activation-functions" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Softmax</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="GradValley.Optimization"><a class="docs-heading-anchor" href="#GradValley.Optimization">GradValley.Optimization</a><a id="GradValley.Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Optimization" title="Permalink"></a></h2><h2 id="GradValley.Functional"><a class="docs-heading-anchor" href="#GradValley.Functional">GradValley.Functional</a><a id="GradValley.Functional-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Functional" title="Permalink"></a></h2></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../tutorials_and_examples/">Tutorials and Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 24 January 2023 13:02">Tuesday 24 January 2023</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
