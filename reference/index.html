<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · GradValley.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="GradValley.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="GradValley.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GradValley.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#GradValley"><span>GradValley</span></a></li><li><a class="tocitem" href="#GradValley.Layers"><span>GradValley.Layers</span></a></li><li><a class="tocitem" href="#GradValley.Optimization"><span>GradValley.Optimization</span></a></li><li><a class="tocitem" href="#GradValley.Functional"><span>GradValley.Functional</span></a></li></ul></li><li><a class="tocitem" href="../tutorials_and_examples/">Tutorials and Examples</a></li><li><a class="tocitem" href="../(pre-trained)_models/">(Pre-Trained) Models</a></li><li><a class="tocitem" href="../saving_and_loading/">Saving and Loading</a></li><li><a class="tocitem" href="../learning/">Learning</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For some (mostly internal) functions, the documentation is still missing because this documentation is still under construction!</p></div></div><ul><li><a href="#Reference">Reference</a></li><li class="no-marker"><ul><li><a href="#GradValley">GradValley</a></li><li class="no-marker"><ul><li><a href="#DataLoader">DataLoader</a></li></ul></li><li><a href="#GradValley.Layers">GradValley.Layers</a></li><li class="no-marker"><ul><li><a href="#Containers">Containers</a></li><li><a href="#Forward-and-Backward-Pass">Forward- and Backward-Pass</a></li><li><a href="#Reset/zero-gradients">Reset/zero gradients</a></li><li><a href="#Training-mode/test-mode">Training mode/test mode</a></li><li><a href="#Convolution">Convolution</a></li><li><a href="#Pooling">Pooling</a></li><li><a href="#Fully-connected">Fully connected</a></li><li><a href="#Identity">Identity</a></li><li><a href="#Normalization">Normalization</a></li><li><a href="#Reshape-/-Flatten">Reshape / Flatten</a></li><li><a href="#Special-activation-functions">Special activation functions</a></li></ul></li><li><a href="#GradValley.Optimization">GradValley.Optimization</a></li><li class="no-marker"><ul><li><a href="#Optimizers">Optimizers</a></li><li><a href="#Optimization-step-function">Optimization step function</a></li><li><a href="#Loss-functions">Loss functions</a></li></ul></li><li><a href="#GradValley.Functional">GradValley.Functional</a></li></ul></li></ul><h2 id="GradValley"><a class="docs-heading-anchor" href="#GradValley">GradValley</a><a id="GradValley-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley" title="Permalink"></a></h2><h3 id="DataLoader"><a class="docs-heading-anchor" href="#DataLoader">DataLoader</a><a id="DataLoader-1"></a><a class="docs-heading-anchor-permalink" href="#DataLoader" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.DataLoader" href="#GradValley.DataLoader"><code>GradValley.DataLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DataLoader(get_function::Function, dataset_size::Integer; batch_size::Integer=1, shuffle::Bool=false, drop_last::Bool=false)</code></pre><p>The DataLoader was designed to easily iterate over batches. Each time a new batch is requested, the data loader loads this batch &quot;just in time&quot; (instead of loading all the batches to memory at once). </p><p>The <code>get_function</code> is expected to load one item from a dataset at a given index. The specified <code>get_function</code> is expected to accept exactly one positional argument, which is the index of the item the <code>get_function</code> will return. A tuple of arbitrary length is expected as the return value of the <code>get_function</code>. Each element in this tuple must be an array. The length/size and type of the tuple and array is expected to be the same at each index. When a batch is requested, the data loader returns the tuple containing the with batch dimensions extended arrays.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The DataLoader is iteratabel and indexable. size(dataloader) returns the given size of the dataset, length(dataloader) returns the total number of batches (equal if batch_size=1). When a range is given as the index argument, a vector containing multiple batches (arrays) is returned.</p></div></div><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>If you <em>really</em> want to load the whole dataset to memory (e.g. useful when training over multiple epochs, with this way, you don&#39;t have to reload the dataset each epoch over and over again), you can do so of course: <code>all_batches = dataloader[start:end]</code> where <code>typeof(dataloader) == DataLoader</code></p></div></div><p><strong>Arguments</strong></p><ul><li><code>get_function::Function</code>: the function which takes the index of an item from a dataset and returns that item (an arbitrary sized tuple containing arrays)</li><li><code>dataset_size::Integer</code>: the maximum index the <code>get_function</code> accepts (the number of items in the dataset, the dataset size)</li><li><code>batch_size::Integer=1</code>: the batch size (the first dimension, the extended batch dimension, of each array in the returned tuple as that size)</li><li><code>shuffle::Bool=false</code>: reshuffle the data (doesn&#39;t reshuffle automatically after each epoch, use <a href="#GradValley.reshuffle!"><code>reshuffle!</code></a> instead)</li><li><code>drop_last::Bool=false</code>: set to true to drop the last incomplete batch, if the dataset size is not divisible by the batch size, if false and the size of dataset is not divisible by the batch size, then the last batch will be smaller</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># EXAMPLE FROM https://jonas208.github.io/GradValley.jl/tutorials_and_examples/#Tutorials-and-Examples
julia&gt; using MLDatasets # a package for downloading datasets
# initialize train- and test-dataset
julia&gt; mnist_train = MNIST(:train) 
julia&gt; mnist_test = MNIST(:test)
# define the get_element function:
# function for getting an image and the corresponding target vector from the train or test partition
julia&gt; function get_element(index, partition)
            # load one image and the corresponding label
            if partition == &quot;train&quot;
                image, label = mnist_train[index]
            else # test partition
                image, label = mnist_test[index]
            end
            # add channel dimension and rescaling the values to their original 8 bit gray scale values
            image = reshape(image, 1, 28, 28) .* 255
            # generate the target vector from the label, one for the correct digit, zeros for the wrong digits
            targets = zeros(10)
            targets[label + 1] = 1.00

            return convert(Array{Float64, 3}, image), targets
       end
# initialize the data loaders (with anonymous function which helps to easily distinguish between test- and train-partition)
train_data_loader = DataLoader(index -&gt; get_element(index, &quot;train&quot;), length(mnist_train), batch_size=32, shuffle=true)
test_data_loader = DataLoader(index -&gt; get_element(index, &quot;test&quot;), length(mnist_test), batch_size=32)
# in most cases NOT recommended: you can force the data loaders to load all the data at once into memory, depending on the dataset&#39;s size, this may take a while
julia&gt; # train_data = train_data_loader[begin:end] # turned off to save time
julia&gt; # test_data = test_data_loader[begin:end] # turned off to save time
# now you can write your train- or test-loop like so 
julia&gt; for (batch, (images_batch, targets_batch)) in enumerate(test_data_loader) #=do anything useful here=# end
julia&gt; for (batch, (images_batch, targets_batch)) in enumerate(train_data_loader) #=do anything useful here=# end</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.reshuffle!" href="#GradValley.reshuffle!"><code>GradValley.reshuffle!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">reshuffle!(data_loader::DataLoader)</code></pre><p>Manually shuffle the data loader (even if shuffle is disabled in the given data loader). It is recommended to reshuffle after each epoch during training.</p></div></section></article><h2 id="GradValley.Layers"><a class="docs-heading-anchor" href="#GradValley.Layers">GradValley.Layers</a><a id="GradValley.Layers-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Layers" title="Permalink"></a></h2><h3 id="Containers"><a class="docs-heading-anchor" href="#Containers">Containers</a><a id="Containers-1"></a><a class="docs-heading-anchor-permalink" href="#Containers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.SequentialContainer" href="#GradValley.Layers.SequentialContainer"><code>GradValley.Layers.SequentialContainer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SequentialContainer(layer_stack::Vector{&lt;: Any})</code></pre><p>A sequential container (recommended method for building models). A SequtialContainer can take a vector of layers or other SequentialContainers (submodules). While forward-pass, the given inputs are <em>sequentially</em> propagated through every layer (or submodule) and the output will be returned. The execution order during forward pass is of course the same as the order in the vector containing the layers or submodules. This container currently (!) only accepts Float64 array inputs. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You can use a SequentialContainer in a GraphContainer (and vice versa). You can also use a SequentialContainer in a SequentialContainer (nesting allowed).</p></div></div><p><strong>Arguments</strong></p><ul><li><code>layer_stack::Vector{&lt;: Any}</code>: the vector containing the layers (or submodules, so other Containers), the order of the modules in the vector corresponds to the execution order</li></ul><p><strong>Indexing and Iteration</strong></p><p>The sequential container is indexable and iterable. Indexing one element/iterating behaves like indexing one element of/iterating over  the <code>sequential_container.layer_stack</code> passed to the container at initialization. However, if the index is a range (UnitRange{&lt;: Integer}),  a new SequentialContainer containing all the requested submodules/layers is initialized and returned.  <code>length(sequential_container)</code> and <code>size(sequential_container)</code> both just return the number of modules in the layers vector (equivalent to <code>length(sequential_container.layer_stack)</code>).</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a simple chain of fully connected layers
julia&gt; m = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# computing the output of the module (with random inputs)
julia&gt; input = rand(32, 1000)
julia&gt; output = forward(m, input)

# a more complicated example with with nested submodules
julia&gt; feature_extractor_part_1 = SequentialContainer([Conv(1, 6, (5, 5), activation_function=&quot;relu&quot;), AvgPool((2, 2))])
julia&gt; feature_extractor_part_2 = SequentialContainer([Conv(6, 16, (5, 5), activation_function=&quot;relu&quot;), AvgPool((2, 2))])
julia&gt; feature_extractor = SequentialContainer([feature_extractor_part_1, feature_extractor_part_2])
julia&gt; classifier = SequentialContainer([Fc(256, 120, activation_function=&quot;relu&quot;), Fc(120, 84, activation_function=&quot;relu&quot;), Fc(84, 10)])
julia&gt; m = SequentialContainer([feature_extractor, Reshape((256, )), classifier, Softmax(dim=2)])
# computing the output of the module (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)

# indexing 
julia&gt; m[begin] # returns the feature_extractor_part_1 submodule (SequentialContainer)
julia&gt; m[end] # returns the softmax layer (Softmax)
julia&gt; m[begin:end-1] # returns the entire model except the softmax layer (a new SequentialContainer is initialized and returned) 

# if a SequentialContainer contains BatchNorm layers (regardless of whether they are nested somewhere in a submodule or not), 
# the mode of all these layers at once can be switched as follows
julia&gt; trainmode!(m)
julia&gt; testmode!(m)

# if a SequentialContainer contains layers with trainable parameters/weights (what is hopefully in nearly all situations the case),
# regardless of whether they are nested somewhere in a submodule or not, the gradients of all these layers at once can be reset as follows
julia&gt; zero_gradients(m)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.GraphContainer" href="#GradValley.Layers.GraphContainer"><code>GradValley.Layers.GraphContainer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GraphContainer(forward_pass::Function, layer_stack::Vector{&lt;: Any})</code></pre><p>A computational graph container (recommended method for building models). A GraphContainer can take a function representing the forward pass of a model and a vector of layers or other containers (submodules). While forward-pass, a tracked version of the given inputs are passed through the given forward pass function and the output will be returned. During forward pass, the computational graph is build by a function overload based automatic differentiation system (AD). During backward pass, this computational graph  is used to compute the gradients. This container currently (!) only accepts Float64 array inputs. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You can use a GraphContainer in a SequentialContainer (and vice versa). You can also use a GraphContainer in a GraphContainer (nesting allowed).</p></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Note that the GraphContainer is an experimental feature. The behavior of this module could change dramatically in the future. Using this module can may cause problems.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>forward_pass::Function</code>: the function representing the forward pass of a model</li><li><code>layer_stack::Vector{&lt;: Any}</code>: the vector containing the layers (or submodules, so other Containers), the order doesn&#39;t matter</li></ul><p><strong>Guidelines</strong></p><p>GradValley has its own little, rudimentary function overload based automatic differentiation system based on <a href="https://github.com/JuliaDiff/ChainRulesCore.jl">ChainRules.jl</a>. It was designed to allow simple modifications of a normal sequential signal flow, which is the basis of most neural networks.  For example, to be able to implement ResNet&#39;s residual connections. So it represents an alternative to data flow layers known from other Deep Learning packages. In a way, it is similar to the forward function known from every <a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#define-the-class">PyTorch model</a>. Since the AD does not offer that much functionality at this point in time, the following guidelines must be observed:</p><ul><li>The forward pass function must take at least two arguments. The first is the vector containing the layers (which was passed to GraphContainer at initialization). The following arguments (the last could also be a Vararg argument) are the data inputs.</li><li>The forward pass function must be written generically enough to accept arrays of type T&lt;:AbstractArray/real numbers of type T&lt;:Real as input (starting with the second argument).</li><li>Array inputs that are being differentiated cannot be mutated.</li><li>The initialization of new arrays (for example with <code>zeros</code> or <code>rand</code>) and their use in mix with the input passed to the forward function is not allowed.</li><li>Avoid dot syntax in most cases, there only exist a few differentiation rules for the most basic vectorized operators (.+, .-, .*, ./, .^).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a simple chain of fully connected layers (equivalent to the first example of SequentialContainer)
julia&gt; layers = [Fc(1000, 500), Fc(500, 250), Fc(250, 125)]
julia&gt; function forward_pass(layers::Vector, input::AbstractArray)
           fc_1, fc_2, fc_3 = layers
           output = forward(fc_1, input)
           output = forward(fc_2, output)
           output = forward(fc_3, output)
           return output
       end
julia&gt; m = GraphContainer(forward_pass, layers)
# computing the output of the module (with random inputs)
julia&gt; input = rand(32, 1000)
julia&gt; output = forward(m, input)

# a more complicated example: implementation of an inverted residual block
julia&gt; layers = [Conv(16, 64, (1, 1), activation_function=&quot;relu&quot;), 
                 DepthwiseConv(64, 64, (3, 3), padding=(1, 1), activation_function=&quot;relu&quot;), 
                 Conv(64, 16, (1, 1), activation_function=&quot;relu&quot;)]
julia&gt; function forward_pass(layers::Vector, input::AbstractArray)
           conv_1, depthwise_conv, conv_2 = layers
           output = forward(conv_1, input)
           output = forward(depthwise_conv, output)
           output = forward(conv_2, output)
           output = output + input # residual/skipped connection
           return output
       end
julia&gt; m = GraphContainer(forward_pass, layers)
# computing the output of the module (with random inputs)
julia&gt; input = rand(32, 16, 50, 50)
julia&gt; output = forward(m, input)

# a simple example with a polynomial, just to show that it is possible to use the GraphContainer like an automatic differentiation (AD) tool 
julia&gt; f(layers, x) = 0.5x^3 - 2x^2 + 10
julia&gt; df(x) = 1.5x^2 - 4x # checking the result of the AD with this manually written derivation 
julia&gt; m = GraphContainer(f, [])
julia&gt; y = forward(m, 3)
julia&gt; dydx = backward(m, 1) # in this case, no loss function was used, so we have no gradient information, therefore we use 1 as the so-called seed
1-element Vector{Float64}:
 1.5
julia&gt; manual_dydx = df(3)
1.5
julia&gt; isapprox(dydx[1], manual_dydx)
true

# if a GraphContainer contains BatchNorm layers (regardless of whether they are nested somewhere in a submodule or not), 
# the mode of all these layers at once can be switched as follows
julia&gt; trainmode!(m)
julia&gt; testmode!(m)

# if a GraphContainer contains layers with trainable parameters/weights (what is hopefully in nearly all situations the case),
# regardless of whether they are nested somewhere in a submodule or not, the gradients of all these layers at once can be reset as follows
julia&gt; zero_gradients(m)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.summarize_model" href="#GradValley.Layers.summarize_model"><code>GradValley.Layers.summarize_model</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">summarize_model(container::Union{SequentialContainer, GraphContainer})</code></pre><p>Return a string (and the total number of parameters) intended for printing with an overview of the model  (currently doesn&#39;t show an visualization of the computational graph) and its number of parameters.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.clean_module_from_backward_information!" href="#GradValley.Layers.clean_module_from_backward_information!"><code>GradValley.Layers.clean_module_from_backward_information!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">clean_module_from_backward_information!(container_or_layer)</code></pre><p>Clean a container (so all the layers it contains) or a single layer from backward pass information (e.g. gradients). It is recommended to run this function on a model which should be saved to file. ```</p></div></section></article><h3 id="Forward-and-Backward-Pass"><a class="docs-heading-anchor" href="#Forward-and-Backward-Pass">Forward- and Backward-Pass</a><a id="Forward-and-Backward-Pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-and-Backward-Pass" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.forward" href="#GradValley.Layers.forward"><code>GradValley.Layers.forward</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">forward(layer, input::Array{Float64})</code></pre><p>The forward function for computing the output of a module. For every layer/container, an individual method exists. However, all these methods work exactly the same. They all take the layer/container as the first argument and the input data as the second argument. The output is returned.  All layers/containers currently (!) only accept Float64 array inputs, so all methods also expect a Float64 array input, the number of dimensions can differ.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define some layers and containers
julia&gt; layer = Conv(3, 6, (5, 5))
julia&gt; container = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# create some random input data
julia&gt; layer_input = rand(32, 3, 50, 50)
julia&gt; container_input = rand(32, 1000)
# compute the output of the modules
julia&gt; layer_output = forward(layer, layer_input)
julia&gt; container_output = forward(container, container_input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.backward" href="#GradValley.Layers.backward"><code>GradValley.Layers.backward</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">backward(layer, next_layer)</code></pre><p>The backward function for computing the gradients for a layer. Also well known as backpropagation. For every layer, an individual method exists. However, all these methods work exactly the same. They all take the current layer for which the gradients should be computed as the first argument  and next layer containing the backpropagated losses used to compute the gradients for the current layer. No gradients are returned, they are just saved in the layer.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Note that this backward function differs from the backward functions for containers. As a user, it is highly recommended to use containers for model building because they create the forward and backward pass automatically. Calling the backward functions for all the layers individually is normally not necessary and also not recommended.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define two layers  
julia&gt; layer_1 = Fc(500, 250)
julia&gt; layer_2 = Fc(250, 125)
# compute the output of the layers (with random inputs)
julia&gt; output = forward(layer_1, rand(32, 500))
julia&gt; output = forward(layer_2, output)
# use a loss function (with random data as target values) and save the derivative of the loss
julia&gt; loss, derivative_loss = mse_loss(output, rand(32, 125)) # note that GradValley.Optimization.mse_loss must be imported
# before the gradients are recalculated, the old gradients should always be reset first
julia&gt; zero_gradients(layer_1)
julia&gt; zero_gradients(layer_2)
# backpropagation (compute the gradients to the weights and backpropagate the losses)
# because there exists no next layer after the last layer to take the backpropagted losses from, we will have to manually store the derivative of the loss in the last layer
julia&gt; layer_2.losses = derivative_loss
# than we can compute the gradients to the weights and backpropagate the losses
compute_gradients(layer_2)
compute_previous_losses(layer_2)
# now we can go on with the actual backward function
backward(layer_1, layer_2)</code></pre></div></section></article><h3 id="Reset/zero-gradients"><a class="docs-heading-anchor" href="#Reset/zero-gradients">Reset/zero gradients</a><a id="Reset/zero-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Reset/zero-gradients" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.zero_gradients" href="#GradValley.Layers.zero_gradients"><code>GradValley.Layers.zero_gradients</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zero_gradients(layer_or_container)</code></pre><p>Resets the gradients of a layer or a container (any kind of module with trainable parameters). </p><p>There only exists methods for layers with parameters, however, if a container without layers with trainable parameters is given, NO error will be thrown. So if the given container contains layers with trainable parameters/weights, regardless of whether they are nested somewhere in a submodule or not,  the gradients of all these layers at once will be reset.</p></div></section></article><h3 id="Training-mode/test-mode"><a class="docs-heading-anchor" href="#Training-mode/test-mode">Training mode/test mode</a><a id="Training-mode/test-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Training-mode/test-mode" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.trainmode!" href="#GradValley.Layers.trainmode!"><code>GradValley.Layers.trainmode!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainmode!(batch_norm_layer_or_container)</code></pre><p>Switches the mode of the given batch normalization layer or container to training mode. See <a href="#Normalization">Normalization</a></p><p>If the given container contains batch normalization layers (regardless of whether they are nested somewhere in a submodule or not),  the mode of all these layers at once will be switched to training mode.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.testmode!" href="#GradValley.Layers.testmode!"><code>GradValley.Layers.testmode!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">testmode!(batch_norm_layer_or_container)</code></pre><p>Switches the mode of the given batch normalization layer or container to test mode. See <a href="#Normalization">Normalization</a></p><p>If the given container contains batch normalization layers (regardless of whether they are nested somewhere in a submodule or not),  the mode of all these layers at once will be switched to test mode.</p></div></section></article><h3 id="Convolution"><a class="docs-heading-anchor" href="#Convolution">Convolution</a><a id="Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.Conv" href="#GradValley.Layers.Conv"><code>GradValley.Layers.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Conv(in_channels::Int, out_channels::Int, kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=(1, 1), padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), groups::Int=1, activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A convolution layer. Apply a 2D convolution over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_channels::Int</code>: the number of channels in the input image</li><li><code>out_channels::Int</code>: the number of channels produced by the convolution</li><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the convolving kernel</li><li><code>stride::Tuple{Int, Int}=(1, 1)</code>: the stride of the convolution</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between kernel elements</li><li><code>groups::Int=1</code>: the number of blocked connections from input channels to output channels (in-channels and out-channels must both be divisible by groups)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the convolution </li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li><li><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C_{in}, H_{in}, W_{in})$</span></li><li>Weight: <span>$(C_{out}, \frac{C_{in}}{groups}, H_{w}, W_{w})$</span></li><li>Bias: <span>$(C_{out}, )$</span></li><li>Output: <span>$(N, C_{out}, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1) - 1}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1) - 1}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>kernels::Array{Float64, 4}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 4}</code>: the current gradients of the weights/kernels</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>For one group, a multichannel 2D convolution (disregarding batch dimension and activation function) can be described as:</p><ul><li><span>$o_{c_{out}, y_{out}, x_{out}} = \big(\sum_{c_{in=1}}^{C_{in}}\sum_{y_w=1}^{H_{w}}\sum_{x_w=1}^{W_{w}} i_{c_{in}, y_{in}, x_{in}} \cdot w_{c_{out}, c_{in}, y_w, x_w}\big) + b_{c_{out}}$</span>, where<ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul></li></ul><p><em>O</em> is the output array, <em>I</em> the input array, <em>W</em> the weight array and <em>B</em> the bias array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># square kernels and fully default values of keyword arguments
julia&gt; m = Conv(3, 6, (5, 5))
# non-square kernels and unequal stride and with padding as well as specified weight initialization mode
# (init_mode=&quot;kaiming&quot; stands for kaiming weight initialization with normally distributed values)
julia&gt; m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(2, 1))
# non-square kernels and unequal stride and with padding, dilation and 3 groups
# (because groups=in_channels and out_channles is divisible by groups, it is even a depthwise convolution)
julia&gt; m = Conv(3, 6, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), groups=3)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.DepthwiseConv" href="#GradValley.Layers.DepthwiseConv"><code>GradValley.Layers.DepthwiseConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DepthwiseConv(in_channels::Int, out_channels::Int, kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=(1, 1), padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A depthwise convolution layer. Apply a 2D depthwise convolution over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_channels::Int</code>: the number of channels in the input image</li><li><code>out_channels::Int</code>: the number of channels produced by the convolution</li><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the convolving kernel</li><li><code>stride::Tuple{Int, Int}=(1, 1)</code>: the stride of the convolution</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between kernel elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the convolution </li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li><li><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C_{in}, H_{in}, W_{in})$</span></li><li>Weight: <span>$(C_{out}, \frac{C_{in}}{groups}, H_{w}, W_{w})$</span>, where <span>$groups = in\_channels$</span></li><li>Bias: <span>$(C_{out}, )$</span></li><li>Output: <span>$(N, C_{out}, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1) - 1}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1) - 1}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>kernels::Array{Float64, 4}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 4}</code>: the current gradients of the weights/kernels</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>A convolution is called depthwise if <span>$groups=in\_channels$</span> and <span>$out\_channels=k \cdot in\_channels$</span>, where <span>$k$</span> is a positive integer. The second condition ensures that the of number out-channels is divisible by the number of groups/in-channels. In the background, the standard convolution operation is also used for this layer.  It is just an interface making clear that this layer can only perform a depthwise convolution.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># square kernels and fully default values of keyword arguments
julia&gt; m = DepthwiseConv(3, 6, (5, 5))
# non-square kernels and unequal stride and with padding as well as specified weight initialization mode
# (init_mode=&quot;kaiming&quot; stands for kaiming weight initialization with normally distributed values)
julia&gt; m = DepthwiseConv(3, 6, (3, 5), stride=(2, 1), padding=(2, 1))
# non-square kernels and unequal stride and with padding, dilation and 3 groups
# (because groups=in_channels and out_channles is divisible by groups, it is even a depthwise convolution)
julia&gt; m = DepthwiseConv(3, 6, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1), groups=3)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.ConvTranspose" href="#GradValley.Layers.ConvTranspose"><code>GradValley.Layers.ConvTranspose</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConvTranspose(in_channels::Int, out_channels::Int, kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=(1, 1), padding::Tuple{Int, Int}=(0, 0), output_padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), groups::Int=1, activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A transpose convolution layer (also known as fractionally-strided convolution or deconvolution). Apply a 2D transposed convolution over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_channels::Int</code>: the number of channels in the input image</li><li><code>out_channels::Int</code>: the number of channels produced by the convolution</li><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the convolving kernel</li><li><code>stride::Tuple{Int, Int}=(1, 1)</code>: the stride of the convolution</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: because transposed convolution can be seen as a partly (not true) inverse of convolution, padding means is this case to cut off the desired number of pixels on each side (instead of adding pixels)</li><li><code>output_padding::Tuple{Int, Int}=(0, 0)</code>: additional size added to one side of each dimension in the output shape (note that output_padding is only used to calculate the output shape, but does not actually add zero-padding to the output)</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between kernel elements</li><li><code>groups::Int=1</code>: the number of blocked connections from input channels to output channels (in-channels and out-channels must both be divisible by groups)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the convolution </li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li><li><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C_{in}, H_{in}, W_{in})$</span></li><li>Weight: <span>$(C_{in}, \frac{C_{out}}{groups}, H_{w}, W_{w})$</span></li><li>Bias: <span>$(C_{out}, )$</span></li><li>Output: <span>$(N, C_{out}, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = (H_{in} - 1) \cdot stride[1] - 2 \cdot padding[1] + dilation[1] \cdot (H_w - 1) + output\_padding[1] + 1$</span></li><li><span>$W_{out} = (W_{in} - 1) \cdot stride[2] - 2 \cdot padding[2] + dilation[2] \cdot (W_w - 1) + output\_padding[2] + 1$</span></li></ul></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>kernels::Array{Float64, 4}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 4}</code>: the current gradients of the weights/kernels</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>A transposed convolution can be seen as the gradient of a normal convolution with respect to its inputs.  The forward pass of a transposed convolution is the backward pass of a normal convolution, so the forward pass of a normal convolution becomes the backward pass of a transposed convolution (with respect to its inputs).  For more detailed information, you can look at the <a href="https://github.com/jonas208/GradValley.jl/blob/main/src/functional/gv_convolution.jl">source code of (transposed) convolution</a>. A nice looking visualization of (transposed) convolution can be found <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># square kernels and fully default values of keyword arguments
julia&gt; m = ConvTranspose(6, 3, (5, 5))
# upsampling an output from normal convolution like in DCGANS, Unet, etc.
julia&gt; input = forward(Conv(3, 6, (5, 5)), rand(32, 3, 50, 50))
julia&gt; output = forward(m, input)
# the size of the output of the transposed convolution is equal to the size of the original input of the normal convolution
julia&gt; size(output)
(32, 3, 50, 50)</code></pre></div></section></article><h3 id="Pooling"><a class="docs-heading-anchor" href="#Pooling">Pooling</a><a id="Pooling-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.MaxPool" href="#GradValley.Layers.MaxPool"><code>GradValley.Layers.MaxPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MaxPool(kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=kernel_size, padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing)</code></pre><p>A maximum pooling layer. Apply a 2D maximum pooling over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the window to take the maximum over</li><li><code>stride::Tuple{Int, Int}=kernel_size</code>: the stride of the window</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between the window elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1) - 1}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1) - 1}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Definition</strong></p><p>A multichannel 2D maximum pooling (disregarding batch dimension and activation function) can be described as:</p><p class="math-container">\[\begin{align*}
o_{c, y_{out}, x_{out}} = \max
_{y_w = 1, ..., kernel\_size[1] \ x_w = 1, ..., kernel\_size[2]}
i_{c, y_{in}, x_{in}}
\end{align*}\]</p><p>Where</p><ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul><p><em>O</em> is the output array and <em>I</em> the input array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># pooling of square window of size=(3, 3) and automatically selected stride
julia&gt; m = MaxPool((3, 3))
# pooling of non-square window with custom stride and padding
julia&gt; m = MaxPool((3, 2), stride=(2, 1), padding=(1, 1))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.AvgPool" href="#GradValley.Layers.AvgPool"><code>GradValley.Layers.AvgPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AvgPool(kernel_size::Tuple{Int, Int}; stride::Tuple{Int, Int}=kernel_size, padding::Tuple{Int, Int}=(0, 0), dilation::Tuple{Int, Int}=(1, 1), activation_function::Union{Nothing, String}=nothing)</code></pre><p>An average pooling layer. Apply a 2D average pooling over an input signal with additional batch and channel dimensions. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>kernel_size::Tuple{Int, Int}</code>: the size of the window to take the average over</li><li><code>stride::Tuple{Int, Int}=kernel_size</code>: the stride of the window</li><li><code>padding::Tuple{Int, Int}=(0, 0)</code>: the zero padding added to all four sides of the input</li><li><code>dilation::Tuple{Int, Int}=(1, 1)</code>: the spacing between the window elements</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <ul><li><span>$H_{out} = {\frac{H_{in} + 2 \cdot padding[1] - dilation[1] \cdot (H_w - 1) - 1}{stride[1]}} + 1$</span></li><li><span>$W_{out} = {\frac{W_{in} + 2 \cdot padding[2] - dilation[2] \cdot (W_w - 1) - 1}{stride[2]}} + 1$</span></li></ul></li></ul><p><strong>Definition</strong></p><p>A multichannel 2D average pooling (disregarding batch dimension and activation function) can be described as:</p><ul><li><span>$o_{c, y_{out}, x_{out}} = \frac{1}{kernel\_size[1] \cdot kernel\_size[2]} \sum_{i=1}^{kernel\_size[1]}\sum_{j=1}^{kernel\_size[2]} i_{c, y_{in}, x_{in}}$</span>, where<ul><li><span>$y_{in} = y_{out} + (stride[1] - 1) \cdot (y_{out} - 1) + (y_w - 1) \cdot dilation[1]$</span></li><li><span>$x_{in} = x_{out} + (stride[2] - 1) \cdot (x_{out} - 1) + (x_w - 1) \cdot dilation[2]$</span></li></ul></li></ul><p><em>O</em> is the output array and <em>I</em> the input array.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># pooling of square window of size=(3, 3) and automatically selected stride
julia&gt; m = AvgPool((3, 3))
# pooling of non-square window with custom stride and padding
julia&gt; m = AvgPool((3, 2), stride=(2, 1), padding=(1, 1))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.AdaptiveMaxPool" href="#GradValley.Layers.AdaptiveMaxPool"><code>GradValley.Layers.AdaptiveMaxPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveMaxPool(output_size::Tuple{Int, Int}; activation_function::Union{Nothing, String}=nothing)</code></pre><p>An adaptive maximum pooling layer. Apply a 2D adaptive maximum pooling over an input signal with additional batch and channel dimensions. For any input size, the size of the output is always equal to the specified <span>$output\_size$</span>. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>output_size::Tuple{Int, Int}</code>: the target output size of the image (can even be larger than the input size) of the form <span>$(H_{out}, W_{out})$</span></li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <span>$(H_{out}, W_{out}) = output\_size$</span></li></ul><p><strong>Definition</strong></p><p>In some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard maximum pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: <a href="https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work">stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work</a>). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The <code>function get_in_indices(in_len, out_len)</code> in  <a href="https://github.com/jonas208/GradValley.jl/blob/main/src/gv_functional.jl"><code>gv_functional.jl</code></a> (line 95 - 113) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at <a href="#GradValley.Layers.MaxPool"><code>MaxPool</code></a> with the difference that the indices <span>$y_{in}$</span> and <span>$x_{in}$</span>  have already been calculated beforehand.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># target output size of 5x5
julia&gt; m = AdaptiveMaxPool((5, 5))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.AdaptiveAvgPool" href="#GradValley.Layers.AdaptiveAvgPool"><code>GradValley.Layers.AdaptiveAvgPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveAvgPool(output_size::Tuple{Int, Int}; activation_function::Union{Nothing, String}=nothing)</code></pre><p>An adaptive average pooling layer. Apply a 2D adaptive average pooling over an input signal with additional batch and channel dimensions. For any input size, the size of the output is always equal to the specified <span>$output\_size$</span>. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>output_size::Tuple{Int, Int}</code>: the target output size of the image (can even be larger than the input size) of the form <span>$(H_{out}, W_{out})$</span></li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output after the pooling</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H_{in}, W_{in})$</span></li><li>Output: <span>$(N, C, H_{out}, W_{out})$</span>, where <span>$(H_{out}, W_{out}) = output\_size$</span></li></ul><p><strong>Definition</strong></p><p>In some cases, the kernel-size and stride could be calculated in a way that the output would have the target size  (using a standard average pooling with the calculated kernel-size and stride, padding and dilation would not  be used in this case). However, this approach would only work if the input size is an integer multiple of the output size (See this question at stack overflow for further information: <a href="https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work">stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work</a>). A more generic approach is to calculate the indices of the input with an additional algorithm only for adaptive pooling.  With this approach, it is even possible that the output is larger than the input what is really unusual for pooling simply because that is the opposite of what pooling actually should do, namely reducing the size. The <code>function get_in_indices(in_len, out_len)</code> in  <a href="https://github.com/jonas208/GradValley.jl/blob/main/src/gv_functional.jl"><code>gv_functional.jl</code></a> (line 95 - 113) implements such an algorithm (similar to the one at the stack overflow question), so you could check there on how exactly it is defined. Thus, the mathematical definition would be identical to the one at <a href="#GradValley.Layers.AvgPool"><code>AvgPool</code></a> with the difference that the indices <span>$y_{in}$</span> and <span>$x_{in}$</span>  have already been calculated beforehand.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># target output size of 5x5
julia&gt; m = AdaptiveAvgPool((5, 5))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 3, 50, 50)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Fully-connected"><a class="docs-heading-anchor" href="#Fully-connected">Fully connected</a><a id="Fully-connected-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-connected" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.Fc" href="#GradValley.Layers.Fc"><code>GradValley.Layers.Fc</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Fc(in_features::Int, out_features::Int; activation_function::Union{Nothing, String}=nothing, init_mode::String=&quot;default_uniform&quot;, use_bias::Bool=true)</code></pre><p>A fully connected layer (sometimes also known as dense or linear). Apply a linear transformation (matrix multiplication) to the input signal with additional batch dimension. This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>in_features::Int</code>: the size of each input sample (<em>&quot;number of input neurons&quot;</em>)</li><li><code>out_features::Int</code>: the size of each output sample (<em>&quot;number of output neurons&quot;</em>)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li><li><code>init_mode::String=&quot;default_uniform&quot;</code>: the initialization mode of the weights   (can be <code>&quot;default_uniform&quot;</code>, <code>&quot;default&quot;</code>, <code>&quot;kaiming_uniform&quot;</code>, <code>&quot;kaiming&quot;</code>, <code>&quot;xavier_uniform&quot;</code> or <code>&quot;xavier&quot;</code>)</li></ul><p><code>use_bias::Bool=true</code>: if true, adds a learnable bias to the output</p><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, in\_features)$</span></li><li>Weight: <span>$(out\_features, in\_features)$</span></li><li>Bias: <span>$(out\_features, )$</span></li><li>Output: <span>$(N, out\_features)$</span></li></ul><p><strong>Useful Fields/Variables</strong></p><ul><li><code>weights::Array{Float64, 2}</code>: the learnable weights of the layer</li><li><code>bias::Vector{Float64}</code>: the learnable bias of the layer (used when <code>use_bias=true</code>)</li><li><code>gradients::Array{Float64, 2}</code>: the current gradients of the weights</li><li><code>bias_gradients::Vector{Float64}</code>: the current gradients of the bias</li></ul><p><strong>Definition</strong></p><p>The forward pass of a fully connected layer is given by the matrix multiplication between the weight matrix and the input vector  (disregarding batch dimension and activation function):</p><ul><li><span>$O = WI + B$</span></li></ul><p>This operation can also be described by:</p><ul><li><span>$o_{j} = \big(\sum_{k=1}^{in\_features} w_{j,k} \cdot i_{k}\big) + b_{j}$</span></li></ul><p><em>O</em> is the output vector, <em>I</em> the input vector, <em>W</em> the weight matrix and <em>B</em> the bias vector. Visually interpreted, it means that each input neuron <em>i</em> is weighted with the corresponding weight <em>w</em> connecting the input neuron  to the output neuron <em>o</em> where all the incoming signals are summed up.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a fully connected layer with 784 input features and 120 output features
julia&gt; m = Fc(784, 120)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 784)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Identity"><a class="docs-heading-anchor" href="#Identity">Identity</a><a id="Identity-1"></a><a class="docs-heading-anchor-permalink" href="#Identity" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.Identity" href="#GradValley.Layers.Identity"><code>GradValley.Layers.Identity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Identity(; activation_function::Union{Nothing, String}=nothing)</code></pre><p>An identity layer (can be used as an activation function layer). If no activation function is used, this layer does not change the signal in any way. However, if an activation function is used, the activation function will be applied to the inputs element-wise.  This layer currently (!) only accepts Float64 array inputs. </p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>This layer is helpful to apply an element-wise activation independent of a &quot;normal&quot; computational layer.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the inputs</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(*)$</span>, where <span>$*$</span> means any number of dimensions</li><li>Output: <span>$(*)$</span> (same shape as input)</li></ul><p><strong>Definition</strong></p><p>A placeholder identity operator, except the optional activation function, the input signal is not changed in any way. If an activation function is used, the activation function will be applied to the inputs element-wise. </p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># an independent relu activation
julia&gt; m = Identity(activation_function=&quot;relu&quot;)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 10)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Normalization"><a class="docs-heading-anchor" href="#Normalization">Normalization</a><a id="Normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Normalization" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.BatchNorm2d" href="#GradValley.Layers.BatchNorm2d"><code>GradValley.Layers.BatchNorm2d</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchNorm2d(num_features::Int; epsilon::Float64=1e-05, momentum::Float64=0.1, affine::Bool=true, track_running_stats::Bool=true, activation_function::Union{Nothing, String}=nothing)</code></pre><p>A batch normalization layer. Apply a batch normalization over a 4D input signal (a mini-batch of 2D inputs with additional channel dimension). This layer currently (!) only accepts Float64 array inputs. </p><p>This layer has two modes: training mode and test mode. If <code>track_running_stats::Bool=true</code>, this layer behaves differently in the two modes. During training, this layer always uses the currently calculated batch statistics. If <code>track_running_stats::Bool=true</code>, the running mean and variance are tracked during training and will be used while testing. If <code>track_running_stats::Bool=false</code>, even in test mode, the currently calculated batch statistics are used. The mode can be switched with <a href="#GradValley.Layers.trainmode!"><code>trainmode!</code></a> or <a href="#GradValley.Layers.testmode!"><code>testmode!</code></a> respectively. The training mode is active by default.</p><p><strong>Arguments</strong></p><ul><li><code>num_features::Int</code>: the number of channels</li><li><code>epsilon::Float64=1e-05</code>: a value added to the denominator for numerical stability</li><li><code>momentum::Float64=0.1</code>: the value used for the running mean and running variance computation</li><li><code>affine::Bool=true</code>: if true, this layer uses learnable affine parameters/weights (<span>$\gamma$</span> and <span>$\beta$</span>)</li><li><code>track_running_stats::Bool=true</code>: if true, this layer tracks the running mean and variance during training and will use them for testing/evaluation, if false, such statistics are not tracked and, even in test mode, the batch statistics are always recalculated for each new input</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, C, H, W)$</span></li><li><span>$\gamma$</span> Weight, <span>$\beta$</span> Bias: <span>$(C, )$</span></li><li>Running Mean/Variance: <span>$(C, )$</span></li><li>Output: <span>$(N, C, H, W)$</span> (same shape as input)</li></ul><p><strong>Useful Fields/Variables</strong></p><p><strong>Weights (used if <code>affine::Bool=true</code>)</strong></p><ul><li><code>weight_gamma::Vector{Float64}</code>: <span>$\gamma$</span>, a learnabele parameter for each channel, initialized with ones</li><li><code>weight_beta::Vector{Float64}</code>: <span>$\beta$</span>, a learnabele parameter for each channel, initialized with zeros</li></ul><p><strong>Gradients of weights (used if <code>affine::Bool=true</code>)</strong></p><ul><li><code>gradient_gamma::Vector{Float64}</code>: the gradients of <span>$\gamma$</span></li><li><code>gradient_beta::Vector{Float64}</code>: the gradients of <span>$\beta$</span></li></ul><p><strong>Running statistics (used if <code>rack_running_stats::Bool=true</code>)</strong></p><ul><li><code>running_mean::Vector{Float64}</code>: the continuously updated batch statistics of the mean</li><li><code>running_variance::Vector{Float64}</code>: the continuously updated batch statistics of the variance</li></ul><p><strong>Definition</strong></p><p>A batch normalization operation can be described as: For input values over a mini-batch: <span>$\mathcal{B} = \{x_1, x_2, ..., x_n\}$</span></p><p class="math-container">\[\begin{align*}
y_i = \frac{x_i - \overline{\mathcal{B}}}{\sqrt{Var(\mathcal{B}) + \epsilon}} \cdot \gamma + \beta
\end{align*}\]</p><p>Where <span>$y_i$</span> is an output value and <span>$x_i$</span> an input value. <span>$\overline{\mathcal{B}}$</span> is the mean of the input values in <span>$\mathcal{B}$</span> and <span>$Var(\mathcal{B})$</span>  is the variance of the input values in <span>$\mathcal{B}$</span>. Note that this definition is fairly general and not specified to 4D inputs. In this case, the input values of <span>$\mathcal{B}$</span> are taken for each channel individually.  So the mean and variance are calculated per channel over the mini-batch.</p><p>The update rule for the running statistics (running mean/variance) is:</p><p class="math-container">\[\begin{align*}
\hat{x}_{new} = (1 - momentum) \cdot \hat{x} + momentum \cdot x
\end{align*}\]</p><p>Where <span>$\hat{x}$</span> is the estimated statistic and <span>$x$</span> is the new observed value. So <span>$\hat{x}_{new}$</span> is the new, updated estimated statistic.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># a batch normalization layer (3 channels) with learnabel parameters and continuously updated batch statistics for evaluation
julia&gt; m = BatchNorm2d(3)
# the mode can be switched with trainmode! or testmode!
julia&gt; trainmode!(m)
julia&gt; testmode!(m)
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Reshape-/-Flatten"><a class="docs-heading-anchor" href="#Reshape-/-Flatten">Reshape / Flatten</a><a id="Reshape-/-Flatten-1"></a><a class="docs-heading-anchor-permalink" href="#Reshape-/-Flatten" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.Reshape" href="#GradValley.Layers.Reshape"><code>GradValley.Layers.Reshape</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Reshape(out_shape; activation_function::Union{Nothing, String}=nothing)</code></pre><p>A reshape layer (probably mostly used as a flatten layer). Reshape the input signal (effects all dimensions except the batch dimension). This layer currently (!) only accepts Float64 array inputs. </p><p><strong>Arguments</strong></p><ul><li><code>out_shape</code>: the target output size (the output has the same data as the input and must have the same number of elements)</li><li><code>activation_function::Union{Nothing, String}=nothing</code>: the element-wise activation function which will be applied to the output</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(N, *)$</span>, where * means any number of dimensions</li><li>Output: <span>$(N, out\_shape...)$</span></li></ul><p><strong>Definition</strong></p><p>This layer uses the standard <a href="https://docs.julialang.org/en/v1/base/arrays/#Base.reshape">reshape function</a> inbuilt in Julia.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># flatten the input of size 1*28*28 to a vector of length 784 (each plus batch dimension of course)
julia&gt; m = Reshape((784, ))
# computing the output of the layer (with random inputs)
julia&gt; input = rand(32, 1, 28, 28)
julia&gt; output = forward(m, input)</code></pre></div></section></article><h3 id="Special-activation-functions"><a class="docs-heading-anchor" href="#Special-activation-functions">Special activation functions</a><a id="Special-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Special-activation-functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Layers.Softmax" href="#GradValley.Layers.Softmax"><code>GradValley.Layers.Softmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Softmax(; dim::Integer=1)</code></pre><p>A softmax activation function layer (probably mostly used at the &quot;end&quot; of a classifier model). Apply the softmax function to an n-dimensional input array. The softmax will be computed along the given dimension (<code>dim::Integer</code>), so every slice along that dimension will sum to 1. This layer currently (!) only accepts Float64 array inputs. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Note that this is the only activation function in form of a layer. All other activation functions can be used with the <code>activation_function::String</code> keyword argument nearly every layer provides. All the activation functions which can be used that way are simple element-wise activation functions. Softmax is currently the only non-element-wise activation function. Besides it is very important to be able to select a specific dimension along the  softmax should be computed. That would also not work well with the use of simple keyword argument taking only a string which is the name of the function.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>dim::Integer=1</code>: the dimension along the softmax will be computed (so every slice along that dimension will sum to 1)</li></ul><p><strong>Shapes</strong></p><ul><li>Input: <span>$(*)$</span>, where <span>$*$</span> means any number of dimensions</li><li>Output: <span>$(*)$</span> (same shape as input)</li></ul><p><strong>Definition</strong></p><p>The softmax function converts a vector of real numbers into a probability distribution. The softmax function is defined as:</p><p class="math-container">\[\begin{align*}
softmax(x_i) = \frac{e^{x_i}}{\sum_{j}e^{x_j}} = \frac{exp(x_i)}{\sum_{j}exp(x_j)}
\end{align*}\]</p><p>Where <em>X</em> is the input array (slice). Note that the <span>$x_j$</span> values are taken from each slice individually along the specified dimension. So each slice along the specified dimension will sum to 1. All values in the output are between 0 and 1.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># the softmax will be computed along the second dimension
julia&gt; m = Softmax(dim=2)
# computing the output of the layer 
# (with random input data which could represent a batch of unnormalized output values from a classifier)
julia&gt; input = rand(32, 10)
julia&gt; output = forward(m, input)
# summing up the values in the output along the second dimension result in a batch of 32 ones
julia&gt; sum(output, dims=2)
32x1 Matrix{Float64}:
1.0
1.0
...
1.0</code></pre></div></section></article><h2 id="GradValley.Optimization"><a class="docs-heading-anchor" href="#GradValley.Optimization">GradValley.Optimization</a><a id="GradValley.Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Optimization" title="Permalink"></a></h2><h3 id="Optimizers"><a class="docs-heading-anchor" href="#Optimizers">Optimizers</a><a id="Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.SGD" href="#GradValley.Optimization.SGD"><code>GradValley.Optimization.SGD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SGD(layer_stack::Union{Vector, SequentialContainer, GraphContainer}, learning_rate::Real; weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)</code></pre><p>Implementation of stochastic gradient descent optimization algorithm (including optional weight decay and dampening).</p><p><strong>Arguments</strong></p><ul><li><code>layer_stack::Union{Vector, SequentialContainer, GraphContainer}</code>: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without parameters)</li><li><code>learning_rate::Real</code>: the learning rate (shouldn&#39;t be 0)</li><li><code>weight_decay::Real=0.00</code>: the weight decay (L2 penalty)</li><li><code>dampening::Real=0.00</code>: the dampening (normally just for optimizers with momentum, however, can be theoretically used without, in this case acts like: <span>$(1 - dampening) \cdot learning\_rate$</span>)</li><li><code>maximize::Bool=false</code>: maximize the parameters, instead of minimizing </li></ul><p><strong>Definition</strong></p><p>For example, a definition of this algorithm in pseudocode can be found <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html?highlight=sgd#torch.optim.SGD">here</a>. (Note that in this case of a simple SGD with no momentum, the momentum <span>$μ$</span> is zero in the sense of the mentioned documentation.)</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# initialize a SGD optimizer with learning-rate equal 0.1 and weight decay equal to 0.5 (otherwise default values)
julia&gt; optimizer = SGD(model, 0.1, weight_decay=0.5)
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative 
julia&gt; loss, loss_derivative = mse_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)
# perform a single optimization step (parameter update)
julia&gt; step!(optimizer)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.MSGD" href="#GradValley.Optimization.MSGD"><code>GradValley.Optimization.MSGD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MSGD(layer_stack::Union{Vector, SequentialContainer, GraphContainer}, learning_rate::Real; momentum::Real=0.90, weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)</code></pre><p>Implementation of stochastic gradient descent with momentum optimization algorithm (including optional weight decay and dampening).</p><p><strong>Arguments</strong></p><ul><li><code>layer_stack::Union{Vector, SequentialContainer, GraphContainer}</code>: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without any parameters)</li><li><code>learning_rate::Real</code>: the learning rate (shouldn&#39;t be 0)</li><li><code>momentum::Real=0.90</code>: the momentum factor (shouldn&#39;t be 0)</li><li><code>weight_decay::Real=0.00</code>: the weight decay (L2 penalty)</li><li><code>dampening::Real=0.00</code>: the dampening for the momentum </li><li><code>maximize::Bool=false</code>: maximize the parameters, instead of minimizing </li></ul><p><strong>Definition</strong></p><p>For example, a definition of this algorithm in pseudocode can be found <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html?highlight=sgd#torch.optim.SGD">here</a>. (Note that in this case of SGD with default momentum, in the sense of the mentioned documentation, the momentum <span>$\mu$</span> isn&#39;t zero (<span>$\mu \neq 0$</span>) and <span>$nesterov$</span> is <span>$false$</span>.)</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# initialize a MSGD optimizer with learning-rate equal 0.1 and momentum equal to 0.75 (otherwise default values)
julia&gt; optimizer = Nesterov(model, 0.1, momentum=0.75)
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative 
julia&gt; loss, loss_derivative = mse_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)
# perform a single optimization step (parameter update)
julia&gt; step!(optimizer)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.Nesterov" href="#GradValley.Optimization.Nesterov"><code>GradValley.Optimization.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(layer_stack::Union{Vector, SequentialContainer, GraphContainer}, learning_rate::Real; momentum::Real=0.90, weight_decay::Real=0.00, dampening::Real=0.00, maximize::Bool=false)</code></pre><p>Implementation of stochastic gradient descent with nesterov momentum optimization algorithm (including optional weight decay and dampening).</p><p><strong>Arguments</strong></p><ul><li><code>layer_stack::Union{Vector, SequentialContainer, GraphContainer}</code>: the vector OR the container (SequentialContainer/GraphContainer, often simply the whole model) containing the layers with the parameters to be optimized (can also contain layers without any parameters)</li><li><code>learning_rate::Real</code>: the learning rate (shouldn&#39;t be 0)</li><li><code>momentum::Real=0.90</code>: the momentum factor (shouldn&#39;t be 0)</li><li><code>weight_decay::Real=0.00</code>: the weight decay (L2 penalty)</li><li><code>dampening::Real=0.00</code>: the dampening for the momentum (for true nesterov momentum, dampening must be 0)</li><li><code>maximize::Bool=false</code>: maximize the parameters, instead of minimizing </li></ul><p><strong>Definition</strong></p><p>For example, a definition of this algorithm in pseudocode can be found <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html?highlight=sgd#torch.optim.SGD">here</a>. (Note that in this case of SGD with nesterov momentum, <span>$nesterov$</span> is <span>$true$</span> in the sense of the mentioned documentation.)</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# initialize a Nesterov optimizer with learning-rate equal 0.1 and nesterov momentum equal to 0.8 (otherwise default values)
julia&gt; optimizer = Nesterov(model, 0.1, momentum=0.8)
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative 
julia&gt; loss, loss_derivative = mse_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)
# perform a single optimization step (parameter update)
julia&gt; step!(optimizer)</code></pre></div></section></article><h3 id="Optimization-step-function"><a class="docs-heading-anchor" href="#Optimization-step-function">Optimization step function</a><a id="Optimization-step-function-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-step-function" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.step!" href="#GradValley.Optimization.step!"><code>GradValley.Optimization.step!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">step!(optimizer::Union{SGD, MSGD, Nesterov})</code></pre><p>Perform a single optimization step (parameter update) for the given optimizer.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# initialize an optimizer (which optimizer specifically dosen&#39;t matter)
julia&gt; optimizer = SGD(model, 0.1)
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative 
julia&gt; loss, loss_derivative = mse_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)
# perform a single optimization step (parameter update)
julia&gt; step!(optimizer)</code></pre></div></section></article><h3 id="Loss-functions"><a class="docs-heading-anchor" href="#Loss-functions">Loss functions</a><a id="Loss-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.mae_loss" href="#GradValley.Optimization.mae_loss"><code>GradValley.Optimization.mae_loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mae_loss(prediction::AbstractArray{&lt;: Real, N}, target::AbstractArray{&lt;: Real, N}; reduction_method::Union{AbstractString, Nothing}=&quot;mean&quot;, return_derivative::Bool=true) where N</code></pre><p>Calculate the (mean) absolute error (L1 norm, with optional reduction to a single loss value (mean or sum)) and it&#39;s derivative with respect to the prediction input.</p><p><strong>Arguments</strong></p><ul><li><code>prediction::AbstractArray{&lt;: Real, N}</code>: the prediction of the model of shape (*), where * means any number of dimensions </li><li><code>target::AbstractArray{&lt;: Real, N}</code>: the corresponding target values of shape (*), must have the same shape as the prediction input </li><li><code>reduction_method::Union{AbstractString, Nothing}=&quot;mean&quot;</code>: can be <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code> or <code>nothing</code>, specifies the reduction method which reduces the element-wise computed loss to a single value</li><li><code>return_derivative::Bool=true</code>: it true, the loss and it&#39;s derivative with respect to the prediction input is returned, if false, just the loss will be returned</li></ul><p><strong>Definition</strong></p><p><span>$L$</span> is the loss value which will be returned. If <code>return_derivative</code> is true, then an array with the same shape as prediction/target is returned as well, it contains the partial derivatives of <span>$L$</span> w.r.t. to each prediction value: <span>$\frac{\partial L}{\partial p_i}$</span>, where <span>$p_i$</span> in one prediction value. If <code>reduction_method</code> is <code>nothing</code>, the element-wise computed losses are returned. Note that for <code>reduction_method=nothing</code>, the derivative is just the same as when <code>reduction_method=&quot;sum&quot;</code>. The element-wise calculation can be defined as (where <span>$t_i$</span> is one target value and <span>$l_i$</span> is one loss value): </p><p class="math-container">\[\begin{align*}
l_i = |p_i - t_i|
\end{align*}\]</p><p>Then, <span>$L$</span> and <span>$\frac{\partial L}{\partial p_i}$</span> differ a little bit from case to case (<span>$n$</span> is the number of values in <code>prediction</code>/<code>target</code>):</p><p class="math-container">\[\begin{align*}
L;\frac{\partial L}{\partial p_i} = \begin{cases}\frac{1}{n}\sum_{i=1}^{n} l_i; \frac{p_i - t_i}{l_i \cdot n} &amp;\text{for reduction\_method=&quot;mean&quot;}\\\sum_{i=1}^{n} l_i; \frac{p_i - t_i}{l_i} &amp;\text{for reduction\_method=&quot;sum&quot;}\end{cases}
\end{align*}\]</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative (with default reduction method &quot;mean&quot;)
julia&gt; loss, loss_derivative = mae_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Optimization.mse_loss" href="#GradValley.Optimization.mse_loss"><code>GradValley.Optimization.mse_loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mse_loss(prediction::AbstractArray{&lt;: Real, N}, target::AbstractArray{&lt;: Real, N}; reduction_method::Union{AbstractString, Nothing}=&quot;mean&quot;, return_derivative::Bool=true) where N</code></pre><p>Calculate the (mean) squared error (squared L2 norm, with optional reduction to a single loss value (mean or sum)) and it&#39;s derivative with respect to the prediction input.</p><p><strong>Arguments</strong></p><ul><li><code>prediction::AbstractArray{&lt;: Real, N}</code>: the prediction of the model of shape (*), where * means any number of dimensions </li><li><code>target::AbstractArray{&lt;: Real, N}</code>: the corresponding target values of shape (*), must have the same shape as the prediction input </li><li><code>reduction_method::Union{AbstractString, Nothing}=&quot;mean&quot;</code>: can be <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code> or <code>nothing</code>, specifies the reduction method which reduces the element-wise computed loss to a single value</li><li><code>return_derivative::Bool=true</code>: it true, the loss and it&#39;s derivative with respect to the prediction input is returned, if false, just the loss will be returned</li></ul><p><strong>Definition</strong></p><p><span>$L$</span> is the loss value which will be returned. If <code>return_derivative</code> is true, then an array with the same shape as prediction/target is returned as well, it contains the partial derivatives of <span>$L$</span> w.r.t. to each prediction value: <span>$\frac{\partial L}{\partial p_i}$</span>, where <span>$p_i$</span> in one prediction value. If <code>reduction_method</code> is <code>nothing</code>, the element-wise computed losses are returned. Note that for <code>reduction_method=nothing</code>, the derivative is just the same as when <code>reduction_method=&quot;sum&quot;</code>. The element-wise calculation can be defined as (where <span>$t_i$</span> is one target value and <span>$l_i$</span> is one loss value): </p><p class="math-container">\[\begin{align*}
l_i = (p_i - t_i)^2
\end{align*}\]</p><p>Then, <span>$L$</span> and <span>$\frac{\partial L}{\partial p_i}$</span> differ a little bit from case to case (<span>$n$</span> is the number of values in <code>prediction</code>/<code>target</code>):</p><p class="math-container">\[\begin{align*}
L;\frac{\partial L}{\partial p_i} = \begin{cases}\frac{1}{n}\sum_{i=1}^{n} l_i; \frac{2}{n}(p_i - t_i)  &amp;\text{for reduction\_method=&quot;mean&quot;}\\\sum_{i=1}^{n} l_i; 2(p_i - t_i) &amp;\text{for reduction\_method=&quot;sum&quot;}\end{cases}
\end{align*}\]</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs"># define a model
julia&gt; model = SequentialContainer([Fc(1000, 500), Fc(500, 250), Fc(250, 125)])
# create some random input data
julia&gt; input = rand(32, 1000)
# compute the output of the model
julia&gt; output = forward(model, input)
# generate some random target values 
julia&gt; target = rand(size(output)...)
# compute the loss and it&#39;s derivative (with default reduction method &quot;mean&quot;)
julia&gt; loss, loss_derivative = mse_loss(output, target)
# computet the gradients 
julia&gt; backward(model, loss_derivative)</code></pre></div></section></article><h2 id="GradValley.Functional"><a class="docs-heading-anchor" href="#GradValley.Functional">GradValley.Functional</a><a id="GradValley.Functional-1"></a><a class="docs-heading-anchor-permalink" href="#GradValley.Functional" title="Permalink"></a></h2><p>GradValley.Functional contains many primitives common for various neuronal networks. Not all functions are documented because they are mostly used only internally (not by the user). </p><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.zero_pad_nd" href="#GradValley.Functional.zero_pad_nd"><code>GradValley.Functional.zero_pad_nd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zero_pad_nd(input::AbstractArray{T, N}, padding::NTuple{2, Integer}) where {T, N}</code></pre><p>Perform a padding-operation (nd =&gt; number of dimensions doesn&#39;t matter) as is usual for neural networks: equal padding one each &quot;end&quot; of an axis/dimension.</p><p><strong>Arguments</strong></p><ul><li><code>input::AbstractArray{T, N}</code>: of shape(d1, d2, ..., dn)</li><li><code>padding::NTuple{2, Integer}</code>: must be always a tuple of length of the number of dimensions of input: (pad-d1, pad-d2, ..., pad-dn)</li></ul><p>Shape of returned output: (d1 + padding[1] * 2, d2 + padding[2] * 2, ..., dn + padding[n] * 2)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.zero_pad_2d" href="#GradValley.Functional.zero_pad_2d"><code>GradValley.Functional.zero_pad_2d</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zero_pad_nd(input::AbstractArray{T, 4}, padding::NTuple{2, Integer}) where {T}</code></pre><p>Perform a padding-operation (2d =&gt; 4 dimensions, where the last 2 dimensions will be padded) as is usual for neural networks: equal padding one each &quot;end&quot; of the last two axis/dimension.</p><p><strong>Arguments</strong></p><ul><li><code>input::AbstractArray{T, 4}</code>: of shape(d1, d2, d3, d4)</li><li><code>padding::NTuple{2, Integer}</code>: must be always a tuple of length of the number of dimensions of input: (pad-d1, pad-d2, ..., pad-dn)</li></ul><p>Shape of returned output: (d1, d2, d3 + padding[1] * 2, d4 + padding[2] * 2)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.convolution2d!" href="#GradValley.Functional.convolution2d!"><code>GradValley.Functional.convolution2d!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">convolution2d!(outputs::AbstractArray{T, 4}, inputs::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}, bias::AbstractVector{T}, use_bias::Bool; stride::NTuple{2, T2}=(1, 1), padding::NTuple{2, T2}=(0, 0), dilation::NTuple{2, T2}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Non-allocating version of the forward function for the associated 2d-Convolution layer, see Conv for details.</p></div></section><section><div><pre><code class="nohighlight hljs">convolution2d!(conv_layer, inputs::AbstractArray{T, 4}; no_grad::Bool=false) where {T &lt;: Real}</code></pre><p>The forward function for the associated 2d-Convolution layer that the layer&#39;s forward function directly calls, see Conv for details.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.convolution2d" href="#GradValley.Functional.convolution2d"><code>GradValley.Functional.convolution2d</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">convolution2d(inputs::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}, bias::AbstractVector{T}, use_bias::Bool; stride::NTuple{2, T2}=(1, 1), padding::NTuple{2, T2}=(0, 0), dilation::NTuple{2, T2}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Allocating version of the forward function for the associated 2d-Convolution layer, see Conv for details.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.convolution2d_data_backward!" href="#GradValley.Functional.convolution2d_data_backward!"><code>GradValley.Functional.convolution2d_data_backward!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">convolution2d_data_backward!(depadded_losses::AbstractArray{T, 4}, out_losses::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}; stride::NTuple{2, T2}=(1, 1), padding::NTuple{2, T2}=(0, 0), dilation::NTuple{2, T2}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Non-allocating version of the data backward function for the associated 2d-Convolution layer (the results are used as the losses for the previous layer), see Conv for details.</p></div></section><section><div><pre><code class="nohighlight hljs">convolution2d_data_backward!(conv_layer)</code></pre><p>The data backward function for the associated 2d-Convolution layer (the results are used as the losses for the previous layer) that the layer&#39;s data backward function directly calls, see Conv for details.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.convolution2d_data_backward" href="#GradValley.Functional.convolution2d_data_backward"><code>GradValley.Functional.convolution2d_data_backward</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">convolution2d_data_backward(out_losses::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}; stride::NTuple{2, T2}=(1, 1), padding::NTuple{2, T2}=(0, 0), dilation::NTuple{2, T2}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Allocating version of the data backward function for the associated 2d-Convolution layer (the results are used as the losses for the previous layer), see Conv for details.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.deconvolution2d!" href="#GradValley.Functional.deconvolution2d!"><code>GradValley.Functional.deconvolution2d!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">deconvolution2d!(outputs::AbstractArray{T, 4}, inputs::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}, bias::AbstractVector{T}, use_bias::Bool; stride::NTuple{2, Integer}=(1, 1), padding::NTuple{2, Integer}=(0, 0), output_padding::NTuple{2, Integer}=(0, 0), dilation::NTuple{2, Integer}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Non-Allocating version of the forward function for the associated 2d-DeConvolution layer, see ConvTranspose for details.</p></div></section><section><div><pre><code class="nohighlight hljs">deconvolution2d!(conv_layer, inputs::AbstractArray{T, 4}; no_grad::Bool=false) where {T &lt;: Real}</code></pre><p>The forward function for the associated 2d-DeConvolution layer that the layer&#39;s forward function directly calls, see ConvTranspose for details.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GradValley.Functional.deconvolution2d" href="#GradValley.Functional.deconvolution2d"><code>GradValley.Functional.deconvolution2d</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">deconvolution2d(inputs::AbstractArray{T, 4}, kernels::AbstractArray{T, 4}, bias::AbstractVector{T}, use_bias::Bool; stride::NTuple{2, Integer}=(1, 1), padding::NTuple{2, Integer}=(0, 0), output_padding::NTuple{2, Integer}=(0, 0), dilation::NTuple{2, Integer}=(1, 1), groups::T2=1) where {T &lt;: Real, T2 &lt;: Integer}</code></pre><p>Allocating version of the forward function for the associated 2d-DeConvolution layer, see ConvTranspose for details.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../tutorials_and_examples/">Tutorials and Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 18 May 2023 21:18">Thursday 18 May 2023</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
